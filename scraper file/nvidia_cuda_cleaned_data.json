[
  {
    "url": "https://docs.nvidia.com/cuda/",
    "content": "cuda toolkit documentation update develop, optimize and deploy gpuaccelerated apps the nvidia cuda toolkit provides a development environment for creating high performance gpuaccelerated applications. with the cuda toolkit, you can develop, optimize, and deploy your applications on gpuaccelerated embedded systems, desktop workstations, enterprise data centers, cloudbased platforms and hpc supercomputers. the toolkit includes gpuaccelerated libraries, debugging and optimization tools, a cc compiler, and a runtime library to deploy your application. using builtin capabilities for distributing computations across multigpu configurations, scientists and researchers can develop applications that scale from single gpu workstations to cloud installations with thousands of gpus. release notesthe release notes for the cuda toolkit. cuda features archivethe list of cuda features by release. eulathe cuda toolkit end user license agreement applies to the nvidia cuda toolkit, the nvidia cuda samples, the nvidia display driver, nvidia nsight tools visual studio edition, and the associated documentation on cuda apis, programming model and development tools. if you do not agree with the terms and conditions of the license agreement, then do not download or use the software. installation guides quick start guidethis guide provides the minimal firststeps instructions for installation and verifying cuda on a standard system. installation guide windowsthis guide discusses how to install and check for correct operation of the cuda development tools on microsoft windows systems. installation guide linuxthis guide discusses how to install and check for correct operation of the cuda development tools on gnulinux systems. programming guides programming guidethis guide provides a detailed discussion of the cuda programming model and programming interface. it then describes the hardware implementation, and provides guidance on how to achieve maximum performance. the appendices include a list of all cudaenabled devices, detailed description of all extensions to the c language, listings of supported mathematical functions, c features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the lowlevel driver api. best practices guidethis guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for cudacapable gpu architectures. the intent is to provide guidelines for obtaining the best performance from nvidia gpus using the cuda toolkit. maxwell compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on gpus based on the nvidia maxwell architecture. this document provides guidance to ensure that your software applications are compatible with maxwell. pascal compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on gpus based on the nvidia pascal architecture. this document provides guidance to ensure that your software applications are compatible with pascal. volta compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on gpus based on the nvidia volta architecture. this document provides guidance to ensure that your software applications are compatible with volta. turing compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on gpus based on the nvidia turing architecture. this document provides guidance to ensure that your software applications are compatible with turing. nvidia ampere gpu architecture compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on gpus based on the nvidia ampere gpu architecture. this document provides guidance to ensure that your software applications are compatible with nvidia ampere gpu architecture. hopper compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on the hopper gpus. this document provides guidance to ensure that your software applications are compatible with hopper architecture. ada compatibility guidethis application note is intended to help developers ensure that their nvidia cuda applications will run properly on the ada gpus. this document provides guidance to ensure that your software applications are compatible with ada architecture. maxwell tuning guidemaxwell is nvidias thgeneration architecture for cuda compute applications. applications that follow the best practices for the kepler architecture should typically see speedups on the maxwell architecture without any code changes. this guide summarizes the ways that applications can be finetuned to gain additional speedups by leveraging maxwell architectural features. pascal tuning guidepascal is nvidias thgeneration architecture for cuda compute applications. applications that follow the best practices for the maxwell architecture should typically see speedups on the pascal architecture without any code changes. this guide summarizes the ways that applications can be finetuned to gain additional speedups by leveraging pascal architectural features. volta tuning guidevolta is nvidias thgeneration architecture for cuda compute applications. applications that follow the best practices for the pascal architecture should typically see speedups on the volta architecture without any code changes. this guide summarizes the ways that applications can be finetuned to gain additional speedups by leveraging volta architectural features. turing tuning guideturing is nvidias thgeneration architecture for cuda compute applications. applications that follow the best practices for the pascal architecture should typically see speedups on the turing architecture without any code changes. this guide summarizes the ways that applications can be finetuned to gain additional speedups by leveraging turing architectural features. nvidia ampere gpu architecture tuning guidenvidia ampere gpu architecture is nvidias thgeneration architecture for cuda compute applications. applications that follow the best practices for the nvidia volta architecture should typically see speedups on the nvidia ampere gpu architecture without any code changes. this guide summarizes the ways that applications can be finetuned to gain additional speedups by leveraging nvidia ampere gpu architectures features. hopper tuning guidehopper gpu architecture is nvidias thgeneration architecture for cuda compute applications. applications that follow the best practices for the nvidia volta architecture should typically see speedups on the hopper gpu architecture without any code changes. this guide summarizes the ways that applications can be finetuned to gain additional speedups by leveraging hopper gpu architectures features. ada tuning guidethe nvidia ada gpu architecture is nvidias latest architecture for cuda compute applications. the nvidia ada gpu architecture retains and extends the same cuda programming model provided by previous nvidia gpu architectures such as nvidia ampere and turing, and applications that follow the best practices for those architectures should typically see speedups on the nvidia ada architecture without any code changes. this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the nvidia ada gpu architectures features. ptx isathis guide provides detailed instructions on the use of ptx, a lowlevel parallel thread execution virtual machine and instruction set architecture isa. ptx exposes the gpu as a dataparallel computing device. video decodernvidia video decoder nvcuvid is deprecated. instead, use the nvidia video codec sdk ptx interoperabilitythis document shows how to write ptx that is abicompliant and interoperable with other cuda code. inline ptx assemblythis document shows how to inline ptx parallel thread execution assembly language statements into cuda code. it describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter. cuda api references cuda runtime apifields in structures might appear in order that is different from the order of declaration. cuda driver apifields in structures might appear in order that is different from the order of declaration. cuda math apithe cuda math api. cublasthe cublas library is an implementation of blas basic linear algebra subprograms on top of the nvidia cuda runtime. it allows the user to access the computational resources of nvidia graphical processing unit gpu, but does not autoparallelize across multiple gpus. cudla apithe cudla api. nvblasthe nvblas library is a multigpus accelerated dropin blas basic linear algebra subprograms built on top of the nvidia cublas library. nvjpegthe nvjpeg library provides highperformance gpu accelerated jpeg decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. cufftthe cufft library user guide. cubthe user guide for cub. cuda c standard librarythe api reference for libcu, the cuda c standard library. cufile api reference guidethe nvidia gpudirect storage cufile api reference guide provides information about the preliminary version of the cufile api reference guide that is used in applications and frameworks to leverage gds technology and describes the intent, context, and operation of those apis, which are part of the gds technology. curandthe curand library user guide. cusparsethe cusparse library user guide. nppnvidia npp is a library of functions for performing cuda accelerated processing. the initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. npp will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. the npp library is written to maximize flexibility, while maintaining high performance. nvjitlinkthe user guide for the nvjitlink library. nvfatbinthe user guide for the nvfatbin library. nvrtc runtime compilationnvrtc is a runtime compilation library for cuda c. it accepts cuda c source code in character string form and creates handles that can be used to obtain the ptx. the ptx string generated by nvrtc can be loaded by cumoduleloaddata and cumoduleloaddataex, and linked with other modules by culinkadddata of the cuda driver api. this facility can often provide optimizations and performance not possible in a purely offline static compilation. thrustthe c parallel algorithms library. cusolverthe cusolver library user guide. ptx compiler api references ptx compiler apisthis guide shows how to compile a ptx program into gpu assembly code using apis provided by the static ptx compiler library. miscellaneous cuda demo suitethis document describes the demo applications shipped with the cuda demo suite. cuda on wslthis guide is intended to help users get started with using nvidia cuda on windows subsystem for linux wsl . the guide covers installation and running cuda applications and containers in this environment. multiinstance gpu migthis edition of the user guide describes the multiinstance gpu feature of the nvidia a gpu. cuda compatibilitythis document describes cuda compatibility, including cuda enhanced compatibility and cuda forward compatible upgrade. cuptithe cuptiapi. the cuda profiling tools interface cupti enables the creation of profiling and tracing tools that target cuda applications. debugger apithe cuda debugger api. gpudirect rdmaa technology introduced in keplerclass gpus and cuda , enabling a direct path for communication between the gpu and a thirdparty peer device on the pci express bus when the devices share the same upstream root complex using standard features of pci express. this document introduces the technology and describes the steps necessary to enable a gpudirect rdma connection to nvidia gpus within the linux device driver model. gpudirect storagethe documentation for gpudirect storage. vgpuvgpus that support cuda. tools nvccthis is a reference document for nvcc, the cuda compiler driver. nvcc accepts a range of conventional compiler options, such as for defining macros and includelibrary paths, and for steering the compilation process. cudagdbthe nvidia tool for debugging cuda applications running on linux and qnx, providing developers with a mechanism for debugging cuda applications running on actual hardware. cudagdb is an extension to the x port of gdb, the gnu project debugger. compute sanitizerthe user guide for compute sanitizer. nsight eclipse plugins installation guidensight eclipse plugins installation guide nsight eclipse plugins editionnsight eclipse plugins edition getting started guide nsight systemsthe documentation for nsight systems. nsight computethe nvidia nsight compute is the nextgeneration interactive kernel profiler for cuda applications. it provides detailed performance metrics and api debugging via a user interface and command line tool. nsight visual studio editionthe documentation for nsight visual studio edition. profilerthis is the guide to the profiler. cuda binary utilitiesthe application notes for cuobjdump, nvdisasm, and nvprune. white papers floating point and ieee a number of issues related to floating point accuracy and compliance are a frequent source of confusion on both cpus and gpus. the purpose of this white paper is to discuss the most common issues related to nvidia gpus and to supplement the documentation in the cuda c programming guide. incompletelu and cholesky preconditioned iterative methodsin this white paper we show how to use the cusparse and cublas libraries to achieve a x speedup over cpu in the incompletelu and cholesky preconditioned iterative methods. we focus on the biconjugate gradient stabilized and conjugate gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms. application notes cuda for tegrathis application note provides an overview of nvidia tegra memory architecture and considerations for porting code from a discrete gpu dgpu attached to an x system to the tegra integrated gpu igpu. it also discusses egl interoperability. compiler sdk libnvvm apithe libnvvm api. libdevice users guidethe libdevice library is an llvm bitcode library that implements common functions for gpu kernels. nvvm irnvvm ir is a compiler ir intermediate representation based on the llvm ir. the nvvm ir is designed to represent gpu compute kernels for example, cuda kernels. highlevel language frontends, like the cuda c compiler frontend, can generate nvvm ir.",
    "depth": 1
  },
  {
    "url": "https://docs.nvidia.com/cuda/cuda-features-archive/index.html",
    "content": "nvidia cuda features archive the list of cuda features by release. . cuda features . compiler .. vs support cuda officially supports the latest vs as host compiler. a separate nsight visual studio installer . must be downloaded from here. a future cuda release will have the nsight visual studio installer with vs support integrated into it. .. new instructions in public ptx new instructions for bit mask creationbmsk, and sign extensionszext, are added to the public ptx isa. you can find documentation for these instructions in the ptx isa guide bmsk and szext. .. unused kernel optimization in cuda , unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. this was an optin feature but in , this feature is enabled by default. as mentioned in the blog, there is an optout flag that can be used in case it becomes necessary for debug purposes or for other special situations. nvcc rdctrue user.cu testlib.a o user xnvlink ignorehostinfo .. new archnative option in addition to the archall and archallmajor options added in cuda , nvcc introduced arch native in cuda update . this archnative option is a convenient way for users to let nvcc determine the right target architecture to compile the cuda device code to based on the gpu installed on the system. this can be particularly helpful for testing when applications are run on the same system they are compiled in. .. generate ptx from nvlink using the following command line, device linker, nvlink will produce ptx as an output in addition to cubin nvcc dlto dlink ptx device linking by nvlink is the final stage in the cuda compilation process. applications that have multiple source translation units have to be compiled in separate compilation mode. lto introduced in cuda allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. however, without the option to output ptx, applications that cared about forward compatibility of device code could not benefit from link time optimization or had to constrain the device code to a single source file. with the option for nvlink that performs lto to generate the output in ptx, customer applications that require forward compatibility across gpu architectures can span across multiple files and can also take advantage of link time optimization. .. bullseye support nvcc compiled source code now works with the code coverage tool bullseye. the code coverage is only for the cpu or the host functions. code coverage for device function is not supported through bullseye. .. int developer tool support in , cuda c support for bit was added. in , developer tools support the datatype as well. with the latest version of libcu, int data datype is supported by math functions. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.",
    "depth": 1
  },
  {
    "url": "https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html",
    "content": "cuda quick start guide minimal firststeps instructions to get cuda running on a standard system. . introduction this guide covers the basic instructions needed to install cuda and verify that a cuda application can run on each supported platform. these instructions are intended to be used on a clean installation of a supported platform. for questions which are not answered in this document, please refer to the windows installation guide and linux installation guide. the cuda installation packages can be found on the cuda downloads page. . windows when installing cuda on windows, you can choose between the network installer and the local installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. for more details, refer to the windows installation guide. . network installer perform the following steps to install cuda and verify the installation. launch the downloaded installer package. read and accept the eula. select next to download and install all components. once the download completes, the installation will begin automatically. once the installation completes, click next to acknowledge the nsight visual studio edition installation summary. click close to close the installer. navigate to the samples nbody directory in open the nbody visual studio solution file for the version of visual studio you have installed, for example, nbodyvs.sln. open the build menu within visual studio and click build solution. navigate to the cuda samples build directory and run the nbody sample. note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. . local installer perform the following steps to install cuda and verify the installation. launch the downloaded installer package. read and accept the eula. select next to install all components. once the installation completes, click next to acknowledge the nsight visual studio edition installation summary. click close to close the installer. navigate to the samples nbody directory in open the nbody visual studio solution file for the version of visual studio you have installed. open the build menu within visual studio and click build solution. navigate to the cuda samples build directory and run the nbody sample. note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. . pip wheels windows nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools these can be installed separately. please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install the nvidiapyindex package, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not uptodate, then use the following command to upgrade these python modules. if these python modules are outofdate then the commands which follow later in this section may fail. py m pip install upgrade setuptools pip wheel you should now be able to install the nvidiapyindex module. py m pip install nvidiapyindex if your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidiapyindex package extraindexurl procedure install the cuda runtime package py m pip install nvidiacudaruntimecu optionally, install additional packages as listed below using the following command py m pip install nvidialibrary metapackages the following metapackages will install the latest version of the named component on windows for the indicated cuda version. cu should be read as cuda. nvidiacudaruntimecu nvidiacudacupticu nvidiacudanvcccu nvidianvmldevcu nvidiacudanvrtccu nvidianvtxcu nvidiacudasanitizerapicu nvidiacublascu nvidiacufftcu nvidiacurandcu nvidiacusolvercu nvidiacusparsecu nvidianppcu nvidianvjpegcu these metapackages install the following packages nvidianvmldevcu nvidiacudanvcccu nvidiacudaruntimecu nvidiacudacupticu nvidiacublascu nvidiacudasanitizerapicu nvidianvtxcu nvidiacudanvrtccu nvidianppcu nvidiacusparsecu nvidiacusolvercu nvidiacurandcu nvidiacufftcu nvidianvjpegcu . conda the conda packages are available at installation to perform a basic install of all cuda toolkit components using conda, run the following command conda install cuda c nvidia uninstallation to uninstall the cuda toolkit using conda, run the following command conda remove cuda . linux cuda on linux can be installed using an rpm, debian, runfile, or conda package, depending on the platform being installed on. . linux x for development on the x architecture. in some cases, x systems may act as host platforms targeting other architectures. see the linux installation guide for more details. .. redhat centos when installing cuda on redhat or centos, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to the linux installation guide. .. rpm installer perform the following steps to install cuda and verify the installation. install epel to satisfy the dkms dependency by following the instructions at epels website. enable optional repos on rhel linux only, execute the following steps to enable optional repositories. on x workstation subscriptionmanager repos enablerhelforxappstreamrpms subscriptionmanager repos enablerhelforxbaseosrpms subscriptionmanager repos enablecodereadybuilderforrhelxrpms install the repository metadata, clean the yum cache, and install cuda sudo rpm install cudarepodistroversion.architecture.rpm sudo rpm erase gpgpubkeyfaaf sudo yum clean expirecache sudo yum install cuda reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. runfile installer perform the following steps to install cuda and verify the installation. disable the nouveau drivers create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo dracut force reboot into runlevel by temporarily adding the number and the word nomodeset to the end of the systems kernel boot parameters. run the installer silently to install with the default selections implies acceptance of the eula sudo sh cudaversionlinux.run silent create an xorg.conf file to use the nvidia gpu for display sudo nvidiaxconfig reboot the system to load the graphical interface sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. fedora when installing cuda on fedora, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to the linux installation guide. .. rpm installer perform the following steps to install cuda and verify the installation. install the rpmfusion free repository to satisfy the akmods dependency su c dnf install nogpgcheck e fedora.noarch.rpm install the repository metadata, clean the dnf cache, and install cuda sudo rpm install cudarepodistroversion.architecture.rpm sudo rpm erase gpgpubkeyfaaf sudo dnf clean expirecache sudo dnf install cuda reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. runfile installer perform the following steps to install cuda and verify the installation. disable the nouveau drivers create a file at usrlibmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo dracut force run the below command sudo grubmkconfig o bootgrubgrub.cfg reboot the system sudo reboot reboot into runlevel by temporarily adding the number and the word nomodeset to the end of the systems kernel boot parameters. run the installer silently to install with the default selections implies acceptance of the eula sudo sh cudaversionlinux.run silent create an xorg.conf file to use the nvidia gpu for display sudo nvidiaxconfig reboot the system to load the graphical interface. set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. suse linux enterprise server when installing cuda on suse linux enterprise server, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to the linux installation guide. .. rpm installer perform the following steps to install cuda and verify the installation. install the repository metadata, refresh the zypper cache, update the gpg key, and install cuda sudo rpm install cudarepodistroversion.architecture.rpm sudo suseconnect product packagehubx sudo zypper refresh sudo rpm erase gpgpubkeyfaaf sudo dnf configmanager addrepo sudo zypper install cuda add the user to the video group sudo usermod a g video username reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the vectoradd sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. runfile installer perform the following steps to install cuda and verify the installation. reboot into runlevel by temporarily adding the number and the word nomodeset to the end of the systems kernel boot parameters. run the installer silently to install with the default selections implies acceptance of the eula sudo sh cudaversionlinux.run silent create an xorg.conf file to use the nvidia gpu for display sudo nvidiaxconfig reboot the system to load the graphical interface sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the vectoradd sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. opensuse when installing cuda on opensuse, you can choose between the runfile installer and the rpm installer. the runfile installer is only available as a local installer. the rpm installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. in the case of the rpm installers, the instructions for the local and network variants are the same. for more details, refer to the linux installation guide. .. rpm installer perform the following steps to install cuda and verify the installation. install the repository metadata, refresh the zypper cache, and install cuda sudo rpm install cudarepodistroversion.architecture.rpm sudo rpm erase gpgpubkeyfaaf sudo zypper refresh sudo zypper install cuda add the user to the video group sudo usermod a g video username reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. runfile installer perform the following steps to install cuda and verify the installation. disable the nouveau drivers create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initrd sudo sbinmkinitrd reboot into runlevel by temporarily adding the number and the word nomodeset to the end of the systems kernel boot parameters. run the installer silently to install with the default selections implies acceptance of the eula sudo sh cudaversionlinux.run silent create an xorg.conf file to use the nvidia gpu for display sudo nvidiaxconfig reboot the system to load the graphical interface sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. amazon linux .. prepare amazon linux perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo dnf install kerneldeveluname r kernelheadersuname r kernelmodulesextrauname r choose an installation method local repo or network repo. .. local repo installation for amazon linux install local repository on file system sudo rpm install cudarepoamznxylocalversion.x.rpm .. network repo installation for amazon linux enable the network repository and clean the dn cache sudo dnf configmanager addrepo sudo dnf clean expirecache .. common installation instructions for amazon linux these instructions apply to both local and network installation for amazon linux. install cuda sdk sudo dnf module install nvidiadriverlatestdkms sudo dnf install cudatoolkit install gpudirect filesystem sudo dnf install nvidiagds add libcuda.so symbolic link, if necessary the libcuda.so library is installed in the usrlib,nvidia directory. for preexisting projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the usrlib, directory. reboot the system sudo reboot perform the postinstallation actions. .. pip wheels linux nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools these can be installed separately. please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install the nvidiapyindex package, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not uptodate, then use the following command to upgrade these python modules. if these python modules are outofdate then the commands which follow later in this section may fail. python m pip install upgrade setuptools pip wheel you should now be able to install the nvidiapyindex module. python m pip install nvidiapyindex if your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidiapyindex package extraindexurl procedure install the cuda runtime package python m pip install nvidiacudaruntimecu optionally, install additional packages as listed below using the following command python m pip install nvidialibrary metapackages the following metapackages will install the latest version of the named component on linux for the indicated cuda version. cu should be read as cuda. nvidiacudaruntimecu nvidiacudacupticu nvidiacudanvcccu nvidianvmldevcu nvidiacudanvrtccu nvidianvtxcu nvidiacudasanitizerapicu nvidiacublascu nvidiacufftcu nvidiacurandcu nvidiacusolvercu nvidiacusparsecu nvidianppcu nvidianvjpegcu nvidiaopenclcu nvidianvjitlinkcu these metapackages install the following packages nvidianvmldevcu nvidiacudanvcccu nvidiacudaruntimecu nvidiacudacupticu nvidiacublascu nvidiacudasanitizerapicu nvidianvtxcu nvidiacudanvrtccu nvidianppcu nvidiacusparsecu nvidiacusolvercu nvidiacurandcu nvidiacufftcu nvidianvjpegcu nvidiaopenclcu nvidianvjitlinkcu .. conda the conda packages are available at installation to perform a basic install of all cuda toolkit components using conda, run the following command conda install cuda c nvidia uninstallation to uninstall the cuda toolkit using conda, run the following command conda remove cuda .. wsl these instructions must be used if you are installing in a wsl environment. do not use the ubuntu instructions in this case. install repository metadata sudo dpkg i cudarepodistroversionarchitecture.deb update the cuda public gpg key sudo aptkey del faaf when installing using the local repo sudo cp varcudarepoubuntulocalcudakeyring.gpg usrsharekeyrings when installing using the network repo wget sudo dpkg i cudakeyring.all.deb pin file to prioritize cuda repository wget sudo mv cudadistro.pin etcaptpreferences.dcudarepositorypin update the apt repository cache and install cuda sudo aptget update sudo aptget install cuda .. ubuntu when installing cuda on ubuntu, you can choose between the runfile installer and the debian installer. the runfile installer is only available as a local installer. the debian installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. in the case of the debian installers, the instructions for the local and network variants are the same. for more details, refer to the linux installation guide. .. debian installer perform the following steps to install cuda and verify the installation. install the repository metadata, update the gpg key, update the aptget cache, and install cuda sudo dpkg install cudarepodistroversion.architecture.deb sudo aptkey del faaf wget sudo dpkg i cudakeyring.all.deb sudo addaptrepository contrib sudo aptget update sudo aptget y install cuda reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. runfile installer perform the following steps to install cuda and verify the installation. disable the nouveau drivers create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo updateinitramfs u reboot into runlevel by temporarily adding the number and the word nomodeset to the end of the systems kernel boot parameters. run the installer silently to install with the default selections implies acceptance of the eula sudo sh cudaversionlinux.run silent create an xorg.conf file to use the nvidia gpu for display sudo nvidiaxconfig reboot the system to load the graphical interface sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. debian when installing cuda on debian , you can choose between the runfile installer and the debian installer. the runfile installer is only available as a local installer. the debian installer is available as both a local installer and a network installer. the network installer allows you to download only the files you need. the local installer is a standalone installer with a large initial download. for more details, refer to the linux installation guide. .. debian installer perform the following steps to install cuda and verify the installation. install the repository metadata, remove old gpg key, install gpg key, update the aptget cache, and install cuda sudo dpkg i cudarepodistroversionarchitecture.deb sudo aptkey adv fetchkeys sudo aptkey del faaf wget sudo dpkg i cudakeyring.all.deb sudo addaptrepository contrib sudo aptget update sudo aptget y install cuda reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. .. runfile installer perform the following steps to install cuda and verify the installation. disable the nouveau drivers create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo updateinitramfs u reboot into runlevel by temporarily adding the number and the word nomodeset to the end of the systems kernel boot parameters. run the installer silently to install with the default selections implies acceptance of the eula sudo sh cudaversionlinux.run silent create an xorg.conf file to use the nvidia gpu for display sudo nvidiaxconfig reboot the system to load the graphical interface sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcudabinpathpath export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath install a writable copy of the samples from then build and run the nbody sample using the linux instructions in note run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.",
    "depth": 1
  },
  {
    "url": "https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html",
    "content": "cuda installation guide for microsoft windows the installation instructions for the cuda toolkit on microsoft windows systems. . introduction cuda is a parallel computing platform and programming model invented by nvidia. it enables dramatic increases in computing performance by harnessing the power of the graphics processing unit gpu. cuda was developed with several design goals in mind provide a small set of extensions to standard programming languages, like c, that enable a straightforward implementation of parallel algorithms. with cuda cc, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. support heterogeneous computation where applications use both the cpu and gpu. serial portions of applications are run on the cpu, and parallel portions are offloaded to the gpu. as such, cuda can be incrementally applied to existing applications. the cpu and gpu are treated as separate devices that have their own memory spaces. this configuration also allows simultaneous computation on the cpu and gpu without contention for memory resources. cudacapable gpus have hundreds of cores that can collectively run thousands of computing threads. these cores have shared resources including a register file and a shared memory. the onchip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. this guide will show you how to install and check the correct operation of the cuda development tools. . system requirements to use cuda on your system, you will need the following installed a cudacapable gpu a supported version of linux with a gcc compiler and toolchain nvidia cuda toolkit available at supported microsoft windows operating systems microsoft windows h microsoft windows hsv microsoft windows h microsoft windows h microsoft windows h microsoft windows server table windows compiler support in cuda compiler ide native x crosscompilation bit on bit c dialect msvc version x visual studio x yes not supported c default, c, c msvc version x visual studio x yes c default, c msvc version x visual studio x rtw and all updates yes c default, c support for visual studio is deprecated in release support for visual studio is deprecated in release . bit compilation native and crosscompilation is removed from cuda and later toolkit. use the cuda toolkit from earlier releases for bit compilation. cuda driver will continue to support running bit application binaries on geforce gpus until ada. ada will be the last architecture with driver support for bit applications. hopper does not support bit applications. support for running x bit applications on x windows is limited to use with cuda driver cuda runtime cudart cuda math library math.h . about this document this document is intended for readers familiar with microsoft windows operating systems and the microsoft visual studio environment. you do not need previous experience with cuda or experience with parallel computation. . installing cuda development tools basic instructions can be found in the quick start guide. read on for more detailed instructions. the setup of cuda development tools on a system running the appropriate version of windows consists of a few simple steps verify the system has a cudacapable gpu. download the nvidia cuda toolkit. install the nvidia cuda toolkit. test that the installed software runs correctly and communicates with the hardware. . verify you have a cudacapable gpu you can verify that you have a cudacapable gpu through the display adapters section in the windows device manager. here you will find the vendor name and model of your graphics cards. if you have an nvidia card that is listed in that gpu is cudacapable. the release notes for the cuda toolkit also contain a list of supported products. the windows device manager can be opened via the following steps open a run window from the start menu run control name microsoft.devicemanager . download the nvidia cuda toolkit the nvidia cuda toolkit is available at choose the platform you are using and one of the following installer formats network installer a minimal installer which later downloads packages required for installation. only the packages selected during the selection phase of the installer are downloaded. this installer is useful for users who want to minimize download time. full installer an installer which contains all the components of the cuda toolkit and does not require any further download. this installer is useful for systems which lack network access and for enterprise deployment. the cuda toolkit installs the cuda driver and tools needed to create, build and run a cuda application as well as libraries, header files, and other resources. download verification the download can be verified by comparing the md checksum posted at with that of the downloaded file. if either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. . install the cuda software before installing the toolkit, you should read the release notes, as they provide details on installation and software functionality. note the driver and toolkit must be installed for cuda to function. if you have not installed a standalone driver, install the driver from the nvidia cuda toolkit. note the installation may fail if windows update starts after the installation has begun. wait until windows update is complete and then try the installation again. graphical installation install the cuda software by executing the cuda installer and following the onscreen prompts. silent installation the installer can be executed in silent mode by executing the package with the s flag. additional parameters can be passed which will install specific subpackages instead of all packages. see the table below for a list of all the subpackage names. table possible subpackage names subpackage name subpackage description toolkit subpackages defaults to cprogram filesnvidia gpu computing toolkitcudav. cudaprofilerapi. cuda profiler api. cudart. cuda runtime libraries. cuobjdump. extracts information from cubin files. cupti. the cuda profiling tools interface for creating profiling and tracing tools that target cuda applications. cuxxfilt. the cuda cu filt demangler tool. demosuite. prebuilt demo applications using cuda. documentation. cuda html and pdf documentation files including the cuda c programming guide, cuda c best practices guide, cuda library documentation, etc. nvcc. cuda compiler. nvdisasm. extracts information from standalone cubin files. nvfatbin. library for creating fatbinaries at runtime. nvjitlink. nvjitlink library. nvmldev. nvml development libraries and headers. nvprof. tool for collecting and viewing cuda application profiling data from the commandline. nvprune. prunes host object files and libraries to only contain device code for the specified targets. nvrtc. nvrtcdev. nvrtc runtime libraries. nvtx. nvtx on windows. opencl. opencl library. visualprofiler. visual profiler. sanitizer. compute sanitizer api. thrust. cuda thrust. cublas. cublasdev. cublas runtime libraries. cufft. cufftdev. cufft runtime libraries. curand. curanddev. curand runtime libraries. cusolver. cusolverdev. cusolver runtime libraries. cusparse. cusparsedev. cusparse runtime libraries. npp. nppdev. npp runtime libraries. nvjpeg. nvjpegdev. nvjpeg libraries. nsightcompute. nsight compute. nsightsystems. nsight systems. nsightvse. installs the nsight visual studio edition plugin in all vs. occupancycalculator. installs the cudaoccupancycalculator.xls tool. visualstudiointegration. installs cuda project wizard and builds customization files in vs. driver subpackages display.driver the nvidia display driver. required to run cuda applications. for example, to install only the compiler and driver components packagename.exe s nvcc. display.driver use the n option if you do not want to reboot automatically after install or uninstall, even if reboot is required. extracting and inspecting the files manually sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. the full installation package can be extracted using a decompression tool which supports the lzma compression method, such as zip or winzip. once extracted, the cuda toolkit files will be in the cudatoolkit folder, and similarily for cuda visual studio integration. within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files. note accessing the files in this manner does not set up any environment settings, such as variables or visual studio integration. this is intended for enterpriselevel deployment. .. uninstalling the cuda software all subpackages can be uninstalled through the windows control panel by using the programs and features widget. . using conda to install the cuda software this section describes the installation and configuration of cuda when using the conda installer. the conda packages are available at .. conda overview the conda installation installs the cuda toolkit. the installation steps are listed below. .. installation to perform a basic install of all cuda toolkit components using conda, run the following command conda install cuda c nvidia .. uninstallation to uninstall the cuda toolkit using conda, run the following command conda remove cuda .. installing previous cuda releases all conda packages released under a specific cuda version are labeled with that release version. to install a previous version, include that label in the install command such as conda install cuda c nvidialabelcuda. note some cuda releases do not move to new versions of all installable components. when this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as conda install cuda c nvidialabelcuda. c nvidialabelcuda. this example will install all packages released as part of cuda .. . use a suitable driver model on windows and later, the operating system provides two driver models under which the nvidia driver may operate the wddm driver model is used for display devices. the tesla compute cluster tcc mode of the nvidia driver is available for nondisplay devices such as nvidia tesla gpus and the geforce gtx titan gpus it uses the windows wdm driver model. tcc is enabled by default on most recent nvidia tesla gpus. to check which driver mode is in use andor to switch driver modes, use the nvidiasmi tool that is included with the nvidia driver installation see nvidiasmi h for details. note keep in mind that when tcc mode is enabled for a particular gpu, that gpu cannot be used as a display device. note nvidia geforce gpus excluding geforce gtx titan gpus do not support tcc mode. . verify the installation before continuing, it is important to verify that the cuda toolkit can find and communicate correctly with the cudacapable hardware. to do this, you need to compile and run some of the included sample programs. .. running the compiled examples the version of the cuda toolkit can be checked by running nvcc v in a command prompt window. you can display a command prompt window by going to start all programs accessories command prompt cuda samples are located in to use the samples, clone the project, build the samples, and run them using the instructions on the github page. to verify a correct configuration of the hardware and software, it is highly recommended that you build and run the devicequery sample program. the sample can be built using the provided vs solution files in the devicequery folder. this assumes that you used the default installation directory structure. if cuda is installed and configured correctly, the output should look similar to figure . figure valid results from devicequery cuda sample the exact appearance and the output lines might be different on your system. the important outcomes are that a device was found, that the devices match what is installed in your system, and that the test passed. if a cudacapable device and the cuda driver are installed but devicequery reports that no cudacapable devices are present, ensure the deivce and driver are properly installed. running the bandwidthtest program, located in the same directory as devicequery above, ensures that the system and the cudacapable device are able to communicate correctly. the output should resemble figure . figure valid results from bandwidthtest cuda sample the device name second line and the bandwidth numbers vary from system to system. the important items are the second line, which confirms a cuda device was found, and the secondtolast line, which confirms that all necessary tests passed. if the tests do not pass, make sure you do have a cudacapable nvidia gpu on your system and make sure it is properly installed. to see a graphical representation of what cuda can do, run the particles sample at . pip wheels nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools these can be installed separately. please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install the nvidiapyindex package, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not uptodate, then use the following command to upgrade these python modules. if these python modules are outofdate then the commands which follow later in this section may fail. py m pip install upgrade setuptools pip wheel you should now be able to install the nvidiapyindex module. py m pip install nvidiapyindex if your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidiapyindex package extraindexurl procedure install the cuda runtime package py m pip install nvidiacudaruntimecu optionally, install additional packages as listed below using the following command py m pip install nvidialibrary metapackages the following metapackages will install the latest version of the named component on windows for the indicated cuda version. cu should be read as cuda. nvidiacublascu nvidiacudaruntimecu nvidiacudacupticu nvidiacudanvcccu nvidiacudanvrtccu nvidiacudasanitizerapicu nvidiacufftcu nvidiacurandcu nvidiacusolvercu nvidiacusparsecu nvidianppcu nvidianvfatbincu nvidianvjitlinkcu nvidianvjpegcu nvidianvmldevcu nvidianvtxcu nvidiaopenclcu these metapackages install the following packages nvidiacublascu nvidiacudaruntimecu nvidiacudacupticu nvidiacudanvcccu nvidiacudanvrtccu nvidiacudasanitizerapicu nvidiacufftcu nvidiacurandcu nvidiacusolvercu nvidiacusparsecu nvidianppcu nvidianvfatbincu nvidianvjitlinkcu nvidianvjpegcu nvidianvmldevcu nvidianvtxcu nvidiaopenclcu . compiling cuda programs the project files in the cuda samples have been designed to provide simple, oneclick builds of the programs that include all source code. to build the windows projects for release or debug mode, use the provided .sln solution files for microsoft visual studio deprecated in cuda , , , or . you can use either the solution files located in each of the examples directories in . compiling sample projects the bandwidthtest project is a good sample project to build and run. it is located in if you elected to use the default installation location, the output is placed in cuda samplesv.binwinrelease. build the program using the appropriate solution file and run the executable. if all works correctly, the output should be similar to figure . . sample projects the sample projects come in two configurations debug and release where release contains no debugging information and different visual studio projects. a few of the example projects require some additional setup. these sample projects also make use of the cudapath environment variable to locate where the cuda toolkit and the associated .props files are. the environment variable is set automatically using the build customization cuda .props file, and is installed automatically as part of the cuda toolkit installation process. table cuda visual studio .props locations visual studio cuda .props file install directory visual studio deprecated cprogram files xmsbuildmicrosoft.cppv.vbuildcustomizations visual studio visual studio install dircommonidevcvctargetsbuildcustomizations visual studio cprogram files xmicrosoft visual studioprofessionalmsbuildmicrosoftvcvbuildcustomizations visual studio cprogram filesmicrosoft visual studioprofessionalmsbuildmicrosoftvcvbuildcustomizations you can reference this cuda .props file when building your own cuda applications. . build customizations for new projects when creating a new cuda application, the visual studio project file must be configured to include cuda build customizations. to accomplish this, click file new project nvidia cuda, then select a template for your cuda toolkit version. for example, selecting the cuda runtime template will configure your project for use with the cuda toolkit. the new project is technically a c project .vcxproj that is preconfigured to use nvidias build customizations. all standard capabilities of visual studio c projects will be available. to specify a custom cuda toolkit location, under cuda cc, select common, and set the cuda toolkit custom dir field as desired. note that the selected toolkit must match the version of the build customizations. note a supported version of msvc must be installed to use this feature. . build customizations for existing projects when adding cuda acceleration to existing applications, the relevant visual studio project files must be updated to include cuda build customizations. this can be done using one of the following two methods open the visual studio project, right click on the project name, and select build dependencies build customizations, then select the cuda toolkit version you would like to target. alternatively, you can configure your project always to build with the most recently installed version of the cuda toolkit. first add a cuda build customization to your project as above. then, right click on the project name and select properties. under cuda cc, select common, and set the cuda toolkit custom dir field to cudapath . note that the cudapath environment variable is set by the installer. while option will allow your project to automatically use any new cuda toolkit version you may install in the future, selecting the toolkit version explicitly as in option is often better in practice, because if there are new cuda configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using option . if you use the cudapath environment variable to target a version of the cuda toolkit for building, and you perform an installation or uninstallation of any version of the cuda toolkit, you should validate that the cudapath environment variable points to the correct installation directory of the cuda toolkit for your purposes. you can access the value of the cudapath environment variable via the following steps open a run window from the start menu. run control sysdm.cpl select the advanced tab at the top of the window. click environment variables at the bottom of the window. files which contain cuda code must be marked as a cuda cc file. this can done when adding the file by right clicking the project you wish to add the file to, selecting add new item, selecting nvidia cuda codecuda cc file, and then selecting the file you wish to add. for advanced users, if you wish to try building your project against a newer cuda toolkit without making changes to any of your project files, go to the visual studio command prompt, change the current directory to the location of your project, and execute a command such as the following msbuild projectname.extension trebuild pcudatoolkitdirdrivepathtonewtoolkit . additional considerations now that you have cudacapable hardware and the nvidia cuda toolkit installed, you can examine and enjoy the numerous included programs. to begin using cuda to accelerate the performance of your own applications, consult the cuda c programming guide, located in the cuda toolkit documentation directory. a number of helpful development tools are included in the cuda toolkit or are available for download from the nvidia developer zone to assist you as you develop your cuda programs, such as nvidia nsight visual studio edition, and nvidia visual profiler. for technical support on programming questions, consult and participate in the developer forums at . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.",
    "depth": 2
  },
  {
    "url": "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html",
    "content": "cuda c best practices guide the programming guide to using the cuda toolkit to obtain the best performance from nvidia gpus. . preface . what is this document? this best practices guide is a manual to help developers obtain the best performance from nvidia cuda gpus. it presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for cudacapable gpu architectures. while the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored. as a result, it is recommended that firsttime readers proceed through the guide sequentially. this approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later. . who should read this guide? the discussions in this guide all use the c programming language, so you should be comfortable reading c code. this guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the cuda website the following documents are especially important resources cuda installation guide cuda c programming guide cuda toolkit reference manual in particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the cuda toolkit if not, please refer to the relevant cuda installation guide for your platform and that you have a basic familiarity with the cuda c programming language and environment if not, please refer to the cuda c programming guide. . assess, parallelize, optimize, deploy this guide introduces the assess, parallelize, optimize, deployapod design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from gpu acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible. apod is a cyclical process initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production. .. assess for an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate gpu acceleration. by understanding the endusers requirements and constraints and by applying amdahls and gustafsons laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application. .. parallelize having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. depending on the original code, this can be as simple as calling into an existing gpuoptimized library such as cublas, cufft, or thrust, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. on the other hand, some applications designs will require some amount of refactoring to expose their inherent parallelism. as even cpu architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the cuda family of parallel programming languages cuda c, cuda fortran, etc. aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on cudacapable gpus designed for maximum parallel throughput. .. optimize after each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. however, as with apod as a whole, program optimization is an iterative process identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat, meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. instead, strategies can be applied incrementally as they are learned. optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to finetuning floatingpoint operation sequences. the available profiling tools are invaluable for guiding this process, as they can help suggest a nextbest course of action for the developers optimization efforts and provide references into the relevant portions of the optimization section of this guide. .. deploy having completed the gpu acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. this is important for a number of reasons for example, it allows the user to profit from their investment as early as possible the speedup may be partial but is still valuable, and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application. . recommendations and best practices throughout this guide, specific recommendations are made regarding the design and implementation of cuda c code. these recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. actions that present substantial improvements for most cuda applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority. before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied. this approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization. the criteria of benefit and scope for establishing priority will vary depending on the nature of the program. in this guide, they represent a typical case. your code might reflect different priority factors. regardless of this possibility, it is good practice to verify that no higherpriority recommendations have been overlooked before undertaking lowerpriority items. note code samples throughout the guide omit error checking for conciseness. production code should, however, systematically check the error code returned by each api call and check for failures in kernel launches by calling cudagetlasterror. . assessing your application from supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance. the core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network. as a result, all modern processors require parallel code in order to achieve good utilization of their computational power. while processors are evolving to expose more finegrained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarsegrained parallel codes for example, where the data is decomposed into regions processed in parallel, with subregions shared using mpi. in order to profit from any modern processor architecture, gpus included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future. . heterogeneous computing cuda programming involves running code on two different platforms concurrently a host system with one or more cpus and one or more cudaenabled nvidia gpu devices. while nvidia gpus are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. this capability makes them well suited to computations that can leverage parallel execution. however, the device is based on a distinctly different design from the host system, and its important to understand those differences and how they determine the performance of cuda applications in order to use cuda effectively. . differences between host and device the primary differences are in threading model and in separate physical memories threading resources execution pipelines on host systems can support a limited number of concurrent threads. for example, servers that have two core processors can run only threads concurrently or small multiple of that if the cpus support simultaneous multithreading. by comparison, the smallest executable unit of parallelism on a cuda device comprises threads termed a warp of threads. modern nvidia gpus can support up to active threads concurrently per multiprocessor see features and specifications of the cuda c programming guide on gpus with multiprocessors, this leads to more than , concurrently active threads. threads threads on a cpu are generally heavyweight entities. the operating system must swap threads on and off cpu execution channels to provide multithreading capability. context switches when two threads are swapped are therefore slow and expensive. by comparison, threads on gpus are extremely lightweight. in a typical system, thousands of threads are queued up for work in warps of threads each. if the gpu must wait on one warp of threads, it simply begins executing work on another. because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among gpu threads. resources stay allocated to each thread until it completes its execution. in short, cpu cores are designed to minimize latency for a small number of threads at a time each, whereas gpus are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput. ram the host system and the device each have their own distinct attached physical memories . as the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in what runs on a cudaenabled device?. these are the primary hardware differences between cpu hosts and gpu devices with respect to parallel programming. other differences are discussed as they arise elsewhere in this document. applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best sequential work on the host and parallel work on the device. . what runs on a cudaenabled device? the following issues should be considered when determining what parts of an application to run on the device the device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel. this typically involves arithmetic on large data sets such as matrices where the same operation can be performed across thousands, if not millions, of elements at the same time. this is a requirement for good performance on cuda the software must use a large number generally thousands or tens of thousands of concurrent threads. the support for running numerous threads in parallel derives from cudas use of a lightweight threading model described above. to use cuda, data values must be transferred from the host to the device. these transfers are costly in terms of performance and should be minimized. see data transfer between host and device. this cost has several ramifications the complexity of operations should justify the cost of moving data to and from the device. code that transfers data for brief use by a small number of threads will see little or no performance benefit. the ideal scenario is one in which many threads perform a substantial amount of work. for example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit. the issue here is the number of operations performed per data element transferred. for the preceding procedure, assuming matrices of size nxn, there are n operations additions and n elements transferred, so the ratio of operations to elements transferred is or o. performance benefits can be more readily achieved when this ratio is higher. for example, a matrix multiplication of the same matrices requires n operations multiplyadd, so the ratio of operations to elements transferred is on, in which case the larger the matrix the greater the performance benefit. the types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions. it is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device. data should be kept on the device as long as possible. because transfers should be minimized, programs that run multiple kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations. so, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in some subsequent calculation, the matrix addition should be performed locally on the device. this approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host. even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory. data transfer between host and device provides further details, including the measurements of bandwidth between the host and the device versus within the device proper. for best performance, there should be some coherence in memory access by adjacent threads running on the device. certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation. data that cannot be laid out so as to enable coalescing, or that doesnt have enough locality to use the l or texture caches effectively, will tend to see lesser speedups when used in computations on gpus. a noteworthy exception to this are completely random memory access patterns. in general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency. however, compared to cache based architectures, like cpus, latency hiding architectures, like gpus, tend to cope better with completely random memory access patterns. on systems on a chip with integrated gpus, such as nvidia tegra, host and device memory are physically the same, but there is still a logical distinction between host and device memory. see the application note on cuda for tegra for details. . application profiling . profile many codes accomplish a significant portion of the work with a relatively small amount of code. using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization. .. creating the profile there are many possible approaches to profiling the code, but in all cases the objective is the same to identify the function or functions in which the application is spending most of its execution time. note high priority to maximize developer productivity, profile the application to determine hotspots and bottlenecks. the most important consideration with any profiling activity is to ensure that the workload is realistic i.e., that information gained from the test and decisions based upon that information are relevant to real data. using unrealistic workloads can lead to suboptimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions. there are a number of tools that can be used to generate the profile. the following example is based on gprof, which is an opensource profiler for linux platforms from the gnu binutils collection. gcc o g pg myprog.c gprof .a.out profile.txt each sample counts as seconds. cumulative self self total time seconds seconds calls mscall mscall name gentimestep calcstats calcsummarydata write mcount tzset tolower strlen strchr main memcpy print profil report .. identifying hotspots in the example above, we can clearly see that the function gentimestep takes onethird of the total running time of the application. this should be our first candidate function for parallelization. understanding scaling discusses the potential benefit we might expect from such parallelization. it is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as calcstats and calcsummarydata. parallelizing these functions as well should increase our speedup potential. however, since apod is a cyclical process, we might opt to parallelize these functions in a subsequent apod pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes. .. understanding scaling the amount of performance benefit an application will realize by running on cuda depends entirely on the extent to which it can be parallelized. code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device. note high priority to get the maximum benefit from cuda, focus first on finding ways to parallelize sequential code. by understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy. strong scaling and amdahls law describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size. weak scaling and gustafsons law describes weak scaling, where the speedup is attained by growing the problem size. in many applications, a combination of strong and weak scaling is desirable. .. strong scaling and amdahls law strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system. an application that exhibits linear strong scaling has a speedup equal to the number of processors used. strong scaling is usually equated with amdahls law, which specifies the maximum speedup that can be expected by parallelizing portions of a serial program. essentially, it states that the maximum speedup s of a program is s frac p fracpn here p is the fraction of the total serial execution time taken by the portion of code that can be parallelized and n is the number of processors over which the parallel portion of the code runs. the larger n isthat is, the greater the number of processors, the smaller the pn fraction. it can be simpler to view n as a very large number, which essentially transforms the equation into s p. now, if of the running time of a sequential program is parallelized, the maximum speedup over serial code is . in reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some degree of strong scaling. for most purposes, the key point is that the larger the parallelizable portion p is, the greater the potential speedup. conversely, if p is a small number meaning that the application is not substantially parallelizable, increasing the number of processors n does little to improve performance. therefore, to get the largest speedup for a fixed problem size, it is worthwhile to spend effort on increasing p, maximizing the amount of code that can be parallelized. .. weak scaling and gustafsons law weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size per processor i.e., where the overall problem size increases as the number of processors is increased. weak scaling is often equated with gustafsons law, which states that in practice, the problem size scales with the number of processors. because of this, the maximum speedup s of a program is s n p n here p is the fraction of the total serial execution time taken by the portion of code that can be parallelized and n is the number of processors over which the parallel portion of the code runs. another way of looking at gustafsons law is that it is not the problem size that remains constant as we scale up the system but rather the execution time. note that gustafsons law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem. .. applying strong and weak scaling understanding which type of scaling is most applicable to an application is an important part of estimating speedup. for some applications the problem size will remain constant and hence only strong scaling is applicable. an example would be modeling how two molecules interact with each other, where the molecule sizes are fixed. for other applications, the problem size will grow to fill the available processors. examples include modeling fluids or structures as meshes or grids and some monte carlo simulations, where increasing the problem size provides increased accuracy. having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either amdahls or gustafsons law to determine an upper bound for the speedup. . parallelizing your application having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. depending on the original code, this can be as simple as calling into an existing gpuoptimized library such as cublas, cufft, or thrust, or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. on the other hand, some applications designs will require some amount of refactoring to expose their inherent parallelism. as even cpu architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the cuda family of parallel programming languages cuda c, cuda fortran, etc. aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on cudacapable gpus designed for maximum parallel throughput. . getting started there are several key strategies for parallelizing sequential code. while the details of how to apply these strategies to a particular application is a complex and problemspecific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore cpus or for use on cuda gpus. . parallel libraries the most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf. the cuda toolkit includes a number of such libraries that have been finetuned for nvidia cuda gpus, such as cublas, cufft, and so on. the key here is that libraries are most useful when they match well with the needs of the application. applications already using other blas libraries can often quite easily switch to cublas, for example, whereas applications that do little to no linear algebra will have little use for cublas. the same goes for other cuda toolkit libraries cufft has an interface similar to that of fftw, etc. also of note is the thrust library, which is a parallel c template library similar to the c standard template library. thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. by describing your computation in terms of these highlevel abstractions you provide thrust with the freedom to select the most efficient implementation automatically. as a result, thrust can be utilized in rapid prototyping of cuda applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial. . parallelizing compilers another common approach to parallelization of sequential codes is to make use of parallelizing compilers. often this means the use of directivesbased approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself. by exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture. the openacc standard provides a set of compiler directives to specify loops and regions of code in standard c, c and fortran that should be offloaded from a host cpu to an attached accelerator such as a cuda gpu. the details of managing the accelerator device are handled implicitly by an openaccenabled compiler and runtime. see for details. . coding to expose parallelism for applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as cuda c that integrate seamlessly with existing sequential code are essential. once we have located a hotspot in our applications profile assessment and determined that custom code is the best approach, we can use cuda c to expose the parallelism in that portion of our code as a cuda kernel. we can then launch this kernel onto the gpu and retrieve the results without requiring major rewrites to the rest of our application. this approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code. more difficult to parallelize are applications with a very flat profile i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base. for the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, cpu and gpu alike, so it is well worth the effort should it become necessary. . getting the right answer obtaining the right answer is clearly the principal goal of all computation. on parallel systems, it is possible to run into difficulties not typically found in traditional serialoriented programming. these include threading issues, unexpected values due to the way floatingpoint values are computed, and challenges arising from differences in the way cpu and gpu processors operate. this chapter examines issues that can affect the correctness of returned data and points to appropriate solutions. . verification .. reference comparison a key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous knowngood reference outputs from representative inputs can be compared to new results. after each change is made, ensure that the results match using whatever criteria apply to the particular algorithm. some will expect bitwise identical results, which is not always possible, especially where floatingpoint arithmetic is concerned see numerical accuracy and precision regarding numerical accuracy. for other algorithms, implementations may be considered correct if they match the reference within some small epsilon. note that the process used for validating numerical results can easily be extended to validate performance results as well. we want to ensure that each change we make is correct and that it improves performance and by how much. checking these things frequently as an integral part of our cyclical apod process will help ensure that we achieve the desired results as rapidly as possible. .. unit testing a useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level. for example, we can write our cuda kernels as a collection of many short device functions rather than one large monolithic global function each device function can be tested independently before hooking them all together. for example, many kernels have complex addressing logic for accessing memory in addition to their actual computation. if we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts. note that the cuda compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least write something out to global memory as a result of our addressing logic in order to successfully apply this strategy. going a step further, if most functions are defined as host device rather than just device functions, then these functions can be tested on both the cpu and the gpu, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results. if there are differences, then those differences will be seen early and can be understood in the context of a simple function. as a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both cpu and gpu execution paths in our application if the bulk of the work of our cuda kernels is done in host device functions, we can easily call those functions from both the host code and the device code without duplication. . debugging cudagdb is a port of the gnu debugger that runs on linux and mac see the nvidia nsight visual studio edition for microsoft windows , windows hpc server , windows , and windows is available as a free plugin for microsoft visual studio see several thirdparty debuggers support cuda debugging as well see for more details. . numerical accuracy and precision incorrect or unexpected results arise principally from issues of floatingpoint accuracy due to the way floatingpoint values are computed and stored. the following sections explain the principal items of interest. other peculiarities of floatingpoint arithmetic are presented in features and technical specifications of the cuda c programming guide as well as in a whitepaper and accompanying webinar on floatingpoint precision and performance available from .. single vs. double precision devices of compute capability and higher provide native support for doubleprecision floatingpoint values that is, values bits wide. results obtained using doubleprecision arithmetic will frequently differ from the same operation performed via singleprecision arithmetic due to the greater precision of the former and due to rounding issues. therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact. .. floating point math is not associative each floatingpoint arithmetic operation involves a certain amount of rounding. consequently, the order in which arithmetic operations are performed is important. if a, b, and c are floatingpoint values, abc is not guaranteed to equal abc as it is in symbolic math. when you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results. this limitation is not specific to cuda, but an inherent part of parallel computation on floatingpoint values. .. ieee compliance all cuda compute devices follow the ieee standard for binary floatingpoint representation, with some small exceptions. these exceptions, which are detailed in features and technical specifications of the cuda c programming guide, can lead to results that differ from ieee values computed on the host system. one of the key differences is the fused multiplyadd fma instruction, which combines multiplyadd operations into a single instruction execution. its result will often differ slightly from results obtained by doing the two operations separately. .. x bit computations x processors can use an bit double extended precision math when performing floatingpoint calculations. the results of these calculations can frequently differ from pure bit operations performed on the cuda device. to get a closer match between values, set the x host processor to use regular double or single precision bits and bits, respectively. this is done with the fldcw x assembly instruction or the equivalent operating system api. . optimizing cuda applications after each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. however, as with apod as a whole, program optimization is an iterative process identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat, meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. instead, strategies can be applied incrementally as they are learned. optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to finetuning floatingpoint operation sequences. the available profiling tools are invaluable for guiding this process, as they can help suggest a nextbest course of action for the developers optimization efforts and provide references into the relevant portions of the optimization section of this guide. . performance metrics when attempting to optimize cuda code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement. this chapter discusses how to correctly measure performance using cpu timers and cuda events. it then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses. . timing cuda calls and kernel executions can be timed using either cpu or gpu timers. this section examines the functionality, advantages, and pitfalls of both approaches. .. using cpu timers any cpu timer can be used to measure the elapsed time of a cuda call or kernel execution. the details of various cpu timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide. when using cpu timers, it is critical to remember that many cuda api functions are asynchronous that is, they return control back to the calling cpu thread prior to completing their work. all kernel launches are asynchronous, as are memorycopy functions with the async suffix on their names. therefore, to accurately measure the elapsed time for a particular call or sequence of cuda calls, it is necessary to synchronize the cpu thread with the gpu by calling cudadevicesynchronize immediately before starting and stopping the cpu timer. cudadevicesynchronizeblocks the calling cpu thread until all cuda calls previously issued by the thread are completed. although it is also possible to synchronize the cpu thread with a particular stream or event on the gpu, these synchronization functions are not suitable for timing code in streams other than the default stream. cudastreamsynchronize blocks the cpu thread until all cuda calls previously issued into the given stream have completed. cudaeventsynchronize blocks until a given event in a particular stream has been recorded by the gpu. because the driver may interleave execution of cuda calls from other nondefault streams, calls in other streams may be included in the timing. because the default stream, stream , exhibits serializing behavior for work on the device an operation in the default stream can begin only after all preceding calls in any stream have completed and no subsequent operation in any stream can begin until it finishes, these functions can be used reliably for timing in the default stream. be aware that cputogpu synchronization points such as those mentioned in this section imply a stall in the gpus processing pipeline and should thus be used sparingly to minimize their performance impact. .. using cuda gpu timers the cuda event api provides calls that create and destroy events, record events including a timestamp, and convert timestamp differences into a floatingpoint value in milliseconds. how to time code using cuda events illustrates their use. how to time code using cuda events cudaeventt start, stop float time cudaeventcreatestart cudaeventcreatestop cudaeventrecord start, kernelgrid,threads dodata, didata, sizex, sizey, numreps cudaeventrecord stop, cudaeventsynchronize stop cudaeventelapsedtime time, start, stop cudaeventdestroy start cudaeventdestroy stop here cudaeventrecord is used to place the start and stop events into the default stream, stream . the device will record a timestamp for the event when it reaches that event in the stream. the cudaeventelapsedtime function returns the time elapsed between the recording of the start and stop events. this value is expressed in milliseconds and has a resolution of approximately half a microsecond. like the other calls in this listing, their specific operation, parameters, and return values are described in the cuda toolkit reference manual. note that the timings are measured on the gpu clock, so the timing resolution is operatingsystemindependent. . bandwidth bandwidth the rate at which data can be transferred is one of the most important gating factors for performance. almost all changes to code should be made in the context of how they affect bandwidth. as described in memory optimizations of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors. to measure performance accurately, it is useful to calculate theoretical and effective bandwidth. when the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it. note high priority use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits. .. theoretical bandwidth calculation theoretical bandwidth can be calculated using hardware specifications available in the product literature. for example, the nvidia tesla v uses hbm double data rate ram with a memory clock rate of mhz and a bitwide memory interface. using these data items, the peak theoretical memory bandwidth of the nvidia tesla v is gbs left. left times right. times times right div textgbs in this calculation, the memory clock rate is converted in to hz, multiplied by the interface width divided by , to convert bits to bytes and multiplied by due to the double data rate. finally, this product is divided by to convert the result to gbs. note some calculations use instead of for the final calculation. in such a case, the bandwidth would be gibs. it is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid. note on gpus with gddr memory with ecc enabled the available dram is reduced by to allow for the storage of ecc bits. fetching ecc bits for each memory transaction also reduced the effective bandwidth by approximately compared to the same gpu with ecc disabled, though the exact impact of ecc on bandwidth can be higher and depends on the memory access pattern. hbm memories, on the other hand, provide dedicated ecc resources, allowing overheadfree ecc protection. .. effective bandwidth calculation effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program. to do so, use this equation texteffective bandwidth left left br bw right div right div texttime here, the effective bandwidth is in units of gbs, br is the number of bytes read per kernel, bw is the number of bytes written per kernel, and time is given in seconds. for example, to compute the effective bandwidth of a x matrix copy, the following formula could be used texteffective bandwidth left left times times right div right div texttime the number of elements is multiplied by the size of each element bytes for a float, multiplied by because of the read and write, divided by or , to obtain gb of memory transferred. this number is divided by the time in seconds to obtain gbs. .. throughput reported by visual profiler for devices with compute capability of or greater, the visual profiler can be used to collect several different memory throughput measures. the following throughput metrics can be displayed in the details or detail graphs view requested global load throughput requested global store throughput global load throughput global store throughput dram read throughput dram write throughput the requested global load throughput and requested global store throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under effective bandwidth calculation. because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel. for global memory accesses, this actual throughput is reported by the global load throughput and global store throughput values. its important to note that both numbers are useful. the actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses see coalesced access to global memory. for global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the global memory load efficiency and global memory store efficiency metrics. as an exception, scattered writes to hbm see some overhead from ecc but much less than the overhead with similar access patterns on eccprotected gddr memory. . memory optimizations memory optimizations are the most important area for performance. the goal is to maximize the use of the hardware by maximizing bandwidth. bandwidth is best served by using as much fast memory and as little slowaccess memory as possible. this chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively. . data transfer between host and device the peak theoretical bandwidth between the device memory and the gpu is much higher gbs on the nvidia tesla v, for example than the peak theoretical bandwidth between host memory and device memory gbs on the pcie x gen. hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the gpu that do not demonstrate any speedup compared with running them on the host cpu. note high priority minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host cpu. intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory. also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing noncontiguous regions of memory into a contiguous buffer and then unpacking after the transfer. finally, higher bandwidth between the host and the device is achieved when using pagelocked or pinned memory, as discussed in the cuda c programming guide and the pinned memory section of this document. .. pinned memory pagelocked or pinned memory transfers attain the highest bandwidth between the host and the device. on pcie x gen cards, for example, pinned memory can attain roughly gbs transfer rates. pinned memory is allocated using the cudahostalloc functions in the runtime api. the bandwidthtest cuda sample shows how to use these functions as well as how to measure memory transfer performance. for regions of system memory that have already been preallocated, cudahostregister can be used to pin the memory onthefly without the need to allocate a separate buffer and copy the data into it. pinned memory should not be overused. excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance. furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters. .. asynchronous and overlapping transfers with computation data transfers between the host and the device using cudamemcpy are blocking transfers that is, control is returned to the host thread only after the data transfer is complete. the cudamemcpyasync function is a nonblocking variant of cudamemcpy in which control is returned immediately to the host thread. in contrast with cudamemcpy, the asynchronous transfer version requires pinned host memory see pinned memory, and it contains an additional argument, a stream id. a stream is simply a sequence of operations that are performed in order on the device. operations in different streams can be interleaved and in some cases overlapped a property that can be used to hide data transfers between the host and the device. asynchronous transfers enable overlap of data transfers with computation in two different ways. on all cudaenabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations. for example, overlapping computation and data transfers demonstrates how host computation in the routine cpufunction is performed while data is transferred to the device and a kernel using the device is executed. overlapping computation and data transfers cudamemcpyasyncad, ah, size, cudamemcpyhosttodevice, kernelgrid, blockad cpufunction the last argument to the cudamemcpyasync function is the stream id, which in this case uses the default stream, stream . the kernel also uses the default stream, and it will not begin execution until the memory copy completes therefore, no explicit synchronization is needed. because the memory copy and the kernel both return control to the host immediately, the host function cpufunction overlaps their execution. in overlapping computation and data transfers, the memory copy and kernel execution occur sequentially. on devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device. whether a device has this capability is indicated by the asyncenginecount field of the cudadeviceprop structure or listed in the output of the devicequery cuda sample. on devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, nondefault streams streams with nonzero stream ids. nondefault streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device in any stream have completed, and no operation on the device in any stream commences until they are finished. concurrent copy and execute illustrates the basic technique. concurrent copy and execute cudastreamcreatestream cudastreamcreatestream cudamemcpyasyncad, ah, size, cudamemcpyhosttodevice, stream kernelgrid, block, , streamotherdatad in this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of the cudamemcpyasync call and the kernels execution configuration. concurrent copy and execute demonstrates how to overlap kernel execution with asynchronous data transfer. this technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives. sequential copy and execute and staged concurrent copy and execute demonstrate this. they produce equivalent results. the first segment shows the reference sequential implementation, which transfers and operates on an array of n floats where n is assumed to be evenly divisible by nthreads. sequential copy and execute cudamemcpyad, ah, nsizeoffloat, dir kernelnnthreads, nthreadsad staged concurrent copy and execute shows how the transfer and kernel execution can be broken up into nstreams stages. this approach permits some overlapping of the data transfer and execution. staged concurrent copy and execute sizensizeoffloatnstreams for i instreams i offset innstreams cudamemcpyasyncadoffset, ahoffset, size, dir, streami kernelnnthreadsnstreams, nthreads, , streamiadoffset in staged concurrent copy and execute, it is assumed that n is evenly divisible by nthreadsnstreams. because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete. current gpus can simultaneously process asynchronous data transfers and execute kernels. gpus with a single copy engine can perform one asynchronous data transfer and execute kernels whereas gpus with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels. the number of copy engines on a gpu is given by the asyncenginecount field of the cudadeviceprop structure, which is also listed in the output of the devicequery cuda sample. it should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous cuda calls complete. it will not allow any other cuda call to begin until it has completed. a diagram depicting the timeline of execution for the two code segments is shown in figure , and nstreams is equal to for staged concurrent copy and execute in the bottom half of the figure. timeline comparison for copy and kernel execution top sequential bottom concurrent for this example, it is assumed that the data transfer and kernel execution times are comparable. in such cases, and when the execution time te exceeds the transfer time tt, a rough estimate for the overall time is te ttnstreams for the staged version versus te tt for the sequential version. if the transfer time exceeds the execution time, a rough estimate for the overall time is tt tenstreams. .. zero copy zero copy is a feature that was added in version of the cuda toolkit. it enables gpu threads to directly access host memory. for this purpose, it requires mapped pinned nonpageable memory. on integrated gpus i.e., gpus with the integrated field of the cuda device properties structure set to , mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated gpu and cpu memory are physically the same. on discrete gpus, mapped pinned memory is advantageous only in certain cases. because the data is not cached on the gpu, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced. zero copy can be used in place of streams because kerneloriginated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams. note low priority use zerocopy operations on integrated gpus for cuda toolkit version and later. the host code in zerocopy host code shows how zero copy is typically set up. zerocopy host code float ah, amap ... cudagetdevicepropertiesprop, if prop.canmaphostmemory exit cudasetdeviceflagscudadevicemaphost cudahostallocah, nbytes, cudahostallocmapped cudahostgetdevicepointeramap, ah, kernelgridsize, blocksizeamap in this code, the canmaphostmemory field of the structure returned by cudagetdeviceproperties is used to check that the device supports mapping host memory to the devices address space. pagelocked memory mapping is enabled by calling cudasetdeviceflags with cudadevicemaphost. note that cudasetdeviceflags must be called prior to setting a device or making a cuda call that requires state that is, essentially, before a context is created. pagelocked mapped host memory is allocated using cudahostalloc, and the pointer to the mapped device address space is obtained via the function cudahostgetdevicepointer. in the code in zerocopy host code, kernel can reference the mapped pinned host memory using the pointer amap in exactly the same was as it would if amap referred to a location in device memory. note mapped pinned host memory allows you to overlap cpugpu memory transfers with computation while avoiding the use of cuda streams. but since any repeated access to such memory areas causes repeated cpugpu transfers, consider creating a second area in device memory to manually cache the previously read host memory data. .. unified virtual addressing devices of compute capability and later support a special addressing mode called unified virtual addressing uva on bit linux and windows. with uva, the host memory and the device memories of all installed supported devices share a single virtual address space. prior to uva, an application had to keep track of which pointers referred to device memory and for which device and which referred to host memory as a separate bit of metadata or as hardcoded information in the program for each pointer. using uva, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using cudapointergetattributes. under uva, pinned host memory allocated with cudahostalloc will have identical host and device pointers, so it is not necessary to call cudahostgetdevicepointer for such allocations. host memory allocations pinned afterthefact via cudahostregister, however, will continue to have different device pointers than their host pointers, so cudahostgetdevicepointer remains necessary in that case. uva is also a necessary precondition for enabling peertopeer pp transfer of data directly across the pcie bus or nvlink for supported gpus in supported configurations, bypassing host memory. see the cuda c programming guide for further explanations and software requirements for uva and pp. . device memory spaces cuda devices use several memory spaces, which have different characteristics that reflect their distinct usages in cuda applications. these memory spaces include global, local, shared, texture, and registers, as shown in figure . memory spaces on a cuda device of these different memory spaces, global memory is the most plentiful see features and technical specifications of the cuda c programming guide for the amounts of memory available in each memory space at each compute capability level. global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file. the various principal traits of the memory types are shown in table . table . salient features of device memory memory location onoff chip cached access scope lifetime register on na rw thread thread local off yes rw thread thread shared on na rw all threads in block block global off rw all threads host host allocation constant off yes r all threads host host allocation texture off yes r all threads host host allocation cached in l and l by default on devices of compute capability and x cached only in l by default on devices of lower compute capabilities, though some allow optin to caching in l as well via compilation flags. cached in l and l by default except on devices of compute capability x devices of compute capability x cache locals only in l. in the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array. texture references that are bound to cuda arrays can be written to via surfacewrite operations by binding a surface to the same underlying cuda array storage. reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are readonly and are not invalidated when the associated global memory is modified. .. coalesced access to global memory a very important performance consideration in programming for cudacapable gpu architectures is the coalescing of global memory accesses. global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions. note high priority ensure global memory accesses are coalesced whenever possible. the access requirements for coalescing depend on the compute capability of the device and are documented in the cuda c programming guide. for devices of compute capability or higher, the requirements can be summarized quite easily the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of byte transactions necessary to service all of the threads of the warp. for certain devices of compute capability , lcaching of accesses to global memory can be optionally enabled. if lcaching is enabled on these devices, the number of required transactions is equal to the number of required byte aligned segments. note on devices of compute capability or higher, lcaching is the default, however the data access unit is byte regardless of whether global loads are cached in l or not. on devices with gddr memory, accessing memory in a coalesced way is even more important when ecc is turned on. scattered accesses increase ecc memory transfer overhead, especially when writing data to global memory. coalescing concepts are illustrated in the following simple examples. these examples assume compute capability or higher and that accesses are for byte words, unless otherwise noted. .. a simple access pattern the first and simplest case of coalescing can be achieved by any cudaenabled device of compute capability or higher the kth thread accesses the kth word in a byte aligned array. not all threads need to participate. for example, if the threads of a warp access adjacent byte words e.g., adjacent float values, four coalesced byte transactions will service that memory access. such a pattern is shown in figure . coalesced access this access pattern results in four byte transactions, indicated by the red rectangles. if from any of the four byte segments only a subset of the words are requested e.g. if several threads had accessed the same word or if some threads did not participate in the access, the full segment is fetched anyway. furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four byte transactions would have been performed by a device with compute capability or higher. .. a sequential but misaligned access pattern if sequential threads in a warp access memory that is sequential but not aligned with a byte segment, five byte segments will be requested, as shown in figure . misaligned sequential addresses that fall within five byte segments memory allocated through the cuda runtime api, such as via cudamalloc, is guaranteed to be aligned to at least bytes. therefore, choosing sensible thread block sizes, such as multiples of the warp size i.e., on current gpus, facilitates memory accesses by warps that are properly aligned. consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example. .. effects of misaligned accesses it is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in a copy kernel that illustrates misaligned accesses. a copy kernel that illustrates misaligned accesses global void offsetcopyfloat odata, float idata, int offset int xid blockidx.x blockdim.x threadidx.x offset odataxid idataxid in a copy kernel that illustrates misaligned accesses, data is copied from the input array idata to the output array, both of which exist in global memory. the kernel is executed within a loop in host code that varies the parameter offset from to . e.g. figure corresponds to this misalignments the effective bandwidth for the copy with various offsets on an nvidia tesla v compute capability is shown in figure . performance of offsetcopy kernel for the nvidia tesla v, global memory accesses with no offset or with offsets that are multiples of words result in four byte transactions. the achieved bandwidth is approximately gbs. otherwise, five byte segments are loaded per warp, and we would expect approximately th of the memory throughput achieved with no offsets. in this particular example, the offset memory throughput achieved is, however, approximately th, because adjacent warps reuse the cache lines their neighbors fetched. so while the impact is still evident it is not as large as we might have expected. it would have been more so if adjacent warps had not exhibited such a high degree of reuse of the overfetched cache lines. .. strided accesses as seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact. it may be different with nonunitstrided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices. for this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices. to illustrate the effect of strided access on effective bandwidth, see the kernel stridecopy in a kernel to illustrate nonunit stride data copy, which copies data with a stride of stride elements between threads from idata to odata. a kernel to illustrate nonunit stride data copy global void stridecopyfloat odata, float idata, int stride int xid blockidx.xblockdim.x threadidx.xstride odataxid idataxid figure illustrates such a situation in this case, threads within a warp access words in memory with a stride of . this action leads to a load of eight l cache segments per warp on the tesla v compute capability . adjacent threads accessing memory with a stride of a stride of results in a of loadstore efficiency since half the elements in the transaction are not used and represent wasted bandwidth. as the stride increases, the effective bandwidth decreases until the point where byte segments are loaded for the threads in a warp, as indicated in figure . performance of stridecopy kernel as illustrated in figure , nonunitstride global memory accesses should be avoided whenever possible. one method for doing so utilizes shared memory, which is discussed in the next section. .. l cache starting with cuda , devices of compute capability and above have the capability to influence persistence of data in the l cache. because l cache is onchip, it potentially provides higher bandwidth and lower latency accesses to global memory. for more details refer to the l access management section in the cuda c programming guide. .. l cache access window when a cuda kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting. on the other hand, if the data is only accessed once, such data accesses can be considered to be streaming. a portion of the l cache can be set aside for persistent accesses to a data region in global memory. if this setaside portion is not used by persistent accesses, then streaming or normal data accesses can use it. the l cache setaside size for persisting accesses may be adjusted, within limits cudagetdevicepropertiesprop, deviceid cudadevicesetlimitcudalimitpersistinglcachesize, prop.persistinglcachemaxsize set aside max possible size of l cache for persisting accesses mapping of user data to l setaside portion can be controlled using an access policy window on a cuda stream or cuda graph kernel node. the example below shows how to use the access policy window on a cuda stream. cudastreamattrvalue streamattribute stream level attributes data structure streamattribute.accesspolicywindow.baseptr reinterpretcastvoidptr global memory data pointer streamattribute.accesspolicywindow.numbytes numbytes number of bytes for persisting accesses. must be less than cudadevicepropaccesspolicymaxwindowsize streamattribute.accesspolicywindow.hitratio hint for l cache hit ratio for persisting accesses in the numbytes region streamattribute.accesspolicywindow.hitprop cudaaccesspropertypersisting type of access property on cache hit streamattribute.accesspolicywindow.missprop cudaaccesspropertystreaming type of access property on cache miss. set the attributes to a cuda stream of type cudastreamt cudastreamsetattributestream, cudastreamattributeaccesspolicywindow, streamattribute the access policy window requires a value for hitratio and numbytes. depending on the value of the numbytes parameter and the size of l cache, one may need to tune the value of hitratio to avoid thrashing of l cache lines. .. tuning the access window hitratio the hitratio parameter can be used to specify the fraction of accesses that receive the hitprop property. for example, if the hitratio value is , of the memory accesses in the global memory region ptr..ptrnumbytes have the persisting property and of the memory accesses have the streaming property. to understand the effect of hitratio and numbytes, we use a sliding window micro benchmark. this microbenchmark uses a mb region in gpu global memory. first, we set aside mb of the l cache for persisting accesses using cudadevicesetlimit, as discussed above. then, as shown in the figure below, we specify that the accesses to the first freqsize sizeofint bytes of the memory region are persistent. this data will thus use the l setaside portion. in our experiment, we vary the size of this persistent data region from mb to mb to model various scenarios where data fits in or exceeds the available l setaside portion of mb. note that the nvidia tesla a gpu has mb of total l cache capacity. accesses to the remaining data of the memory region i.e., streaming data are considered normal or streaming accesses and will thus use the remaining mb of the non setaside l portion unless part of the l setaside portion is unused. mapping persistent data accesses to setaside l in sliding window experiment consider the following kernel code and access window parameters, as the implementation of the sliding window experiment. global void kernelint datapersistent, int datastreaming, int datasize, int freqsize int tid blockidx.x blockdim.x threadidx.x each cuda thread accesses one element in the persistent data section and one element in the streaming data section. because the size of the persistent memory region freqsize sizeofint bytes is much smaller than the size of the streaming memory region datasize sizeofint bytes, data in the persistent region is accessed more frequently datapersistenttid freqsize datapersistenttid freqsize datastreamingtid datasize datastreamingtid datasize streamattribute.accesspolicywindow.baseptr reinterpretcastvoiddatapersistent streamattribute.accesspolicywindow.numbytes freqsize sizeofint number of bytes for persisting accesses in range mb streamattribute.accesspolicywindow.hitratio hint for cache hit ratio. fixed value the performance of the above kernel is shown in the chart below. when the persistent data region fits well into the mb setaside portion of the l cache, a performance increase of as much as is observed. however, once the size of this persistent data region exceeds the size of the l setaside cache portion, approximately performance drop is observed due to thrashing of l cache lines. the performance of the slidingwindow benchmark with fixed hitratio of in order to optimize the performance, when the size of the persistent data is more than the size of the setaside l cache portion, we tune the numbytes and hitratio parameters in the access window as below. streamattribute.accesspolicywindow.baseptr reinterpretcastvoiddatapersistent streamattribute.accesspolicywindow.numbytes mb streamattribute.accesspolicywindow.hitratio floatfreqsizesizeofint such that up to mb of data is resident. we fix the numbytes in the access window to mb and tune the hitratio such that a random mb of the total persistent data is resident in the l setaside cache portion. the remaining portion of this persistent data will be accessed using the streaming property. this helps in reducing cache thrashing. the results are shown in the chart below, where we see good performance regardless of whether the persistent data fits in the l setaside or not. the performance of the slidingwindow benchmark with tuned hitratio .. shared memory because it is onchip, shared memory has much higher bandwidth and lower latency than local and global memory provided there are no bank conflicts between the threads, as detailed in the following section. .. shared memory and memory banks to achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules banks that can be accessed simultaneously. therefore, any memory load or store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single bank. however, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized. the hardware splits a memory request that has bank conflicts into as many separate conflictfree requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests. the one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast. in this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads. to minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests. on devices of compute capability x or newer, each bank has a bandwidth of bits every clock cycle, and successive bit words are assigned to successive banks. the warp size is threads and the number of banks is also , so bank conflicts can occur between any threads in the warp. see compute capability x in the cuda c programming guide for further details. .. shared memory in matrix multiplication cab shared memory enables cooperation between threads in a block. when multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once. shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and then reordering it in shared memory. aside from memory bank conflicts, there is no penalty for nonsequential or unaligned accesses by a warp in shared memory. the use of shared memory is illustrated via the simple example of a matrix multiplication c ab for the case with a of dimension mxw, b of dimension wxn, and c of dimension mxn. to keep the kernels simple, m and n are multiples of , since the warp size w is for current devices. a natural decomposition of the problem is to use a block and tile size of wxw threads. therefore, in terms of wxw tiles, a is a column matrix, b is a row matrix, and c is their outer product see figure . a grid of nw by mw blocks is launched, where each thread block calculates the elements of a different tile in c from a single tile of a and a single tile of b. blockcolumn matrix multiplied by blockrow matrix. blockcolumn matrix a multiplied by blockrow matrix b with resulting product matrix c. to do this, the simplemultiply kernel unoptimized matrix multiplication calculates the output elements of a tile of matrix c. unoptimized matrix multiplication global void simplemultiplyfloat a, float b, float c, int n int row blockidx.y blockdim.y threadidx.y int col blockidx.x blockdim.x threadidx.x float sum f for int i i tiledim i sum arowtiledimi bincol crowncol sum in unoptimized matrix multiplication, a, b, and c are pointers to global memory for the matrices a, b, and c, respectively blockdim.x, blockdim.y, and tiledim are all equal to w. each thread in the wxwthread block calculates one element in a tile of c. row and col are the row and column of the element in c being calculated by a particular thread. the for loop over i multiplies a row of a by a column of b, which is then written to c. the effective bandwidth of this kernel is gbs on an nvidia tesla v. to analyze performance, it is necessary to consider how warps access global memory in the for loop. each warp of threads calculates one row of a tile of c, which depends on a single row of a and an entire tile of b as illustrated in figure . computing a row of a tile. computing a row of a tile in c using one row of a and an entire tile of b. for each iteration i of the for loop, the threads in a warp read a row of the b tile, which is a sequential and coalesced access for all compute capabilities. however, for each iteration i, all threads in a warp read the same value from global memory for matrix a, as the index rowtiledimi is constant within a warp. even though such an access requires only transaction on devices of compute capability or higher, there is wasted bandwidth in the transaction, because only one byte word out of words in a byte cache segment is used. we can reuse this cache line in subsequent iterations of the loop, and we would eventually utilize all words however, when many warps execute on the same multiprocessor simultaneously, as is generally the case, the cache line may easily be evicted from the cache between iterations i and i. the performance on a device of any compute capability can be improved by reading a tile of a into shared memory as shown in using shared memory to improve the global memory load efficiency in matrix multiplication. using shared memory to improve the global memory load efficiency in matrix multiplication global void coalescedmultiplyfloat a, float b, float c, int n shared float atiletiledimtiledim int row blockidx.y blockdim.y threadidx.y int col blockidx.x blockdim.x threadidx.x float sum f atilethreadidx.ythreadidx.x arowtiledimthreadidx.x syncwarp for int i i tiledim i sum atilethreadidx.yi bincol crowncol sum in using shared memory to improve the global memory load efficiency in matrix multiplication, each element in a tile of a is read from global memory only once, in a fully coalesced fashion with no wasted bandwidth, to shared memory. within each iteration of the for loop, a value in shared memory is broadcast to all threads in a warp. instead of a syncthreadssynchronization barrier call, a syncwarp is sufficient after reading the tile of a into shared memory because only threads within the warp that write the data into shared memory read this data. this kernel has an effective bandwidth of gbs on an nvidia tesla v. this illustrates the use of the shared memory as a usermanaged cache when the hardware l cache eviction policy does not match up well with the needs of the application or when l cache is not used for reads from global memory. a further improvement can be made to how using shared memory to improve the global memory load efficiency in matrix multiplication deals with matrix b. in calculating each of the rows of a tile of matrix c, the entire tile of b is read. the repeated reading of the b tile can be eliminated by reading it into shared memory once improvement by reading additional data into shared memory. improvement by reading additional data into shared memory global void sharedabmultiplyfloat a, float b, float c, int n shared float atiletiledimtiledim, btiletiledimtiledim int row blockidx.y blockdim.y threadidx.y int col blockidx.x blockdim.x threadidx.x float sum f atilethreadidx.ythreadidx.x arowtiledimthreadidx.x btilethreadidx.ythreadidx.x bthreadidx.yncol syncthreads for int i i tiledim i sum atilethreadidx.yi btileithreadidx.x crowncol sum note that in improvement by reading additional data into shared memory, a syncthreads call is required after reading the b tile because a warp reads data from shared memory that were written to shared memory by different warps. the effective bandwidth of this routine is gbs on an nvidia tesla v. note that the performance improvement is not due to improved coalescing in either case, but to avoiding redundant transfers from global memory. the results of the various optimizations are summarized in table . table . performance improvements optimizing c ab matrix multiply class tablenostripes optimization nvidia tesla v no optimization gbs coalesced using shared memory to store a tile of a gbs using shared memory to eliminate redundant reads of a tile of b gbs note medium priority use shared memory to avoid redundant transfers from global memory. .. shared memory in matrix multiplication caat a variant of the previous matrix multiplication can be used to illustrate how strided accesses to global memory, as well as shared memory bank conflicts, are handled. this variant simply uses the transpose of a in place of b, so c aat. a simple implementation for c aat is shown in unoptimized handling of strided accesses to global memory unoptimized handling of strided accesses to global memory global void simplemultiplyfloat a, float c, int m int row blockidx.y blockdim.y threadidx.y int col blockidx.x blockdim.x threadidx.x float sum f for int i i tiledim i sum arowtiledimi acoltiledimi crowmcol sum in unoptimized handling of strided accesses to global memory, the rowth, colth element of c is obtained by taking the dot product of the rowth and colth rows of a. the effective bandwidth for this kernel is gbs on an nvidia tesla v. these results are substantially lower than the corresponding measurements for the c ab kernel. the difference is in how threads in a half warp access elements of a in the second term, acoltiledimi, for each iteration i. for a warp of threads, col represents sequential columns of the transpose of a, and therefore coltiledim represents a strided access of global memory with a stride of w, resulting in plenty of wasted bandwidth. the way to avoid strided access is to use shared memory as before, except in this case a warp reads a row of a into a column of a shared memory tile, as shown in an optimized handling of strided accesses using coalesced reads from global memory. an optimized handling of strided accesses using coalesced reads from global memory global void coalescedmultiplyfloat a, float c, int m shared float atiletiledimtiledim, transposedtiletiledimtiledim int row blockidx.y blockdim.y threadidx.y int col blockidx.x blockdim.x threadidx.x float sum f atilethreadidx.ythreadidx.x arowtiledimthreadidx.x transposedtilethreadidx.xthreadidx.y ablockidx.xblockdim.x threadidx.ytiledim threadidx.x syncthreads for int i i tiledim i sum atilethreadidx.yi transposedtileithreadidx.x crowmcol sum an optimized handling of strided accesses using coalesced reads from global memory uses the shared transposedtile to avoid uncoalesced accesses in the second term in the dot product and the shared atile technique from the previous example to avoid uncoalesced accesses in the first term. the effective bandwidth of this kernel is gbs on an nvidia tesla v.these results are lower than those obtained by the final kernel for c ab. the cause of the difference is shared memory bank conflicts. the reads of elements in transposedtile within the for loop are free of conflicts, because threads of each half warp read across rows of the tile, resulting in unit stride across the banks. however, bank conflicts occur when copying the tile from global memory into shared memory. to enable the loads from global memory to be coalesced, data are read from global memory sequentially. however, this requires writing to shared memory in columns, and because of the use of wxw tiles in shared memory, this results in a stride between threads of w banks every thread of the warp hits the same bank recall that w is selected as . these manyway bank conflicts are very expensive. the simple remedy is to pad the shared memory array so that it has an extra column, as in the following line of code. shared float transposedtiletiledimtiledim this padding eliminates the conflicts entirely, because now the stride between threads is w banks i.e., for current devices, which, due to modulo arithmetic used to compute bank indices, is equivalent to a unit stride. after this change, the effective bandwidth is gbs on an nvidia tesla v, which is comparable to the results from the last c ab kernel. the results of these optimizations are summarized in table . table . performance improvements optimizing c aat matrix multiplication optimization nvidia tesla v no optimization gbs using shared memory to coalesce global reads gbs removing bank conflicts gbs these results should be compared with those in table . as can be seen from these tables, judicious use of shared memory can dramatically improve performance. the examples in this section have illustrated three reasons to use shared memory to enable coalesced accesses to global memory, especially to avoid large strides for general matrices, strides are much larger than to eliminate or reduce redundant loads from global memory to avoid wasted bandwidth .. asynchronous copy from global memory to shared memory cuda introduces an asynccopy feature that can be used within device code to explicitly manage the asynchronous copying of data from global memory to shared memory. this feature enables cuda kernels to overlap copying data from global to shared memory with computation. it also avoids an intermediary register file access traditionally present between the global memory read and the shared memory write. for more details refer to the memcpyasync section in the cuda c programming guide. to understand the performance difference between synchronous copy and asynchronous copy of data from global memory to shared memory, consider the following micro benchmark cuda kernels for demonstrating the synchronous and asynchronous approaches. asynchronous copies are hardware accelerated for nvidia a gpu. template typename t global void pipelinekernelsynct global, uintt clock, sizet copycount extern shared char s t shared reinterpretcastt s uintt clockstart clock for sizet i i copycount i sharedblockdim.x i threadidx.x globalblockdim.x i threadidx.x uintt clockend clock atomicaddreinterpretcastunsigned long long clock, clockend clockstart template typename t global void pipelinekernelasynct global, uintt clock, sizet copycount extern shared char s t shared reinterpretcastt s uintt clockstart clock pipeline pipe for sizet i i copycount i pipelinememcpyasyncsharedblockdim.x i threadidx.x, globalblockdim.x i threadidx.x, sizeoft pipelinecommit pipelinewaitprior uintt clockend clock atomicaddreinterpretcastunsigned long long clock, clockend clockstart the synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory. in the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as pipelinememcpyasync function is called. the pipelinewaitprior will wait until all the instructions in the pipe object have been executed. using asynchronous copies does not use any intermediate register. not using intermediate registers can help reduce register pressure and can increase kernel occupancy. data copied from global memory to shared memory using asynchronous copy instructions can be cached in the l cache or the l cache can be optionally bypassed. if individual cuda threads are copying elements of bytes, the l cache can be bypassed. this difference is illustrated in figure . comparing synchronous vs asynchronous copy from global memory to shared memory we evaluate the performance of both kernels using elements of size b, b and b per thread i.e., using int, int and int for the template parameter. we adjust the copycount in the kernels such that each thread block copies from bytes up to mb. the performance of the kernels is shown in figure . comparing performance of synchronous vs asynchronous copy from global memory to shared memory from the performance chart, the following observations can be made for this experiment. best performance with synchronous copy is achieved when the copycount parameter is a multiple of for all three element sizes. the compiler can optimize groups of load and store instructions. this is evident from the saw tooth curves. asynchronous copy achieves better performance in nearly all cases. the asynccopy does not require the copycount parameter to be a multiple of , to maximize performance through compiler optimizations. overall, best performance is achieved when using asynchronous copies with an element of size or bytes. .. local memory local memory is so named because its scope is local to the thread, not because of its physical location. in fact, local memory is offchip. hence, access to local memory is as expensive as access to global memory. in other words, the term local in the name does not imply faster access. local memory is used only to hold automatic variables. this is done by the nvcc compiler when it determines that there is insufficient register space to hold the variable. automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically. inspection of the ptx assembly code obtained by compiling with ptx or keep commandline options to nvcc reveals whether a variable has been placed in local memory during the first compilation phases. if it has, it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics. if it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture. there is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel lmem when run with theptxasoptionsv option. .. texture memory the readonly texture memory space is cached. therefore, a texture fetch costs one device memory read only on a cache miss otherwise, it just costs one read from the texture cache. the texture cache is optimized for d spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance. texture memory is also designed for streaming fetches with a constant latency that is, a cache hit reduces dram bandwidth demand, but not fetch latency. in certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory. .. additional texture capabilities if textures are fetched using texd,texd, or texd rather than texdfetch, the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in table . table . useful features for texd, texd, and texd fetches feature use caveat filtering fast, lowprecision interpolation between texels valid only if the texture reference returns floatingpoint data normalized texture coordinates resolutionindependent coding none addressing modes automatic handling of boundary cases can be used only with normalized texture coordinates the automatic handling of boundary cases in the bottom row of table refers to how a texture coordinate is resolved when it falls outside the valid addressing range. there are two options clamp and wrap. if x is the coordinate and n is the number of texels for a onedimensional texture, then with clamp, x is replaced by if x and by n if x. with wrap, x is replaced by fracx where fracx x floorx. floor returns the largest integer less than or equal to x. so, in clamp mode where n , an x of is clamped to whereas in wrap mode, it is converted to within a kernel call, the texture cache is not kept coherent with respect to global memory writes, so texture fetches from addresses that have been written via global stores in the same kernel call return undefined data. that is, a thread can safely read a memory location via texture if the location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread within the same kernel call. .. constant memory there is a total of kb constant memory on a device. the constant memory space is cached. as a result, a read from constant memory costs one memory read from device memory only on a cache miss otherwise, it just costs one read from the constant cache. accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. as such, the constant cache is best when threads in the same warp accesses only a few distinct locations. if all threads of a warp access the same location, then constant memory can be as fast as a register access. .. registers generally, accessing a register consumes zero extra clock cycles per instruction, but delays may occur due to register readafterwrite dependencies and register memory bank conflicts. the compiler and hardware thread scheduler will schedule instructions as optimally as possible to avoid register memory bank conflicts. an application has no direct control over these bank conflicts. in particular, there is no registerrelated reason to pack data into vector data types such as float or int types. .. register pressure register pressure occurs when there are not enough registers available for a given task. even though each multiprocessor contains thousands of bit registers see features and technical specifications of the cuda c programming guide, these are partitioned among concurrent threads. to prevent the compiler from allocating too many registers, use the maxrregcountn compiler commandline option see nvcc or the launch bounds kernel definition qualifier see execution configuration of the cuda c programming guide to control the maximum number of registers to allocated per thread. . allocation device memory allocation and deallocation via cudamalloc and cudafree are expensive operations. it is recommended to use cudamallocasync and cudafreeasync which are stream ordered pool allocators to manage device memory. . numa best practices some recent linux distributions enable automatic numa balancing or autonuma by default. in some instances, operations performed by automatic numa balancing may degrade the performance of applications running on nvidia gpus. for optimal performance, users should manually tune the numa characteristics of their application. the optimal numa tuning will depend on the characteristics and desired hardware affinities of each application and node, but in general applications computing on nvidia gpus are advised to choose a policy that disables automatic numa balancing. for example, on ibm newell power nodes where the cpus correspond to numa nodes and , use numactl membind, to bind memory allocations to the cpus. . execution configuration optimizations one of the keys to good performance is to keep the multiprocessors on the device as busy as possible. a device in which work is poorly balanced across the multiprocessors will deliver suboptimal performance. hence, its important to design your application to use threads and blocks in a way that maximizes hardware utilization and to limit practices that impede the free distribution of work. a key concept in this effort is occupancy, which is explained in the following sections. hardware utilization can also be improved in some cases by designing your application so that multiple, independent kernels can execute at the same time. multiple kernels executing at the same time is known as concurrent kernel execution. concurrent kernel execution is described below. another important concept is the management of system resources allocated for a particular task. how to manage this resource utilization is discussed in the final sections of this chapter. . occupancy thread instructions are executed sequentially in cuda, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. this metric is occupancy. occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. to determine the latter number, see the devicequery cuda sample or refer to compute capabilities in the cuda c programming guide. another way to view occupancy is the percentage of the hardwares ability to process warps that is actively in use. higher occupancy does not always equate to higher performancethere is a point above which additional occupancy does not improve performance. however, low occupancy always interferes with the ability to hide memory latency, resulting in performance degradation. per thread resources required by a cuda kernel might limit the maximum block size in an unwanted way. in order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an sm, developers should include the single argument launchboundsmaxthreadsperblock which specifies the largest block size that the kernel will be launched with. failure to do so could lead to too many resources requested for launch errors. providing the two argument version of launchboundsmaxthreadsperblock,minblockspermultiprocessor can improve performance in some cases. the right value for minblockspermultiprocessor should be determined using a detailed per kernel analysis. .. calculating occupancy one of several factors that determine occupancy is register availability. register storage enables threads to keep local variables nearby for lowlatency access. however, the set of registers known as the register file is a limited commodity that all threads resident on a multiprocessor must share. registers are allocated to an entire block all at once. so, if each thread block uses many registers, the number of thread blocks that can be resident on a multiprocessor is reduced, thereby lowering the occupancy of the multiprocessor. the maximum number of registers per thread can be set manually at compilation time perfile using the maxrregcount option or perkernel using the launchbounds qualifier see register pressure. for purposes of calculating occupancy, the number of registers used by each thread is one of the key factors. for example, on devices of compute capability each multiprocessor has , bit registers and can have a maximum of simultaneous threads resident warps x threads per warp. this means that in one of these devices, for a multiprocessor to have occupancy, each thread can use at most registers. however, this approach of determining how register count affects occupancy does not take into account the register allocation granularity. for example, on a device of compute capability , a kernel with thread blocks using registers per thread results in an occupancy of with active thread blocks per multiprocessor, whereas a kernel with thread blocks using the same registers per thread results in an occupancy of because only four thread blocks can reside on a multiprocessor. furthermore, register allocations are rounded up to the nearest registers per warp. the number of registers available, the maximum number of simultaneous threads resident on each multiprocessor, and the register allocation granularity vary over different compute capabilities. because of these nuances in register allocation and the fact that a multiprocessors shared memory is also partitioned between resident thread blocks, the exact relationship between register usage and occupancy can be difficult to determine. the ptxas optionsv option of nvcc details the number of registers used per thread for each kernel. see hardware multithreading of the cuda c programming guide for the register allocation formulas for devices of various compute capabilities and features and technical specifications of the cuda c programming guide for the total number of registers available on those devices. alternatively, nvidia provides an occupancy calculator in the form of an excel spreadsheet that enables developers to hone in on the optimal balance and to test different possible scenarios more easily. this spreadsheet, shown in figure , is called cudaoccupancycalculator.xls and is located in the tools subdirectory of the cuda toolkit installation. using the cuda occupancy calculator to project gpu multiprocessor occupancy in addition to the calculator spreadsheet, occupancy can be determined using the nvidia nsight compute profiler. details about occupancy are displayed in the occupancy section. an application can also use the occupancy api from the cuda runtime, e.g. cudaoccupancymaxactiveblockspermultiprocessor, to dynamically select launch configurations based on runtime parameters. . hiding register dependencies note medium priority to hide latency arising from register dependencies, maintain sufficient numbers of active threads per multiprocessor i.e., sufficient occupancy. register dependencies arise when an instruction uses a result stored in a register written by an instruction before it. the latency of most arithmetic instructions is typically cycles on devices of compute capability . so threads must wait approximatly cycles before using an arithmetic result. however, this latency can be completely hidden by the execution of threads in other warps. see registers for details. . thread and block heuristics note medium priority the number of threads per block should be a multiple of threads, because this provides optimal computing efficiency and facilitates coalescing. the dimension and size of blocks per grid and the dimension and size of threads per block are both important factors. the multidimensional aspect of these parameters allows easier mapping of multidimensional problems to cuda and does not play a role in performance. as a result, this section discusses size but not dimension. latency hiding and occupancy depend on the number of active warps per multiprocessor, which is implicitly determined by the execution parameters along with resource register and shared memory constraints. choosing execution parameters is a matter of striking a balance between latency hiding occupancy and resource utilization. choosing the execution configuration parameters should be done in tandem however, there are certain heuristics that apply to each parameter individually. when choosing the first execution configuration parameterthe number of blocks per grid, or grid size the primary concern is keeping the entire gpu busy. the number of blocks in a grid should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. furthermore, there should be multiple active blocks per multiprocessor so that blocks that arent waiting for a syncthreads can keep the hardware busy. this recommendation is subject to resource availability therefore, it should be determined in the context of the second execution parameter the number of threads per block, or block size as well as shared memory usage. to scale to future devices, the number of blocks per kernel launch should be in the thousands. when choosing the block size, it is important to remember that multiple concurrent blocks can reside on a multiprocessor, so occupancy is not determined by block size alone. in particular, a larger block size does not imply a higher occupancy. as mentioned in occupancy, higher occupancy does not always equate to better performance. for example, improving occupancy from percent to percent generally does not translate to a similar increase in performance. a lower occupancy kernel will have more registers available per thread than a higher occupancy kernel, which may result in less register spilling to local memory in particular, with a high degree of exposed instructionlevel parallelism ilp it is, in some cases, possible to fully cover latency with a low occupancy. there are many such factors involved in selecting block size, and inevitably some experimentation is required. however, a few rules of thumb should be followed threads per block should be a multiple of warp size to avoid wasting computation on underpopulated warps and to facilitate coalescing. a minimum of threads per block should be used, and only if there are multiple concurrent blocks per multiprocessor. between and threads per block is a good initial range for experimentation with different block sizes. use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. this is particularly beneficial to kernels that frequently call syncthreads. note that when a thread block allocates more registers than are available on a multiprocessor, the kernel launch fails, as it will when too much shared memory or too many threads are requested. . effects of shared memory shared memory can be helpful in several situations, such as helping to coalesce or eliminate redundant access to global memory. however, it also can act as a constraint on occupancy. in many cases, the amount of shared memory required by a kernel is related to the block size that was chosen, but the mapping of threads to shared memory elements does not need to be onetoone. for example, it may be desirable to use a x element shared memory array in a kernel, but because the maximum number of threads per block is , it is not possible to launch a kernel with x threads per block. in such cases, kernels with x or x threads can be launched with each thread processing four elements of the shared memory array. the approach of using a single thread to process multiple elements of a shared memory array can be beneficial even if limits such as threads per block are not an issue. this is because some operations common to each element can be performed by the thread once, amortizing the cost over the number of shared memory elements processed by the thread. a useful technique to determine the sensitivity of performance to occupancy is through experimentation with the amount of dynamically allocated shared memory, as specified in the third parameter of the execution configuration. by simply increasing this parameter without modifying the kernel, it is possible to effectively reduce the occupancy of the kernel and measure its effect on performance. . concurrent kernel execution as described in asynchronous and overlapping transfers with computation, cuda streams can be used to overlap kernel execution with data transfers. on devices that are capable of concurrent kernel execution, streams can also be used to execute multiple kernels simultaneously to more fully take advantage of the devices multiprocessors. whether a device has this capability is indicated by the concurrentkernels field of the cudadeviceprop structure or listed in the output of the devicequery cuda sample. nondefault streams streams other than stream are required for concurrent execution because kernel calls that use the default stream begin only after all preceding calls on the device in any stream have completed, and no operation on the device in any stream commences until they are finished. the following example illustrates the basic technique. because kernel and kernel are executed in different, nondefault streams, a capable device can execute the kernels at the same time. cudastreamcreatestream cudastreamcreatestream kernelgrid, block, , streamdata kernelgrid, block, , streamdata . multiple contexts cuda work occurs within a process space for a particular gpu known as a context. the context encapsulates kernel launches and memory allocations for that gpu as well as supporting constructs such as the page tables. the context is explicit in the cuda driver api but is entirely implicit in the cuda runtime api, which creates and manages contexts automatically. with the cuda driver api, a cuda application process can potentially create more than one context for a given gpu. if multiple cuda application processes access the same gpu concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless multiprocess service is in use. while multiple contexts and their associated resources such as global memory allocations can be allocated concurrently on a given gpu, only one of these contexts can execute work at any given moment on that gpu contexts sharing the same gpu are timesliced. creating additional contexts incurs memory overhead for percontext data and time overhead for context switching. furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently see also concurrent kernel execution. therefore, it is best to avoid multiple contexts per gpu within the same cuda application. to assist with this, the cuda driver api provides methods to access and manage a special context on each gpu called the primary context. these are the same contexts used implicitly by the cuda runtime when there is not already a current context for a thread. when initializing the programlibrary cucontext ctx cudeviceprimaryctxretainctx, dev when the programlibrary launches work cuctxpushcurrentctx kernel...... cuctxpopcurrentctx when the programlibrary is finished with the context cudeviceprimaryctxreleasedev note nvidiasmi can be used to configure a gpu for exclusive process mode, which limits the number of contexts per gpu to one. this context can be current to as many threads as desired within the creating process, and cudeviceprimaryctxretain will fail if a nonprimary context that was created with the cuda driver api already exists on the device. . instruction optimization awareness of how instructions are executed often permits lowlevel optimizations that can be useful, especially in code that is run frequently the socalled hot spot in a program. best practices suggest that this optimization be performed after all higherlevel optimizations have been completed. . arithmetic instructions singleprecision floats provide the best performance, and their use is highly encouraged. the throughput of individual arithmetic operations is detailed in the cuda c programming guide. .. division modulo operations note low priority use shift operations to avoid expensive division and modulo calculations. integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible if n is a power of , in is equivalent to i gg logn and i n is equivalent to ileft n right . the compiler will perform these conversions if n is literal. for further information, refer to performance guidelines in the cuda c programming guide. .. loop counters signed vs. unsigned note low medium priority use signed integers rather than unsigned integers as loop counters. in the c language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results. therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic. this is of particular note with loop counters since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned. for slightly better performance, however, they should instead be declared as signed. for example, consider the following code for i i n i outi inoffset stridei here, the subexpression stridei could overflow a bit integer, so if i is declared as unsigned, the overflow semantics prevent the compiler from using some optimizations that might otherwise have applied, such as strength reduction. if instead i is declared as signed, where the overflow semantics are undefined, the compiler has more leeway to use these optimizations. .. reciprocal square root the reciprocal square root should always be invoked explicitly as rsqrtf for single precision and rsqrt for double precision. the compiler optimizes fsqrtfx into rsqrtf only when this does not violate ieee semantics. .. other arithmetic instructions note low priority avoid automatic conversion of doubles to floats. the compiler must on occasion insert conversion instructions, introducing additional execution cycles. this is the case for functions operating on char or short whose operands generally need to be converted to an int doubleprecision floatingpoint constants defined without any type suffix used as input to singleprecision floatingpoint computations the latter case can be avoided by using singleprecision floatingpoint constants, defined with an f suffix such as f, f, f. for singleprecision code, use of the float type and the singleprecision math functions are highly recommended. it should also be noted that the cuda math librarys complementary error function, erfcf, is particularly fast with full singleprecision accuracy. .. exponentiation with small fractional arguments for some fractional exponents, exponentiation can be accelerated significantly compared to the use of pow by using square roots, cube roots, and their inverses. for those exponentiations where the exponent is not exactly representable as a floatingpoint number, such as , this can also provide much more accurate results, as use of pow magnifies the initial representational error. the formulas in the table below are valid for x , x , that is, signbitx . table . formulae for exponentiation by small fractions computation formula x r rcbrtrcbrtx x r cbrtrcbrtx x r rcbrtrsqrtx x r rcbrtsqrtx x r rsqrtrsqrtx x r sqrtrsqrtx x r cbrtx x r rcbrtx x r sqrtx x r rsqrtx x r cbrtx r rr x r rcbrtx r rr x r sqrtx r rsqrtr x r rsqrtx r rsqrtr x r xrcbrtrsqrtx x r x rcbrtsqrtx x r xrsqrtrsqrtx x r xsqrtrsqrtx x r xcbrtx x r xrcbrtx x r xsqrtx x r xrsqrtx .. math libraries note medium priority use the fast math library whenever speed trumps precision. two types of runtime math operations are supported. they can be distinguished by their names some have names with prepended underscores, whereas others do not e.g., functionname versus functionname. functions following the functionname naming convention map directly to the hardware level. they are faster but provide somewhat lower accuracy e.g., sinfx and expfx. functions following functionname naming convention are slower but have higher accuracy e.g., sinfx and expfx. the throughput of sinfx, cosfx, andexpfx is much greater than that of sinfx, cosfx, and expfx. the latter become even more expensive about an order of magnitude slower if the magnitude of the argument x needs to be reduced. moreover, in such cases, the argumentreduction code uses local memory, which can affect performance even more because of the high latency of local memory. more details are available in the cuda c programming guide. note also that whenever sine and cosine of the same argument are computed, the sincos family of instructions should be used to optimize performance sincosf for singleprecision fast math see next paragraph sincosf for regular singleprecision sincos for double precision the usefastmath compiler option of nvcc coerces every functionname call to the equivalent functionname call. it also disables singleprecision denormal support and lowers the precision of singleprecision division in general. this is an aggressive optimization that can both reduce numerical accuracy and alter special case handling. a more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated. note this switch is effective only on singleprecision floating point. note medium priority prefer faster, more specialized math functions over slower, more general ones when possible. for small integer powers e.g., x or x, explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow. while compiler optimization improvements continually seek to narrow this gap, explicit multiplication or the use of an equivalent purposebuilt inline function or macro can have a significant advantage. this advantage is increased when several powers of the same base are needed e.g., where both x and x are calculated in close proximity, as this aids the compiler in its common subexpression elimination cse optimization. for exponentiation using base or , use the functions exp or expf and exp or expf rather than the functions pow or powf. both pow and powf are heavyweight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent. the functions exp, expf, exp, and expf, on the other hand, are similar to exp and expf in terms of performance, and can be as much as ten times faster than their powpowf equivalents. for exponentiation with an exponent of , use the cbrt or cbrtf function rather than the generic exponentiation functions pow or powf, as the former are significantly faster than the latter. likewise, for exponentation with an exponent of , use rcbrt or rcbrtf. replace sinexpr with sinpiexpr, cosexpr with cospiexpr, and sincosexpr with sincospiexpr. this is advantageous with regard to both accuracy and performance. as a particular example, to evaluate the sine function in degrees instead of radians, use sinpix. similarly, the singleprecision functions sinpif, cospif, and sincospif should replace calls to sinf, cosf, and sincosf when the function argument is of the form expr. the performance advantage sinpi has over sin is due to simplified argument reduction the accuracy advantage is because sinpi multiplies by only implicitly, effectively using an infinitely precise mathematical rather than a single or doubleprecision approximation thereof. .. precisionrelated compiler flags by default, the nvcc compiler generates ieeecompliant code, but it also provides options to generate code that somewhat less accurate but faster ftztrue denormalized numbers are flushed to zero precdivfalse less precise division precsqrtfalse less precise square root another, more aggressive, option is usefastmath, which coerces every functionname call to the equivalent functionname call. this makes the code run faster at the cost of diminished precision and accuracy. see math libraries. . memory instructions note high priority minimize the use of global memory. prefer shared memory access where possible. memory instructions include any instruction that reads from or writes to shared, local, or global memory. when accessing uncached local or global memory, there are hundreds of clock cycles of memory latency. as an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory shared float shared device float device sharedthreadidx.x devicethreadidx.x much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete. however, it is best to avoid accessing global memory whenever possible. . control flow . branching and divergence note high priority avoid different execution paths within the same warp. flow control instructions if, switch, do, for, while can significantly affect the instruction throughput by causing threads of the same warp to diverge that is, to follow different execution paths. if this happens, the different execution paths must be executed separately this increases the total number of instructions executed for this warp. to obtain best performance in cases where the control flow depends on the thread id, the controlling condition should be written so as to minimize the number of divergent warps. this is possible because the distribution of the warps across the block is deterministic as mentioned in simt architecture of the cuda c programming guide. a trivial example is when the controlling condition depends only on threadidx wsize where wsize is the warp size. in this case, no warp diverges because the controlling condition is perfectly aligned with the warps. for branches including just a few instructions, warp divergence generally results in marginal performance losses. for example, the compiler may use predication to avoid an actual branch. instead, all instructions are scheduled, but a perthread condition code or predicate controls which threads execute the instructions. threads with a false predicate do not write results, and also do not evaluate addresses or read operands. starting with the volta architecture, independent thread scheduling allows a warp to remain diverged outside of the datadependent conditional block. an explicit syncwarp can be used to guarantee that the warp has reconverged for subsequent instructions. . branch predication note low priority make it easy for the compiler to use branch predication in lieu of loops or control statements. sometimes, the compiler may unroll loops or optimize out if or switch statements by using branch predication instead. in these cases, no warp can ever diverge. the programmer can also control loop unrolling using pragma unroll for more information on this pragma, refer to the cuda c programming guide. when using branch predication, none of the instructions whose execution depends on the controlling condition is skipped. instead, each such instruction is associated with a perthread condition code or predicate that is set to true or false according to the controlling condition. although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed. instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands. the compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold. . deploying cuda applications having completed the gpu acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. this is important for a number of reasons for example, it allows the user to profit from their investment as early as possible the speedup may be partial but is still valuable, and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application. . understanding the programming environment with each generation of nvidia processors, new features are added to the gpu that cuda can leverage. consequently, its important to understand the characteristics of the architecture. programmers should be aware of two version numbers. the first is the compute capability, and the second is the version number of the cuda runtime and cuda driver apis. . cuda compute capability the compute capability describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor. higher compute capability versions are supersets of lower that is, earlier versions, so they are backward compatible. the compute capability of the gpu in the device can be queried programmatically as illustrated in the devicequery cuda sample. the output for that program is shown in figure . this information is obtained by calling cudagetdeviceproperties and accessing the information in the structure it returns. sample cuda configuration data reported by devicequery the major and minor revision numbers of the compute capability are shown on the seventh line of figure . device of this system has compute capability . more details about the compute capabilities of various gpus are in cudaenabled gpus and compute capabilities of the cuda c programming guide. in particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device. . additional hardware data certain hardware features are not described by the compute capability. for example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all gpus irrespective of the compute capability. in such cases, call cudagetdeviceproperties to determine whether the device is capable of a certain feature. for example, the asyncenginecount field of the device property structure indicates whether overlapping kernel execution and data transfers is possible and, if so, how many concurrent transfers are possible likewise, the canmaphostmemory field indicates whether zerocopy data transfers can be performed. . which compute capability target to target specific versions of nvidia hardware and cuda software, use the arch, code, and gencode options of nvcc. code that uses the warp shuffle operation, for example, must be compiled with archsm or higher compute capability. see building for maximum compatibility for further discussion of the flags used for building code for multiple generations of cudacapable device simultaneously. . cuda runtime the host runtime component of the cuda software environment can be used only by host functions. it provides functions to handle the following device management context management memory management code module management execution control texture reference management interoperability with opengl and directd as compared to the lowerlevel cuda driver api, the cuda runtime greatly eases device management by providing implicit initialization, context management, and device code module management. the c host code generated by nvcc utilizes the cuda runtime, so applications that link to this code will depend on the cuda runtime similarly, any code that uses the cublas, cufft, and other cuda toolkit libraries will also depend on the cuda runtime, which is used internally by these libraries. the functions that make up the cuda runtime api are explained in the cuda toolkit reference manual. the cuda runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched. the implicit driver version checking, code initialization, cuda context management, cuda module management cubin to function mapping, kernel configuration, and parameter passing are all performed by the cuda runtime. it comprises two principal parts a cstyle function interface cudaruntimeapi.h. cstyle convenience wrappers cudaruntime.h built on top of the cstyle functions. for more information on the runtime api, refer to cuda runtime of the cuda c programming guide. . cuda compatibility developers guide cuda toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes. cuda compatibility allows users to update the latest cuda toolkit software including the compiler, libraries, and tools without requiring update to the entire driver stack. the cuda software environment consists of three parts cuda toolkit libraries, cuda runtime and developer tools sdk for developers to build cuda applications. cuda driver usermode driver component used to run cuda applications e.g. libcuda.so on linux systems. nvidia gpu device driver kernelmode driver component for nvidia gpus. on linux systems, the cuda driver and kernel mode components are delivered together in the nvidia display driver package. this is shown in figure . components of cuda the cuda compiler nvcc, provides a way to handle cuda and noncuda code by splitting and steering compilation, along with the cuda runtime, is part of the cuda compiler toolchain. the cuda runtime api provides developers with highlevel c interface for simplified management of devices, kernel executions etc., while the cuda driver api provides cuda driver api a lowlevel programming interface for applications to target nvidia hardware. built on top of these technologies are cuda libraries, some of which are included in the cuda toolkit, while others such as cudnn may be released independently of the cuda toolkit. . cuda toolkit versioning starting with cuda , the toolkit versions are based on an industrystandard semantic versioning scheme .x.y.z, where .x stands for the major version apis have changed and binary compatibility is broken. .y stands for the minor version introduction of new apis, deprecation of old apis, and source compatibility might be broken but binary compatibility is maintained. .z stands for the releasepatch version new updates and patches will increment this. each component in the toolkit is recommended to be semantically versioned. from cuda nvrtc is also semantically versioned. we will note some of them later on in the document. the versions of the components in the toolkit are available in this table. compatibility of the cuda platform is thus intended to address a few scenarios nvidia driver upgrades to systems with gpus running in production for enterprises or datacenters can be complex and may need advance planning. delays in rolling out new nvidia drivers could mean that users of such systems may not have access to new features available in cuda releases. not requiring driver updates for new cuda releases can mean that new versions of the software can be made available faster to users. many software libraries and applications built on top of cuda e.g. math libraries or deep learning frameworks do not have a direct dependency on the cuda runtime, compiler or driver. in such cases, users or developers can still benefit from not having to upgrade the entire cuda toolkit or driver to use these libraries or frameworks. upgrading dependencies is errorprone and time consuming, and in some corner cases, can even change the semantics of a program. constantly recompiling with the latest cuda toolkit means forcing upgrades on the endcustomers of an application product. package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process. cuda supports several compatibility choices first introduced in cuda , the cuda forward compatible upgrade is designed to allow users to get access to new cuda features and run applications built with new cuda releases on systems with older installations of the nvidia datacenter driver. first introduced in cuda , cuda enhanced compatibility provides two benefits by leveraging semantic versioning across components in the cuda toolkit, an application can be built for one cuda minor release for example and work across all future minor releases within the major family i.e. x. the cuda runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release. the cuda driver ensures backward binary compatibility is maintained for compiled cuda applications. applications compiled with cuda toolkit versions as old as will run on newer drivers. . source compatibility we define source compatibility as a set of guarantees provided by the library, where a wellformed application built against a specific version of the library using the sdk will continue to build and run without errors when a newer version of the sdk is installed. both the cuda driver and the cuda runtime are not source compatible across the different sdk releases. apis can be deprecated and removed. therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit. developers are notified through deprecation and documentation mechanisms of any current or upcoming changes. this does not mean that application binaries compiled using an older toolkit will not be supported anymore. application binaries rely on cuda driver api interface and even though the cuda driver api itself may also have changed across toolkit versions, cuda guarantees binary compatibility of the cuda driver api interface. . binary compatibility we define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library. the cuda driver api has a versioned cstyle abi, which guarantees that applications that were running against an older driver for example cuda will still run and function correctly against a modern driver for example one shipped with cuda . this means that even though an application source might need to be changed if it has to be recompiled against a newer cuda toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions. the cuda driver api thus is binarycompatible the os loader can pick up a newer version and the application continues to work but not sourcecompatible rebuilding your application against a newer sdk might require source changes. cuda toolkit and minimum driver versions before we proceed further on this topic, its important for developers to understand the concept of minimum driver version and how that may affect them. each version of the cuda toolkit and runtime requires a minimum version of the nvidia driver. applications compiled against a cuda toolkit version will only run on systems with the specified minimum driver version for that toolkit version. prior to cuda , the minimum driver version for a toolkit was the same as the driver shipped with that version of the cuda toolkit. so, when an application is built with cuda , it can only run on a system with an r or later driver. if such an application is run on a system with the r driver installed, cuda initialization will return an error as can be seen in the example below. in this example, the devicequery sample is compiled with cuda and is run on a system with r. in this scenario, cuda initialization returns an error due to the minimum driver requirement. ubuntusamplesutilitiesdevicequery make usrlocalcudabinnvcc ccbin g i....commoninc m gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codecompute o devicequery.o c devicequery.cpp usrlocalcudabinnvcc ccbin g m gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codesm gencode archcompute,codecompute o devicequery devicequery.o nvidiasmi nvidiasmi . driver version . cuda version gpu name persistencem busid disp.a volatile uncorr. ecc fan temp perf pwrusagecap memoryusage gpuutil compute m. tesla t on e. off na c p w w mib mib default processes gpu memory gpu pid type process name usage no running processes found samplesbinxlinuxreleasedevicequery samplesbinxlinuxreleasedevicequery starting... cuda device query runtime api version cudart static linking cudagetdevicecount returned initialization error result fail refer to the cuda toolkit release notes for details for the minimum driver version and the version of the driver shipped with the toolkit. .. cuda binary cubin compatibility a slightly related but important topic is one of application binary compatibility across gpu architectures in cuda. cuda c provides a simple path for users familiar with the c programming language to easily write programs for execution by the device. kernels can be written using the cuda instruction set architecture, called ptx, which is described in the ptx reference manual. it is however usually more effective to use a highlevel programming language such as c. in both cases, kernels must be compiled into binary code by nvcc called cubins to execute on the device. the cubins are architecturespecific. binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions. in other words, a cubin object generated for compute capability x.y will only execute on devices of compute capability x.z where zy. to execute code on devices of specific compute capability, an application must load binary or ptx code that is compatible with this compute capability. for portability, that is, to be able to execute code on future gpu architectures with higher compute capability for which no binary code can be generated yet, an application must load ptx code that will be justintime compiled by the nvidia driver for these future devices. more information on cubins, ptx and application compatibility can be found in the cuda c programming guide. . cuda compatibility across minor releases by leveraging the semantic versioning, starting with cuda , components in the cuda toolkit will remain binary compatible across the minor versions of the toolkit. in order to maintain binary compatibility across minor versions, the cuda runtime no longer bumps up the minimum driver version required for every minor release this only happens when a major release is shipped. one of the main reasons a new toolchain requires a new minimum driver is to handle the jit compilation of ptx code and the jit linking of binary code. in this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the cuda platform. .. existing cuda applications within minor versions of cuda nvidiasmi nvidiasmi . driver version . cuda version gpu name persistencem busid disp.a volatile uncorr. ecc fan temp perf pwrusagecap memoryusage gpuutil compute m. mig m. tesla t on e. off na c p w w mib mib default na processes gpu gi ci pid type process name gpu memory id id usage no running processes found when our cuda application i.e. cudart is statically linked is run on the system, we see that it runs successfully even when the driver reports a version that is, without requiring the driver or other toolkit components to be updated on the system. samplesbinxlinuxreleasedevicequery samplesbinxlinuxreleasedevicequery starting... cuda device query runtime api version cudart static linking detected cuda capable devices device tesla t cuda driver version runtime version cuda capability majorminor version number ...snip... devicequery, cuda driver cudart, cuda driver version , cuda runtime version , numdevs result pass by using new cuda versions, users can benefit from new cuda programming model apis, compiler optimizations and math library features. the following sections discuss some caveats and considerations. .. handling new cuda features and driver apis a subset of cuda apis dont need a new driver and they can all be used without any driver dependencies. for example, cumemmap apis or any of apis introduced prior to cuda , such as cudadevicesynchronize, do not require a driver upgrade. to use other cuda apis introduced in a minor release that require a new driver, one would have to implement fallbacks or fail gracefully. this situation is not different from what is available today where developers use macros to compile out features based on cuda versions. users should refer to the cuda headers and documentation for new cuda apis introduced in a release. when working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older cuda driver. users wishing to take advantage of such a feature should query its availability with a dynamic check in the code static bool hostregisterfeaturesupported false static bool hostregisterisdeviceaddress false static errort cufoofunctionint ptr int dptr null if hostregisterfeaturesupported cudahostregisterptr, size, flags if hostregisterisdeviceaddress qptr ptr else cudahostgetdevicepointerqptr, ptr, else cudamalloc cudamemcpy gemm,dptr cudadevicesynchronize int main rest of code here cudadevicegetattribute hostregisterfeaturesupported, cudadevattrhostregistersupported, cudadevicegetattribute hostregisterisdeviceaddress, cudadevattrcanusehostpointerforregisteredmem, cufoofunction malloced pointer alternatively the applications interface might not work at all without a new cuda driver and then its best to return an error right away define minversion cudaerrort foo int version cudagetdriverversionversion if version minversion return cudaerrorinsufficientdriver proceed as normal a new error code is added to indicate that the functionality is missing from the driver you are running against cudaerrorcallrequiresnewerdriver. .. using ptx ptx defines a virtual machine and isa for general purpose parallel thread execution. ptx programs are translated at load time to the target hardware instruction set via the jit compiler which is part of the cuda driver. as ptx is compiled by the cuda driver, new toolchains will generate ptx that is not compatible with the older cuda driver. this is not a problem when ptx is used for future device compatibility the most common case, but can lead to issues when used for runtime compilation. for codes continuing to make use of ptx, in order to support compiling on an older driver, your code must be first transformed into device code via the static ptxjitcompiler library or nvrtc with the option of generating code for a specific architecture e.g. sm rather than a virtual architecture e.g. compute. for this workflow, a new nvptxcompilerstatic library is shipped with the cuda toolkit. we can see this usage in the following example char compileptxtonvelf nvptxcompilerhandle compiler null nvptxcompileresult status sizet elfsize, infosize, errorsize char elf, infolog, errorlog int minorver, majorver const char compileoptions gpunamesm, devicedebug nvptxcompilergetversionmajorver, minorver nvptxcompilercreatecompiler, sizetstrlenptxcode, ptxcode status nvptxcompilercompilecompiler, , compileoptions if status nvptxcompilesuccess nvptxcompilergeterrorlogsizecompiler, voiderrorsize if errorsize errorlog charmallocerrorsize nvptxcompilergeterrorlogcompiler, voiderrorlog printferror log sn, errorlog freeerrorlog exit nvptxcompilergetcompiledprogramsizecompiler, elfsize elf charmallocelfsize nvptxcompilergetcompiledprogramcompiler, voidelf nvptxcompilergetinfologsizecompiler, voidinfosize if infosize infolog charmallocinfosize nvptxcompilergetinfologcompiler, voidinfolog printfinfo log sn, infolog freeinfolog nvptxcompilerdestroycompiler return elf .. dynamic code generation nvrtc is a runtime compilation library for cuda c. it accepts cuda c source code in character string form and creates handles that can be used to obtain the ptx. the ptx string generated by nvrtc can be loaded by cumoduleloaddata and cumoduleloaddataex. dealing with relocatable objects is not yet supported, therefore the culink set of apis in the cuda driver will not work with enhanced compatibility. an upgraded driver matching the cuda runtime version is currently required for those apis. as mentioned in the ptx section, the compilation of ptx to device code lives along with the cuda driver, hence the generated ptx might be newer than what is supported by the driver on the deployment system. when using nvrtc, it is recommended that the resulting ptx code is first transformed to the final device code via the steps outlined by the ptx user workflow. this ensures your code is compatible. alternatively, nvrtc can generate cubins directly starting with cuda . applications using the new api can load the final device code directly using driver apis cumoduleloaddata and cumoduleloaddataex. nvrtc used to support only virtual architectures through the option arch, since it was only emitting ptx. it will now support actual architectures as well to emit sass. the interface is augmented to retrieve either the ptx or cubin if an actual architecture is specified. the example below shows how an existing example can be adapted to use the new features, guarded by the usecubin macro in this case include nvrtc.h include cuda.h include iostream void nvrtcsafecallnvrtcresult result if result nvrtcsuccess stdcerr nnvrtc error nvrtcgeterrorstringresult n stdexit void cudasafecallcuresult result if result cudasuccess const char msg cugeterrornameresult, msg stdcerr ncuda error msg n stdexit const char hello n extern c global void hello n printfhello worldn n n int main nvrtcprogram prog nvrtcsafecallnvrtccreateprogramprog, hello, hello.cu, , null, null ifdef usecubin const char opts archsm else const char opts archcompute endif nvrtcresult compileresult nvrtccompileprogramprog, , opts sizet logsize nvrtcsafecallnvrtcgetprogramlogsizeprog, logsize char log new charlogsize nvrtcsafecallnvrtcgetprogramlogprog, log stdcout log n delete log if compileresult nvrtcsuccess exit sizet codesize ifdef usecubin nvrtcsafecallnvrtcgetcubinsizeprog, codesize char code new charcodesize nvrtcsafecallnvrtcgetcubinprog, code else nvrtcsafecallnvrtcgetptxsizeprog, codesize char code new charcodesize nvrtcsafecallnvrtcgetptxprog, code endif nvrtcsafecallnvrtcdestroyprogramprog cudevice cudevice cucontext context cumodule module cufunction kernel cudasafecallcuinit cudasafecallcudevicegetcudevice, cudasafecallcuctxcreatecontext, , cudevice cudasafecallcumoduleloaddataexmodule, code, , , cudasafecallcumodulegetfunctionkernel, module, hello cudasafecallculaunchkernelkernel, , , , , , , , null, null, cudasafecallcuctxsynchronize cudasafecallcumoduleunloadmodule cudasafecallcuctxdestroycontext delete code .. recommendations for building a minorversion compatible library we recommend that the cuda runtime be statically linked to minimize dependencies. verify that your library doesnt leak dependencies, breakages, namespaces, etc. outside your established abi contract. follow semantic versioning for your librarys soname. having a semantically versioned abi means the interfaces need to be maintained and versioned. the library should follow semantic rules and increment the version number when a change is made that affects this abi contract. missing dependencies is also a binary compatibility break, hence you should provide fallbacks or guards for functionality that depends on those interfaces. increment major versions when there are abi breaking changes such as api deprecation and modifications. new apis can be added in minor versions. conditionally use features to remain compatible against older drivers. if no new features are used or if they are used conditionally with fallbacks provided youll be able to remain compatible. dont expose abi structures that can change. a pointer to a structure with a size embedded is a better solution. when linking with dynamic libraries from the toolkit, the library must be equal to or newer than what is needed by any one of the components involved in the linking of your application. for example, if you link against the cuda dynamic runtime, and use functionality from , as well as a separate shared library that was linked against the cuda dynamic runtime that requires functionality, the final link step must include a cuda or newer dynamic runtime. .. recommendations for taking advantage of minor version compatibility in your application certain functionality might not be available so you should query where applicable. this is common for building applications that are gpu architecture, platform and compiler agnostic. however we now add the underlying driver to that mix. as with the previous section on library building recommendations, if using the cuda runtime, we recommend linking to the cuda runtime statically when building your application. when using the driver apis directly, we recommend using the new driver entry point access api cugetprocaddress documented here cuda driver api cuda toolkit documentation. when using a shared or static library, follow the release notes of said library to determine if the library supports minor version compatibility. . preparing for deployment . testing for cuda availability when deploying a cuda application, it is often desirable to ensure that the application will continue to function properly even if the target machine does not have a cudacapable gpu andor a sufficient version of the nvidia driver installed. developers targeting a single machine with known configuration may choose to skip this section. detecting a cudacapable gpu when an application will be deployed to target machines of arbitraryunknown configuration, the application should explicitly test for the existence of a cudacapable gpu in order to take appropriate action when no such device is available. the cudagetdevicecount function can be used to query for the number of available devices. like all cuda runtime api functions, this function will fail gracefully and return cudaerrornodevice to the application if there is no cudacapable gpu or cudaerrorinsufficientdriver if there is not an appropriate version of the nvidia driver installed. if cudagetdevicecount reports an error, the application should fall back to an alternative code path. a system with multiple gpus may contain gpus of different hardware versions and capabilities. when using multiple gpus from the same application, it is recommended to use gpus of the same type, rather than mixing hardware generations. the cudachoosedevice function can be used to select the device that most closely matches a desired set of features. detecting hardware and software configuration when an application depends on the availability of certain hardware or software capabilities to enable certain functionality, the cuda api can be queried for details about the configuration of the available device and for the installed software versions. the cudagetdeviceproperties function reports various features of the available devices, including the cuda compute capability of the device see also the compute capabilities section of the cuda c programming guide. see version management for details on how to query the available cuda software api versions. . error handling all cuda runtime api calls return an error code of type cudaerrort the return value will be equal to cudasuccess if no errors have occurred. the exceptions to this are kernel launches, which return void, and cudageterrorstring, which returns a character string describing the cudaerrort code that was passed into it. the cuda toolkit libraries cublas, cufft, etc. likewise return their own sets of error codes. since some cuda api calls and all kernel launches are asynchronous with respect to the host code, errors may be reported to the host asynchronously as well often this occurs the next time the host and device synchronize with each other, such as during a call to cudamemcpy or to cudadevicesynchronize. always check the error return values on all cuda api functions, even for functions that are not expected to fail, as this will allow the application to detect and recover from errors as soon as possible should they occur. to check for errors occurring during kernel launches using the ... syntax, which does not return any error code, the return code of cudagetlasterror should be checked immediately after the kernel launch. applications that do not check for cuda api errors could at times run to completion without having noticed that the data calculated by the gpu is incomplete, invalid, or uninitialized. note the cuda toolkit samples provide several helper functions for error checking with the various cuda apis these helper functions are located in the samplescommoninchelpercuda.h file in the cuda toolkit. . building for maximum compatibility each generation of cudacapable device has an associated compute capability version that indicates the feature set supported by the device see cuda compute capability. one or more compute capability versions can be specified to the nvcc compiler while building a file compiling for the native compute capability for the target gpus of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of gpu. when an application is built for multiple compute capabilities simultaneously using several instances of the gencode flag to nvcc, the binaries for the specified compute capabilities are combined into the executable, and the cuda driver selects the most appropriate binary at runtime according to the compute capability of the present device. if an appropriate native binary cubin is not available, but the intermediate ptx code which targets an abstract virtual instruction set and is used for forwardcompatibility is available, then the kernel will be compiled just in time jit see compiler jit cache management tools from the ptx to the native cubin for the device. if the ptx is also not available, then the kernel launch will fail. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, the nvcc commandline option archsmxx can be used as a shorthand equivalent to the following more explicit gencode commandline options described above gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target by default due to the codecomputexx target it implies, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. . distributing the cuda runtime and libraries cuda applications are built against the cuda runtime library, which handles device, memory, and kernel management. unlike the cuda driver, the cuda runtime guarantees neither forward nor backward binary compatibility across versions. it is therefore best to redistribute the cuda runtime library with the application when using dynamic linking or else to statically link against the cuda runtime. this will ensure that the executable will be able to run even if the user does not have the same cuda toolkit installed that the application was built against. note when statically linking to the cuda runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously for example, if an application uses one version of the cuda runtime, and a plugin to that application is statically linked to a different version, that is perfectly acceptable, as long as the installed nvidia driver is sufficient for both. staticallylinked cuda runtime the easiest option is to statically link against the cuda runtime. this is the default if using nvcc to link in cuda and later. static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the cuda runtime library. dynamicallylinked cuda runtime if static linking against the cuda runtime is impractical for some reason, then a dynamicallylinked version of the cuda runtime library is also available. this was the default and only option provided in cuda versions and earlier. to use dynamic linking with the cuda runtime when using the nvcc from cuda or later to link the application, add the cudartshared flag to the link command line otherwise the staticallylinked cuda runtime library is used by default. after the application is dynamically linked against the cuda runtime, this version of the runtime library should be bundled with the application. it can be copied into the same directory as the application executable or into a subdirectory of that installation path. other cuda libraries although the cuda runtime provides the option of static linking, some libraries included in the cuda toolkit are available only in dynamicallylinked form. as with the dynamicallylinked version of the cuda runtime library, these libraries should be bundled with the application executable when distributing that application. .. cuda toolkit library redistribution the cuda toolkits enduser license agreement eula allows for redistribution of many of the cuda libraries under certain terms and conditions. this allows applications that depend on these libraries to redistribute the exact versions of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the cuda toolkit or perhaps none at all installed on their machines. please refer to the eula for details. note this does not apply to the nvidia driver the end user must still download and install an nvidia driver appropriate to their gpus and operating system. .. which files to redistribute when redistributing the dynamicallylinked versions of one or more cuda libraries, it is important to identify the exact files that need to be redistributed. the following examples use the cublas library from cuda toolkit as an illustration linux in a shared library on linux, there is a string field called the soname that indicates the binary compatibility level of the library. the soname of the library against which the application was built must match the filename of the library that is redistributed with the application. for example, in the standard cuda toolkit installation, the files libcublas.so and libcublas.so. are both symlinks pointing to a specific build of cublas, which is named like libcublas.so..x, where x is the build number e.g., libcublas.so... however, the soname of this library is given as libcublas.so. objdump p usrlocalcudaliblibcublas.so grep soname soname libcublas.so. because of this, even if lcublas with no version number specified is used when linking the application, the soname found at link time implies that libcublas.so. is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file or a symlink to the same that is redistributed with the application. the ldd tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the dynamic loader would select when loading the application given the current library search path ldd a.out grep libcublas libcublas.so. usrlocalcudaliblibcublas.so. mac in a shared library on mac os x, there is a field called the install name that indicates the expected installation path and filename the library the cuda libraries also use this filename to indicate binary compatibility. the value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime. for example, if the install name of the cublas library is given as rpathlibcublas..dylib, then the library is version and the copy of this library redistributed with the application must be named libcublas..dylib, even though only lcublas with no version number specified is used at link time. furthermore, this file should be installed into the rpath of the application see where to install redistributed cuda libraries. to view a librarys install name, use the otool l command otool l a.out a.out rpathlibcublas..dylib ... windows the binary compatibility version of the cuda libraries on windows is indicated as part of the filename. for example, a bit application linked to cublas will look for cublas.dll at runtime, so this is the file that should be redistributed with that application, even though cublas.lib is the file that the application is linked against. for bit applications, the file would be cublas.dll. to verify the exact dll filename that the application expects to find at runtime, use the dumpbin tool from the visual studio command prompt dumpbin imports a.exe microsoft r coffpe dumper version . copyright c microsoft corporation. all rights reserved. dump of file a.exe file type executable image section contains the following imports ... cublas.dll ... .. where to install redistributed cuda libraries once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them. on windows, if the cuda runtime or other dynamicallylinked cuda toolkit library is placed in the same directory as the executable, windows will locate it automatically. on linux and mac, the rpath linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths linuxmac nvcc i cudahomeinclude xlinker rpath origin cudartshared o myprogram myprogram.cu windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt cudartshared o releasemyprogram.exe myprogram.cu note it may be necessary to adjust the value of ccbin to reflect the location of your visual studio installation. to specify an alternate path where the libraries will be distributed, use linker options similar to those below linuxmac nvcc i cudahomeinclude xlinker rpath originlib cudartshared o myprogram myprogram.cu windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt delay cudartshared o releasemyprogram.exe myprogram.cu for linux and mac, the rpath option is used as before. for windows, the delay option is used this requires that the application call setdlldirectory before the first call to any cuda api function in order to specify the directory containing the cuda dlls. note for windows , setdefaultdlldirectories and adddlldirectory should be used instead of setdlldirectory. please see the msdn documentation for these routines for more information. . deployment infrastructure tools . nvidiasmi the nvidia system management interface nvidiasmi is a command line utility that aids in the management and monitoring of nvidia gpu devices. this utility allows administrators to query gpu device state and, with the appropriate privileges, permits administrators to modify gpu device state. nvidiasmi is targeted at tesla and certain quadro gpus, though limited support is also available on other nvidia gpus. nvidiasmi ships with nvidia gpu display drivers on linux, and with bit windows server r and windows . nvidiasmi can output queried information as xml or as humanreadable plain text either to standard output or to a file. see the nvidiasmi documenation for details. please note that new versions of nvidiasmi are not guaranteed to be backwardcompatible with previous versions. .. queryable state ecc error counts both correctable singlebit and detectable doublebit errors are reported. error counts are provided for both the current boot cycle and the lifetime of the gpu. gpu utilization current utilization rates are reported for both the compute resources of the gpu and the memory interface. active compute process the list of active processes running on the gpu is reported, along with the corresponding process nameid and allocated gpu memory. clocks and performance state max and current clock rates are reported for several important clock domains, as well as the current gpu performance state pstate. temperature and fan speed the current gpu core temperature is reported, along with fan speeds for products with active cooling. power management the current board power draw and power limits are reported for products that report these measurements. identification various dynamic and static information is reported, including board serial numbers, pci device ids, vbiosinforom version numbers and product names. .. modifiable state ecc mode enable and disable ecc reporting. ecc reset clear singlebit and doublebit ecc error counts. compute mode indicate whether compute processes can run on the gpu and whether they run exclusively or concurrently with other compute processes. persistence mode indicate whether the nvidia driver stays loaded when no applications are connected to the gpu. it is best to enable this option in most circumstances. gpu reset reinitialize the gpu hardware and software state via a secondary bus reset. . nvml the nvidia management library nvml is a cbased interface that provides direct access to the queries and commands exposed via nvidiasmi intended as a platform for building rdparty system management applications. the nvml api is shipped with the cuda toolkit since version and is also available standalone on the nvidia developer website as part of the gpu deployment kit through a single header file accompanied by pdf documentation, stub libraries, and sample applications see each new version of nvml is backwardcompatible. an additional set of perl and python bindings are provided for the nvml api. these bindings expose the same features as the cbased interface and also provide backwards compatibility. the perl bindings are provided via cpan and the python bindings via pypi. all of these products nvidiasmi, nvml, and the nvml language bindings are updated with each new cuda release and provide roughly the same functionality. see for additional information. . cluster management tools managing your gpu cluster will help achieve maximum gpu utilization and help you and your users extract the best possible performance. many of the industrys most popular cluster management tools support cuda gpus via nvml. for a listing of some of these tools, see . compiler jit cache management tools any ptx device code loaded by an application at runtime is compiled further to binary code by the device driver. this is called justintime compilation jit. justintime compilation increases application load time but allows applications to benefit from latest compiler improvements. it is also the only way for applications to run on devices that did not exist at the time the application was compiled. when jit compilation of ptx device code is used, the nvidia driver caches the resulting binary code on disk. some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables see just in time compilation of the cuda c programming guide. . cudavisibledevices it is possible to rearrange the collection of installed cuda devices that will be visible to and enumerated by a cuda application prior to the start of that application by way of the cudavisibledevices environment variable. devices to be made visible to the application should be included as a commaseparated list in terms of the systemwide list of enumerable devices. for example, to use only devices and from the systemwide list of devices, set cudavisibledevices, before launching the application. the application will then enumerate these devices as device and device , respectively. . recommendations and best practices this chapter contains a summary of the recommendations for optimization that are explained in this document. . overall performance optimization strategies performance optimization revolves around three basic strategies maximizing parallel execution optimizing memory usage to achieve maximum memory bandwidth optimizing instruction usage to achieve maximum instruction throughput maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible. once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible. this is done by carefully choosing the execution configuration of each kernel launch. the application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device. optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers. kernel access to global memory also should be minimized by maximizing the use of shared memory on the device. sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed. the effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory. the next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns. this optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles. shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts. as for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided. this suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision. finally, particular attention must be paid to control flow instructions due to the simt single instruction multiple thread nature of the device. . nvcc compiler switches . nvcc the nvidia nvcc compiler driver converts .cu files into c for the host system and cuda assembly or binary instructions for the device. it supports a number of commandline parameters, of which the following are especially useful for optimization and related best practices maxrregcountn specifies the maximum number of registers kernels can use at a perfile level. see register pressure. see also thelaunchbounds qualifier discussed in execution configuration of the cuda c programming guide to control the number of registers used on a perkernel basis. ptxasoptionsv or xptxasv lists perkernel register, shared, and constant memory usage. ftztrue denormalized numbers are flushed to zero precdivfalse less precise division precsqrtfalse less precise square root usefastmath compiler option of nvcc coerces every functionname call to the equivalent functionname call. this makes the code run faster at the cost of diminished precision and accuracy. see math libraries. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.",
    "depth": 3
  },
  {
    "url": "https://docs.nvidia.com/cuda/eula/index.html",
    "content": "end user license agreement nvidia software license agreement and cuda supplement to software license agreement. last updated october , the cuda toolkit end user license agreement applies to the nvidia cuda toolkit, the nvidia cuda samples, the nvidia display driver, nvidia nsight tools visual studio edition, and the associated documentation on cuda apis, programming model and development tools. if you do not agree with the terms and conditions of the license agreement, then do not download or use the software. last updated october , . preface the software license agreement in chapter and the supplement in chapter contain license terms and conditions that govern the use of nvidia cuda toolkit. by accepting this agreement, you agree to comply with all the terms and conditions applicable to the products included herein. nvidia driver description this package contains the operating system driver and fundamental system software components for nvidia gpus. nvidia cuda toolkit description the nvidia cuda toolkit provides commandline and graphical tools for building, debugging and optimizing the performance of applications accelerated by nvidia gpus, runtime and math libraries, and documentation including programming guides, user manuals, and api references. default install location of cuda toolkit windows platform programfilesnvidia gpu computing toolkitcudav. linux platform usrlocalcuda. mac platform developernvidiacuda. nvidia cuda samples description cuda samples are now located in which includes instructions for obtaining, building, and running the samples. they are no longer included in the cuda toolkit. nvidia nsight visual studio edition windows only description nvidia nsight development platform, visual studio edition is a development environment integrated into microsoft visual studio that provides tools for debugging, profiling, analyzing and optimizing your gpu computing and graphics applications. default install location of nsight visual studio edition windows platform programfilesxnvidia corporationnsight visual studio edition . . license agreement for nvidia software development kits important noticeread before downloading, installing, copying or using the licensed software this license agreement, including exhibits attached agreement is a legal agreement between you and nvidia corporation nvidia and governs your use of a nvidia software development kit sdk. each sdk has its own set of software and materials, but here is a description of the types of items that may be included in a sdk source code, header files, apis, data sets and assets examples include images, textures, models, scenes, videos, native api inputoutput files, binary software, sample code, libraries, utility programs, programming code and documentation. this agreement can be accepted only by an adult of legal age of majority in the country in which the sdk is used. if you are entering into this agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this agreement, in which case you will mean the entity you represent. if you dont have the required age or authority to accept this agreement, or if you dont accept all the terms and conditions of this agreement, do not download, install or use the sdk. you agree to use the sdk only for purposes that are permitted by a this agreement, and b any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions. . license .. license grant subject to the terms of this agreement, nvidia hereby grants you a nonexclusive, nontransferable license, without the right to sublicense except as expressly provided in this agreement to install and use the sdk, modify and create derivative works of sample source code delivered in the sdk, and distribute those portions of the sdk that are identified in this agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this agreement. .. distribution requirements these are the distribution requirements for you to exercise the distribution grant your application must have material additional functionality, beyond the included portions of the sdk. the distributable portions of the sdk shall only be accessed by your application. the following notice shall be included in modifications and derivative works of sample source code distributed this software contains source code provided by nvidia corporation. unless a developer tool is identified in this agreement as distributable, it is delivered for your internal use only. the terms under which you distribute your application must be consistent with the terms of this agreement, including without limitation terms relating to the license grant and license restrictions and protection of nvidias intellectual property rights. additionally, you agree that you will protect the privacy, security and legal rights of your application users. you agree to notify nvidia in writing of any known or suspected distribution or use of the sdk not in compliance with the requirements of this agreement, and to enforce the terms of your agreements with respect to distributed sdk. .. authorized users you may allow employees and contractors of your entity or of your subsidiaryies to access and use the sdk from your secure network to perform work on your behalf. if you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the sdk from your secure network. you are responsible for the compliance with the terms of this agreement by your authorized users. if you become aware that your authorized users didnt follow the terms of this agreement, you agree to take reasonable steps to resolve the noncompliance and prevent new occurrences. .. prerelease sdk the sdk versions identified as alpha, beta, preview or otherwise as prerelease, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of nvidia software and materials. use of a prerelease sdk may result in unexpected results, loss of data, project delays or other unpredictable damage or loss. you may use a prerelease sdk at your own risk, understanding that prerelease sdks are not intended for use in production or businesscritical systems. nvidia may choose not to make available a commercial version of any prerelease sdk. nvidia may also choose to abandon development and terminate the availability of a prerelease sdk at any time without liability. .. updates nvidia may, at its option, make available patches, workarounds or other updates to this sdk. unless the updates are provided with their separate governing terms, they are deemed part of the sdk licensed to you as provided in this agreement. you agree that the form and content of the sdk that nvidia provides may change without prior notice to you. while nvidia generally maintains compatibility between versions, nvidia may in some cases make changes that introduce incompatibilities in future versions of the sdk. .. components under other licenses the sdk may come bundled with, or otherwise include or be distributed with, nvidia or thirdparty components with separate legal notices or terms as may be described in proprietary notices accompanying the sdk. if and to the extent there is a conflict between the terms in this agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict. subject to the other terms of this agreement, you may use the sdk to develop and test applications released under open source initiative osi approved open source software licenses. .. reservation of rights nvidia reserves all rights, title, and interest in and to the sdk, not expressly granted to you under this agreement. . limitations the following license limitations apply to your use of the sdk you may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the sdk or copies of the sdk. except as expressly provided in this agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the sdk. for clarity, you may not distribute or sublicense the sdk as a standalone product. unless you have an agreement with nvidia for this purpose, you may not indicate that an application created with the sdk is sponsored or endorsed by nvidia. you may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the sdk. you may not use the sdk in any manner that would cause it to become subject to an open source software license. as examples, licenses that require as a condition of use, modification, andor distribution that the sdk be disclosed or distributed in source code form licensed for the purpose of making derivative works or redistributable at no charge. you acknowledge that the sdk as delivered is not tested or certified by nvidia for use in connection with the design, construction, maintenance, andor operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages each, a critical application. examples of critical applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. nvidia shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. you are solely responsible for ensuring that any product or service developed with the sdk as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements. you agree to defend, indemnify and hold harmless nvidia and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses including but not limited to attorneys fees and costs incident to establishing the right of indemnification arising out of or related to products or services that use the sdk in or for critical applications, and for use of the sdk outside of the scope of this agreement or not in compliance with its terms. you may not reverse engineer, decompile or disassemble any portion of the output generated using sdk elements for the purpose of translating such output artifacts to target a nonnvidia platform. . ownership nvidia or its licensors hold all rights, title and interest in and to the sdk and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under section .. this sdk may include software and materials from nvidias licensors, and these licensors are intended third party beneficiaries that may enforce this agreement with respect to their intellectual property rights. you hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the sdk, including their respective intellectual property rights, subject to nvidias rights under section .. you may, but dont have to, provide to nvidia suggestions, feature requests or other feedback regarding the sdk, including possible enhancements or modifications to the sdk. for any feedback that you voluntarily provide, you hereby grant nvidia and its affiliates a perpetual, nonexclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense through multiple tiers of sublicensees, and distribute through multiple tiers of distributors it without the payment of any royalties or fees to you. nvidia will use feedback at its choice. nvidia is constantly looking for ways to improve its products, so you may send feedback to nvidia through the developer portal at . no warranties the sdk is provided by nvidia as is and with all faults. to the maximum extent permitted by law, nvidia and its affiliates expressly disclaim all warranties of any kind or nature, whether express, implied or statutory, including, but not limited to, any warranties of merchantability, fitness for a particular purpose, title, noninfringement, or the absence of any defects therein, whether latent or patent. no warranty is made on the basis of trade usage, course of dealing or course of trade. . limitation of liability to the maximum extent permitted by law, nvidia and its affiliates shall not be liable for any i special, incidental, punitive or consequential damages, or ii damages for a any lost profits, loss of use, loss of data or loss of goodwill, or b the costs of procuring substitute products, arising out of or in connection with this agreement or the use or performance of the sdk, whether such liability arises from any claim based upon breach of contract, breach of warranty, tort including negligence, product liability or any other cause of action or theory of liability. in no event will nvidias and its affiliates total cumulative liability under or arising out of this agreement exceed us. the nature of the liability or the number of claims or suits shall not enlarge or extend this limit. these exclusions and limitations of liability shall apply regardless if nvidia or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. these exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this agreement, including, without limitation, the economic terms, would be substantially different. . termination this agreement will continue to apply until terminated by either you or nvidia as described below. if you want to terminate this agreement, you may do so by stopping to use the sdk. nvidia may, at any time, terminate this agreement if i you fail to comply with any term of this agreement and the noncompliance is not fixed within thirty days following notice from nvidia or immediately if you violate nvidias intellectual property rights ii you commence or participate in any legal proceeding against nvidia with respect to the sdk or iii nvidia decides to no longer provide the sdk in a country or, in nvidias sole discretion, the continued use of it is no longer commercially viable. upon any termination of this agreement, you agree to promptly discontinue use of the sdk and destroy all copies in your possession or control. your prior distributions in accordance with this agreement are not affected by the termination of this agreement. upon written request, you will certify in writing that you have complied with your commitments under this section. upon any termination of this agreement all provisions survive except for the license grant provisions. . general if you wish to assign this agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact nvidia to ask for permission. any attempted assignment not approved by nvidia in writing shall be void and of no effect. nvidia may assign, delegate or transfer this agreement and its rights and obligations, and if to a nonaffiliate you will be notified. you agree to cooperate with nvidia and provide reasonably requested information to verify your compliance with this agreement. this agreement will be governed in all respects by the laws of the united states and of the state of delaware, without regard to the conflicts of laws principles. the united nations convention on contracts for the international sale of goods is specifically disclaimed. you agree to all terms of this agreement in the english language. the state or federal courts residing in santa clara county, california shall have exclusive jurisdiction over any dispute or claim arising out of this agreement. notwithstanding this, you agree that nvidia shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction. if any court of competent jurisdiction determines that any provision of this agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect. unless otherwise specified, remedies are cumulative. each party acknowledges and agrees that the other is an independent contractor in the performance of this agreement. the sdk has been developed entirely at private expense and is commercial items consisting of commercial computer software and commercial computer software documentation provided with restricted rights. use, duplication or disclosure by the u.s. government or a u.s. government subcontractor is subject to the restrictions in this agreement pursuant to dfars a or as set forth in subparagraphs c and of the commercial computer software restricted rights clause at far , as applicable. contractormanufacturer is nvidia, san tomas expressway, santa clara, ca . the sdk is subject to united states export laws and regulations. you agree that you will not ship, transfer or export the sdk into any country, or use the sdk in any manner, prohibited by the united states bureau of industry and security or economic sanctions regulations administered by the u.s. department of treasurys office of foreign assets control ofac, or any applicable export laws, restrictions or regulations. these laws include restrictions on destinations, end users and end use. by accepting this agreement, you confirm that you are not located in a country currently embargoed by the u.s. or otherwise prohibited from receiving the sdk under u.s. law. any notice delivered by nvidia to you under this agreement will be delivered via mail, email or fax. you agree that any notices that nvidia sends you electronically will satisfy any legal communication requirements. please direct your legal notices or other correspondence to nvidia corporation, san tomas expressway, santa clara, california , united states of america, attention legal department. this agreement and any exhibits incorporated into this agreement constitute the entire agreement of the parties with respect to the subject matter of this agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this sdk license. any additional andor conflicting terms on documents issued by you are null, void, and invalid. any amendment or waiver under this agreement shall be in writing and signed by representatives of both parties. . cuda toolkit supplement to software license agreement for nvidia software development kits the terms in this supplement govern your use of the nvidia cuda toolkit sdk under the terms of your license agreement agreement as modified by this supplement. capitalized terms used but not defined below have the meaning assigned to them in the agreement. this supplement is an exhibit to the agreement and is incorporated as an integral part of the agreement. in the event of conflict between the terms in this supplement and the terms in the agreement, the terms in this supplement govern. . license scope the sdk is licensed for you to develop applications only for use in systems with nvidia gpus. . distribution the portions of the sdk that are distributable under the agreement are listed in attachment a. . operating systems those portions of the sdk designed exclusively for use on the linux or freebsd operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this agreement, provided that the object code files are not modified in any way except for unzipping of compressed files. . audio and video encoders and decoders you acknowledge and agree that it is your sole responsibility to obtain any additional thirdparty licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any thirdparty software and content relating to audio andor video encoders and decoders from, including but not limited to, microsoft, thomson, fraunhofer iis, sisvel s.p.a., mpegla, and coding technologies. nvidia does not grant to you under this agreement any necessary patent or other rights with respect to any audio andor video encoders and decoders. . licensing if the distribution terms in this agreement are not suitable for your organization, or for any questions regarding this agreement, please contact nvidia at nvidiacomputelicensequestionsnvidia.com. . attachment a the following cuda toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name as an example only, for release version of the bit windows software, the file cudart.dll is redistributable. component cuda runtime windows cudart.dll, cudartstatic.lib, cudadevrt.lib mac osx libcudart.dylib, libcudartstatic.a, libcudadevrt.a linux libcudart.so, libcudartstatic.a, libcudadevrt.a android libcudart.so, libcudartstatic.a, libcudadevrt.a component cuda fft library windows cufft.dll, cufftw.dll, cufft.lib, cufftw.lib mac osx libcufft.dylib, libcufftstatic.a, libcufftw.dylib, libcufftwstatic.a linux libcufft.so, libcufftstatic.a, libcufftw.so, libcufftwstatic.a android libcufft.so, libcufftstatic.a, libcufftw.so, libcufftwstatic.a component cuda blas library windows cublas.dll, cublaslt.dll mac osx libcublas.dylib, libcublaslt.dylib, libcublasstatic.a, libcublasltstatic.a linux libcublas.so, libcublaslt.so, libcublasstatic.a, libcublasltstatic.a android libcublas.so, libcublaslt.so, libcublasstatic.a, libcublasltstatic.a component nvidia dropin blas library windows nvblas.dll mac osx libnvblas.dylib linux libnvblas.so component cuda sparse matrix library windows cusparse.dll, cusparse.lib mac osx libcusparse.dylib, libcusparsestatic.a linux libcusparse.so, libcusparsestatic.a android libcusparse.so, libcusparsestatic.a component cuda linear solver library windows cusolver.dll, cusolver.lib mac osx libcusolver.dylib, libcusolverstatic.a linux libcusolver.so, libcusolverstatic.a android libcusolver.so, libcusolverstatic.a component cuda random number generation library windows curand.dll, curand.lib mac osx libcurand.dylib, libcurandstatic.a linux libcurand.so, libcurandstatic.a android libcurand.so, libcurandstatic.a component nvidia performance primitives library windows nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib mac osx libnppc.dylib, libnppcstatic.a, libnppial.dylib, libnppialstatic.a, libnppicc.dylib, libnppiccstatic.a, libnppicom.dylib, libnppicomstatic.a, libnppidei.dylib, libnppideistatic.a, libnppif.dylib, libnppifstatic.a, libnppig.dylib, libnppigstatic.a, libnppim.dylib, libnppisustatic.a, libnppitc.dylib, libnppitcstatic.a, libnpps.dylib, libnppsstatic.a linux libnppc.so, libnppcstatic.a, libnppial.so, libnppialstatic.a, libnppicc.so, libnppiccstatic.a, libnppicom.so, libnppicomstatic.a, libnppidei.so, libnppideistatic.a, libnppif.so, libnppifstatic.a libnppig.so, libnppigstatic.a, libnppim.so, libnppimstatic.a, libnppist.so, libnppiststatic.a, libnppisu.so, libnppisustatic.a, libnppitc.so libnppitcstatic.a, libnpps.so, libnppsstatic.a android libnppc.so, libnppcstatic.a, libnppial.so, libnppialstatic.a, libnppicc.so, libnppiccstatic.a, libnppicom.so, libnppicomstatic.a, libnppidei.so, libnppideistatic.a, libnppif.so, libnppifstatic.a libnppig.so, libnppigstatic.a, libnppim.so, libnppimstatic.a, libnppist.so, libnppiststatic.a, libnppisu.so, libnppisustatic.a, libnppitc.so libnppitcstatic.a, libnpps.so, libnppsstatic.a component nvidia jpeg library windows nvjpeg.lib, nvjpeg.dll linux libnvjpeg.so, libnvjpegstatic.a component internal common library required for statically linking to cublas, cusparse, cufft, curand, nvjpeg and npp mac osx libculibos.a linux libculibos.a component nvidia runtime compilation library and header all nvrtc.h windows nvrtc.dll, nvrtcbuiltins.dll mac osx libnvrtc.dylib, libnvrtcbuiltins.dylib linux libnvrtc.so, libnvrtcbuiltins.so, libnvrtcstatic.a, libnvrtxbuiltinsstatic.a component nvidia optimizing compiler library windows nvvm.dll mac osx libnvvm.dylib linux libnvvm.so component nvidia jit linking library windows libnvjitlink.dll, libnvjitlink.lib linux libnvjitlink.so, libnvjitlinkstatic.a component nvidia common device math functions library windows libdevice.bc mac osx libdevice.bc linux libdevice.bc component cuda occupancy calculation header library all cudaoccupancy.h component cuda half precision headers all cudafp.h, cudafp.hpp component cuda profiling tools interface cupti library windows cupti.dll mac osx libcupti.dylib linux libcupti.so component nvidia tools extension library windows nvtoolsext.dll, nvtoolsext.lib mac osx libnvtoolsext.dylib linux libnvtoolsext.so component nvidia cuda driver libraries linux libcuda.so, libnvidiaptxjitcompiler.so, libnvptxcompilerstatic.a component nvidia cuda file io libraries and header all cufile.h linux libcufile.so, libcufilerdma.so, libcufilestatic.a, libcufilerdmastatic.a in addition to the rights above, for parties that are developing software intended solely for use on jetson development kits or jetson modules, and running linux for tegra software, the following shall apply the sdk may be distributed in its entirety, as provided by nvidia, and without separation of its components, for you andor your licensees to create software development kits for use only on the jetson platform and running linux for tegra software. . attachment b additional licensing obligations the following third party components included in the software are licensed to licensee pursuant to the following terms and conditions licensees use of the gdb third party component is subject to the terms and conditions of gnu gpl v this product includes copyrighted thirdparty software licensed under the terms of the gnu general public license v gpl v. all thirdparty software packages are copyright by their respective authors. gpl v terms and conditions are hereby incorporated into the agreement by this reference consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. to obtain source code for software provided under licenses that require redistribution of source code, including the gnu general public license gpl and gnu lesser general public license lgpl, contact ossrequestsnvidia.com. this offer is valid for a period of three years from the date of the distribution of this product by nvidia corporation. component license cudagdb gpl v licensee represents and warrants that any and all third party licensing andor royalty payment obligations in connection with licensees use of the h. video codecs are solely the responsibility of licensee. licensees use of the thrust library is subject to the terms and conditions of the apache license version . all thirdparty software packages are copyright by their respective authors. apache license version terms and conditions are hereby incorporated into the agreement by this reference. in addition, licensee acknowledges the following notice thrust includes source code from the boost iterator, tuple, system, and random number libraries. boost software license version august th, . . . . permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license the software to use, reproduce, display, distribute, execute, and transmit the software, and to prepare derivative works of the software, and to permit thirdparties to whom the software is furnished to do so, all subject to the following the copyright notices in the software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the software, in whole or in part, and all derivative works of the software, unless such copies or derivative works are solely in the form of machineexecutable object code generated by a source language processor. the software is provided as is, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, title and noninfringement. in no event shall the copyright holders or anyone distributing the software be liable for any damages or other liability, whether in contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software. licensees use of the llvm third party component is subject to the following terms and conditions llvm release license university of illinoisncsa open source license copyright c university of illinois at urbanachampaign. all rights reserved. developed by llvm team university of illinois at urbanachampaign permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files the software, to deal with the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, andor sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation andor other materials provided with the distribution. neither the names of the llvm team, university of illinois at urbana champaign, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. the software is provided as is, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the contributors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings with the software. licensees use of the pcre third party component is subject to the following terms and conditions pcre licence pcre is a library of functions to support regular expressions whose syntax and semantics are as close as possible to those of the perl language. release of pcre is distributed under the terms of the bsd licence, as specified below. the documentation for pcre, supplied in the doc directory, is distributed under the same terms as the software itself. the basic library functions are written in c and are freestanding. also included in the distribution is a set of c wrapper functions, and a just intime compiler that can be used to optimize pattern matching. these are both optional features that can be omitted when the library is built. the basic library functions written by philip hazel email local part ph email domain cam.ac.uk university of cambridge computing service, cambridge, england. copyright c university of cambridge all rights reserved. pcre justintime compilation support written by zoltan herczeg email local part hzmester emain domain freemail.hu copyrightc zoltan herczeg all rights reserved. stackless justintime compiler written by zoltan herczeg email local part hzmester emain domain freemail.hu copyrightc zoltan herczeg all rights reserved. the c wrapper functions contributed by google inc. copyright c , google inc. all rights reserved. the bsd licence redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name of the university of cambridge nor the name of google inc. nor the names of their contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright owner or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the cublas library routines were written by or derived from code written by vasily volkov and are subject to the modified berkeley software distribution license as follows copyright c , regents of the university of california all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name of the university of california, berkeley nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the author as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the author be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the cublas library routines were written by or derived from code written by davide barbieri and are subject to the modified berkeley software distribution license as follows copyright c davide barbieri university of rome tor vergata. all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. the name of the author may not be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the author as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the author be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the cublas library routines were derived from code developed by the university of tennessee and are subject to the modified berkeley software distribution license as follows copyright c the university of tennessee. all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the documentation andor other materials provided with the distribution. neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright owner or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the cublas library routines were written by or derived from code written by jonathan hogg and are subject to the modified berkeley software distribution license as follows copyright c , the science and technology facilities council stfc. all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name of the stfc nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the stfc be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the cublas library routines were written by or derived from code written by ahmad m. abdelfattah, david keyes, and hatem ltaief, and are subject to the apache license, version , as follows c copyright king abdullah university of science and technology authors ahmad abdelfattah ahmad.ahmadkaust.edu.sa david keyes david.keyeskaust.edu.sa hatem ltaief hatem.ltaiefkaust.edu.sa redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name of the king abdullah university of science and technology nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright holders or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage some of the cusparse library routines were written by or derived from code written by liwen chang and are subject to the ncsa open source license as follows copyright c , university of illinois. all rights reserved. developed by impact group, university of illinois, permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files the software, to deal with the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, andor sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation andor other materials provided with the distribution. neither the names of impact group, university of illinois, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. the software is provided as is, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the contributors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings with the software. some of the curand library routines were written by or derived from code written by mutsuo saito and makoto matsumoto and are subject to the following license copyright c , mutsuo saito, makoto matsumoto and hiroshima university. all rights reserved. copyright c mutsuo saito, makoto matsumoto, hiroshima university and university of tokyo. all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name of the hiroshima university nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright owner or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the curand library routines were derived from code developed by d. e. shaw research and are subject to the following license copyright , d. e. shaw research. all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name of d. e. shaw research nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright owner or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. some of the math library routines were written by or derived from code developed by norbert juffa and are subject to the following license copyright c , norbert juffa all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met . redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. . redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright holder or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. licensees use of the lz third party component is subject to the following terms and conditions copyright c , yann collet. bsd clause license redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright owner or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage. the npp library uses code from the boost math toolkit, and is subject to the following license boost software license version august th, . . . . permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license the software to use, reproduce, display, distribute, execute, and transmit the software, and to prepare derivative works of the software, and to permit thirdparties to whom the software is furnished to do so, all subject to the following the copyright notices in the software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the software, in whole or in part, and all derivative works of the software, unless such copies or derivative works are solely in the form of machineexecutable object code generated by a source language processor. the software is provided as is, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, title and noninfringement. in no event shall the copyright holders or anyone distributing the software be liable for any damages or other liability, whether in contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software. portions of the nsight eclipse edition is subject to the following license the eclipse foundation makes available all content in this plugin content. unless otherwise indicated below, the content is provided to you under the terms and conditions of the eclipse public license version epl. a copy of the epl is available at http www.eclipse.orglegaleplv.html. for purposes of the epl, program will mean the content. if you did not receive this content directly from the eclipse foundation, the content is being redistributed by another party redistributor and different terms and conditions may apply to your use of any object code in the content. check the redistributors license that was provided with the content. if no such license exists, contact the redistributor. unless otherwise indicated below, the terms and conditions of the epl still apply to any source code in the content and such source code may be obtained at some of the cublas library routines uses code from openai, which is subject to the following license license url license text the mit license copyright c openai google inc. permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files the software, to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, andor sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software. the software is provided as is, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software. licensees use of the visual studio setup configuration samples is subject to the following license the mit license mit copyright c microsoft corporation. all rights reserved. permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files the software, to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, andor sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software. the software is provided as is, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software. licensees use of linmath.h header for cpu functions for gl vectormatrix operations from lunarg is subject to the apache license version . the dxcuda sample uses the ddx.h header, which is subject to the mit license . components of the driver and compiler used for binary management, including nvfatbin, nvcc, and cuobjdump, use the zstandard library which is subject to the following license bsd license for zstandard software copyright c meta platforms, inc. and affiliates. all rights reserved. redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation andor other materials provided with the distribution. neither the name facebook, nor meta, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. this software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall the copyright holder or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages including, but not limited to, procurement of substitute goods or services loss of use, data, or profits or business interruption however caused and on any theory of liability, whether in contract, strict liability, or tort including negligence or otherwise arising in any way out of the use of this software, even if advised of the possibility of such damage.",
    "depth": 4
  },
  {
    "url": "https://docs.nvidia.com/cuda/maxwell-compatibility-guide/index.html",
    "content": "maxwell compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia maxwell architecture. . maxwell compatibility . about this document this application note, maxwell compatibility guide for cuda applications, is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia maxwell architecture. this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with maxwell. . application compatibility on maxwell the nvidia cuda c compiler, nvcc, can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel. each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number. for example, cubin files that target compute capability are supported on all computecapability x kepler devices but are not supported on computecapability x maxwell devices. for this reason, to ensure forward compatibility with gpu architectures introduced after the application has been released, it is recommended that all applications include ptx versions of their kernels. note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default, keeping the ptx path strictly for forwardcompatibility purposes. applications that already include ptx versions of their kernels should work asis on maxwellbased gpus. applications that only support specific gpu architectures via cubin files, however, will need to be updated to provide maxwellcompatible ptx or cubins. . verifying maxwell compatibility for existing applications the first step is to check that maxwellcompatible device code at least ptx is compiled in to the application. the following sections show how to accomplish this for applications built with different cuda toolkit versions. .. applications using cuda toolkit or earlier cuda applications built using cuda toolkit versions through are compatible with maxwell as long as they are built to include ptx versions of their kernels. to test that ptx jit is working for your application, you can do the following download and install the latest driver from set the environment variable cudaforceptxjit. launch your application. when starting a cuda application for the first time with the above environment flag, the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code. if you set the environment variable above and then launch your program and it works properly, then you have successfully verified maxwell compatibility. note be sure to unset the cudaforceptxjit environment variable when you are done testing. .. applications using cuda toolkit or later cuda applications built using cuda toolkit or later are compatible with maxwell as long as they are built to include kernels in either maxwellnative cubin format see building applications with maxwell support or ptx format see applications using cuda toolkit or earlier or both. . building applications with maxwell support when a cuda application launches a kernel, the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available. if a cubin file supporting the architecture of the target gpu is available, it is used otherwise, the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it. if neither is available, then the kernel launch will fail. the method used to build your application with either native cubin or at least ptx support for maxwell depend on the version of the cuda toolkit used. the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx. all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. especially when using large libraries, this jit compilation can take a significant amount of time. the cuda driver will cache the cubins generated as a result of the ptx jit, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus, meaning that nativecompiled code may be faster or of greater accuracy. .. applications using cuda toolkit or earlier the compilers included in cuda toolkit or earlier generate cubin files native to earlier nvidia architectures such as fermi and kepler, but they cannot generate cubin files native to the maxwell architecture. to allow support for maxwell and future architectures when using version or earlier of the cuda toolkit, the compiler must generate a ptx version of each kernel. below are compiler settings that could be used to build mykernel.cu to run on fermi or kepler devices natively and on maxwell devices via ptx jit. note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide maxwell compatibility. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, you may be familiar with the simplified nvcc commandline option archsmxx, which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmxx expands to the following gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. .. applications using cuda toolkit or later with version of the cuda toolkit, nvcc can generate cubin files native to the firstgeneration maxwell architecture compute capability cuda toolkit and later further add native support for secondgeneration maxwell devices compute capability . when using cuda toolkit x or later, to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. . revision history version initial public release. version updated for secondgeneration maxwell compute capability . version use cuda c instead of cuda cc. updated cuda toolkit reference to and later. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated. future cuda toolkit version might deprecate support for the maxwell architecture.",
    "depth": 4
  },
  {
    "url": "https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html",
    "content": "nvidia cuda installation guide for linux the installation instructions for the cuda toolkit on linux. . introduction cuda is a parallel computing platform and programming model invented by nvidia. it enables dramatic increases in computing performance by harnessing the power of the graphics processing unit gpu. cuda was developed with several design goals in mind provide a small set of extensions to standard programming languages, like c, that enable a straightforward implementation of parallel algorithms. with cuda cc, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. support heterogeneous computation where applications use both the cpu and gpu. serial portions of applications are run on the cpu, and parallel portions are offloaded to the gpu. as such, cuda can be incrementally applied to existing applications. the cpu and gpu are treated as separate devices that have their own memory spaces. this configuration also allows simultaneous computation on the cpu and gpu without contention for memory resources. cudacapable gpus have hundreds of cores that can collectively run thousands of computing threads. these cores have shared resources including a register file and a shared memory. the onchip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. this guide will show you how to install and check the correct operation of the cuda development tools. . system requirements to use nvidia cuda on your system, you will need the following installed cudacapable gpu a supported version of linux with a gcc compiler and toolchain cuda toolkit available at the cuda development environment relies on tight integration with the host development environment, including the host compiler and c runtime libraries, and is therefore only supported on distribution versions that have been qualified for this cuda toolkit release. the following table lists the supported linux distributions. please review the footnotes associated with the table. table native linux distribution support in cuda update distribution kernel default gcc glibc x rhel y y . . rhel y y . . opensuse leap y y . . rocky linux y y . . rocky linux y y . . suse sles y y . . ubuntu lts . . ubuntu .z z lts . . ubuntu .z z lts . . debian x x . . debian y y . . debian z z . . fedora . . kylinos v sp ..v.ky . amazon linux . . arm sbsa rhel y y . . rhel y y . . suse sles y y . . ubuntu lts . . ubuntu lts z lts . . ubuntu .z z lts . . arm sbsa jetson dgpu . lts rel jp x .tegra . . lts rel jp.x .tegra . aarch jetson igpu lt ubuntu rel jp.x .tegra . . the following notes apply to the kernel versions supported by cuda for specific kernel versions supported on red hat enterprise linux rhel, visit a list of kernel versions including the release dates for suse linux enterprise server sles is available at lt provides a linux kernel and a sample root filesystem derived from ubuntu . for more details, visit . os support policy cuda support for ubuntu .x, ubuntu .x, rhel x, rhel x, rocky linux x, rocky linux x, suse sles x and opensuse leap x will be until the standard eoss as defined for each os. please refer to the support lifecycle for these oses to know their support timelines. cuda supports the latest fedora release version. for fedora release timelines, visit cuda supports a single kylinos release version. for details, visit refer to the support lifecycle for these supported oses to know their support timelines and plan to move to newer releases accordingly. . host compiler support policy in order to compile the cpu host code in the cuda source, the cuda compiler nvcc requires a compatible host compiler to be installed on the system. the version of the host compiler supported on linux platforms is tabulated as below. nvcc performs a version check on the host compilers major version and so newer minor versions of the compilers listed below will be supported, but major versions falling outside the range will not be supported. table supported compilers distribution gcc clang nvhpc xlc armcc icc x x x x no no arm sbsa x x x no . no for gcc and clang, the preceding table indicates the minimum version and the latest version supported. if you are on a linux distribution that may use an older version of gcc toolchain as default than what is listed above, it is recommended to upgrade to a newer toolchain cuda or later toolkit. newer gcc toolchains are available with the red hat developer toolset for example. for platforms that ship a compiler version older than gcc by default, linking to static or dynamic libraries that are shipped with the cuda toolkit is not supported. we only support libstdc gccs implementation for all the supported host compilers for the platforms listed above. .. supported c dialects nvcc and nvrtc cuda runtime compiler support the following c dialect c, c, c, c on supported host compilers. the default c dialect of nvcc is determined by the default dialect of the host compiler used for compilation. refer to host compiler documentation and the cuda programming guide for more details on language support. c is supported with the following flavors of host compiler in both host and device code. distribution gcc clang nvhpc arm cc x x x x x . about this document this document is intended for readers familiar with the linux environment and the compilation of c programs from the command line. you do not need previous experience with cuda or experience with parallel computation. note this guide covers installation only on systems with x windows installed. note many commands in this document might require superuser privileges. on most distributions of linux, this will require you to log in as root. for systems that have enabled the sudo package, use the sudo prefix for all necessary commands. . preinstallation actions some actions must be taken before the cuda toolkit and driver can be installed on linux verify the system has a cudacapable gpu. verify the system is running a supported version of linux. verify the system has gcc installed. verify the system has the correct kernel headers and development packages installed. download the nvidia cuda toolkit. handle conflicting installation methods. note you can override the installtime prerequisite checks by running the installer with the override flag. remember that the prerequisites will still be required to use the nvidia cuda toolkit. . verify you have a cudacapable gpu to verify that your gpu is cudacapable, go to your distributions equivalent of system properties, or, from the command line, enter lspci grep i nvidia if you do not see any settings, update the pci hardware database that linux maintains by entering updatepciids generally found in sbin at the command line and rerun the previous lspci command. if your graphics card is from nvidia and it is listed in your gpu is cudacapable. the release notes for the cuda toolkit also contain a list of supported products. . verify you have a supported version of linux the cuda development tools are only supported on some specific distributions of linux. these are listed in the cuda toolkit release notes. to determine which distribution and release number youre running, type the following at the command line uname m cat etcrelease you should see output similar to the following, modified for your particular system x red hat enterprise linux workstation release santiago the x line indicates you are running on a bit system. the remainder gives information about your distribution. . verify the system has gcc installed the gcc compiler is required for development using the cuda toolkit. it is not required for running cuda applications. it is generally installed as part of the linux installation, and in most cases the version of gcc installed with a supported version of linux will work correctly. to verify the version of gcc installed on your system, type the following on the command line gcc version if an error message displays, you need to install the development tools from your linux distribution or obtain a version of gcc and its accompanying toolchain from the web. . verify the system has the correct kernel headers and development packages installed the cuda driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. for example, if your system is running kernel version ., the . kernel headers and development packages must also be installed. while the runfile installation performs no package validation, the rpm and deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed. however, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using. therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the cuda drivers, as well as whenever you change the kernel version. the version of the kernel your system is running can be found by running the following command uname r this is the version of the kernel headers and development packages that must be installed prior to installing the cuda drivers. this command will be used multiple times below to specify the version of the packages to install. note that below are the commoncase scenarios for kernel usage. more advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running. note if you perform a system update which changes the version of the linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed. otherwise, the cuda driver will fail to work with the new kernel. . install gpudirect storage if you intend to use gpudirectstorage gds, you must install the cuda package and mlnxofed package. gds packages can be installed using the cuda packaging guide. follow the instructions in mlnxofed requirements and installation. gds is supported in two different modes gds defaultfull perf mode and compatibility mode. installation instructions for them differ slightly. compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations. full gds support is restricted to the following linux distros ubuntu , ubuntu rhel , rhel , rhel starting with cuda toolkit ., gds kernel driver package nvidiagds version . provided by nvidiafsdkms . and above is only supported with the nvidia open kernel driver. follow the instructions in removing cuda toolkit and driver to remove existing nvidia driver packages and then follow instructions in nvidia open gpu kernel modules to install nvidia open kernel driver packages. . choose an installation method the cuda toolkit can be installed using either of two different installation mechanisms distributionspecific packages rpm and deb packages, or a distributionindependent package runfile packages. the distributionindependent package has the advantage of working across a wider set of linux distributions, but does not update the distributions native package management system. the distributionspecific packages interface with the distributions native package management system. it is recommended to use the distributionspecific packages, where possible. note for both native as well as cross development, the toolkit must be installed using the distributionspecific installer. see the cuda crossplatform installation section for more details. . download the nvidia cuda toolkit the nvidia cuda toolkit is available at choose the platform you are using and download the nvidia cuda toolkit. the cuda toolkit contains the cuda driver and tools needed to create, build and run a cuda application as well as libraries, header files, and other resources. download verification the download can be verified by comparing the md checksum posted at with that of the downloaded file. if either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. to calculate the md checksum of the downloaded file, run the following mdsum file . address custom xorg.conf, if applicable the driver relies on an automatically generated xorg.conf file at etcxxorg.conf. if a custombuilt xorg.conf file is present, this functionality will be disabled and the driver may not work. you can try removing the existing xorg.conf file, or adding the contents of etcxxorg.conf.dnvidia.conf to the xorg.conf file. the xorg.conf file will most likely need manual tweaking for systems with a nontrivial gpu configuration. . handle conflicting installation methods before installing cuda, any previous installations that could conflict should be uninstalled. this will not affect systems which have not had cuda installed previously, or systems where the installation method has been preserved rpmdeb vs. runfile. see the following charts for specifics. table cuda toolkit installation compatibility matrix installed toolkit version x.y installed toolkit version x.y rpmdeb run rpmdeb run installing toolkit version x.y rpmdeb no action uninstall run no action no action run uninstall rpmdeb uninstall run no action no action table nvidia driver installation compatibility matrix installed driver version x.y installed driver version x.y rpmdeb run rpmdeb run installing driver version x.y rpmdeb no action uninstall run no action uninstall run run uninstall rpmdeb no action uninstall rpmdeb no action use the following command to uninstall a toolkit runfile installation sudo usrlocalcudax.ybincudauninstaller use the following command to uninstall a driver runfile installation sudo usrbinnvidiauninstall use the following commands to uninstall an rpmdeb installation sudo dnf remove packagename rhel rocky linux sudo dnf remove packagename fedora sudo zypper remove packagename opensuse sles sudo aptget purge remove packagename ubuntu . package manager installation basic instructions can be found in the quick start guide. read on for more detailed instructions. . overview installation using rpm or debian packages interfaces with your systems package management system. when using rpm or debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in var. such a package only informs the package manager where to find the actual installation packages, but will not install them. if the online network repository is enabled, rpm or debian packages will be automatically downloaded at installation time using the package manager aptget, dnf, yum, or zypper. distributionspecific instructions detail how to install cuda rhel rocky linux rhel rocky linux kylinos fedora sles opensuse wsl ubuntu debian amazon linux finally, some helpful package manager capabilities are detailed. these instructions are for native development only. for crossplatform development, see the cuda crossplatform environment section. note optional components such as nvidiafs, libnvidianscq, and fabricmanager are not installed by default and will have to be installed separately as needed. . rhel rocky .. prepare rhel rocky perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo dnf install kerneldeveluname r kernelheadersuname r if matching kernelheaders and kerneldevel packages are not available for the currently running kernel version, you may need to use the previously shipped version of these packages. see for more information. satisfy thirdparty package dependency satisfy dkms dependency the nvidia driver rpm packages depend on other external packages, such as dkms and libvdpau. those packages are only available on thirdparty repositories, such as epel. any such thirdparty repositories must be added to the package manager repository database before installing the nvidia driver rpm packages, or missing dependencies will prevent the installation from proceeding. to enable epel sudo dnf install enable optional repos on rhel linux only, execute the following steps to enable optional repositories. on x systems subscriptionmanager repos enablerhelforxappstreamrpms subscriptionmanager repos enablerhelforxbaseosrpms subscriptionmanager repos enablecodereadybuilderforrhelxrpms remove outdated signing key sudo rpm erase gpgpubkeyfaaf choose an installation method local repo or network repo. .. local repo installation for rhel rocky install local repository on file system sudo rpm install cudarepodistroxylocalversion.arch.rpm .. network repo installation for rhel rocky enable the network repo sudo dnf configmanager addrepo where distroarch should be replaced by one of the following rhelcrosslinuxsbsa rhelsbsa rhelx install the new cuda public gpg key the new gpg public key for the cuda repository rpmbased distros is dd. on a fresh installation of rhel, the dnf package manager will prompt the user to accept new keys when installing packages the first time. indicate you accept the change when prompted. for upgrades, you must also also fetch an updated .repo entry sudo dnf configmanager addrepo clean yum repository cache sudo dnf clean expirecache .. common instructions for rhel rocky these instructions apply to both local and network installation. install cuda sdk sudo dnf module install nvidiadriverlatestdkms sudo dnf install cudatoolkit install gpudirect filesystem sudo dnf install nvidiagds add libcuda.so symbolic link, if necessary the libcuda.so library is installed in the usrlib,nvidia directory. for preexisting projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the usrlib, directory. reboot the system sudo reboot perform the postinstallation actions. . rhel rocky .. prepare rhel rocky perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo dnf install kerneldeveluname r kernelheadersuname r satisfy thirdparty package dependency satisfy dkms dependency the nvidia driver rpm packages depend on other external packages, such as dkms and libvdpau. those packages are only available on thirdparty repositories, such as epel. any such thirdparty repositories must be added to the package manager repository database before installing the nvidia driver rpm packages, or missing dependencies will prevent the installation from proceeding. to enable epel sudo dnf install enable optional repos on rhel linux only, execute the following steps to enable optional repositories. on x systems subscriptionmanager repos enablerhelforxappstreamrpms subscriptionmanager repos enablerhelforxbaseosrpms subscriptionmanager repos enablecodereadybuilderforrhelxrpms remove outdated signing key sudo rpm erase gpgpubkeyfaaf choose an installation method local repo or network repo. .. local repo installation for rhel rocky install local repository on file system sudo rpm install cudarepodistroxylocalversion.arch.rpm .. network repo installation for rhel rocky enable the network repo sudo dnf configmanager addrepo where distroarch should be replaced by one of the following rhelcrosslinuxsbsa rhelsbsa rhelx install the new cuda public gpg key the new gpg public key for the cuda repository rpmbased distros is dd. on a fresh installation of rhel, the dnf package manager will prompt the user to accept new keys when installing packages the first time. indicate you accept the change when prompted. for upgrades, you must also also fetch an updated .repo entry sudo dnf configmanager addrepo clean yum repository cache sudo dnf clean expirecache .. common instructions for rhel rocky these instructions apply to both local and network installation. install cuda sdk sudo dnf module install nvidiadriverlatestdkms sudo dnf install cudatoolkit install gpudirect filesystem sudo dnf install nvidiagds add libcuda.so symbolic link, if necessary the libcuda.so library is installed in the usrlib,nvidia directory. for preexisting projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the usrlib, directory. reboot the system sudo reboot perform the postinstallation actions. . kylinos .. prepare kylinos perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo dnf install kerneldeveluname r kernelheadersuname r choose an installation method local repo or network repo. .. local repo installation for kylinos install local repository on file system sudo rpm install cudarepokylinxylocalversion.arch.rpm .. network repo installation for kylinos enable the network repo sudo dnf configmanager addrepo install the new cuda public gpg key the new gpg public key for the cuda repository rpmbased distros is dd. on a fresh installation of rhel, the dnf package manager will prompt the user to accept new keys when installing packages the first time. indicate you accept the change when prompted. clean yum repository cache sudo dnf clean expirecache .. common instructions for kylinos these instructions apply to both local and network installation. install cuda sdk sudo dnf module install nvidiadriverlatestdkms sudo dnf install cudatoolkit install gpudirect filesystem sudo dnf install nvidiagds add libcuda.so symbolic link, if necessary the libcuda.so library is installed in the usrlib,nvidia directory. for preexisting projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the usrlib, directory. reboot the system sudo reboot perform the postinstallation actions. . fedora .. prepare fedora perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo dnf install kerneldeveluname r kernelheadersuname r remove outdated signing key sudo rpm erase gpgpubkeyfaaf choose an installation method local repo or network repo. .. local repo installation for fedora install local repository on file system sudo rpm install cudarepodistroxylocalversion.x.rpm where distro is fedora or fedora, for example. .. network repo installation for fedora enable the network repo sudo dnf configmanager addrepo where distro should be replaced by one of the following fedora fedora install the new cuda public gpg key the new gpg public key for the cuda repository rpmbased distros is dd. on a fresh installation of fedora, the dnf package manager will prompt the user to accept new keys when installing packages the first time. indicate you accept the change when prompted. for upgrades, you must also fetch an updated .repo entry sudo dnf configmanager addrepo clean dnf repository cache sudo dnf clean expirecache .. common installation instructions for fedora these instructions apply to both local and network installation for fedora. install cuda sdk sudo dnf module install nvidiadriverlatestdkms sudo dnf install cudatoolkit note the cuda driver installation may fail if the rpmfusion nonfree repository is enabled. in this case, cuda installations should temporarily disable the rpmfusion nonfree repository. sudo dnf disablereporpmfusionnonfree install cuda it may be necessary to rebuild the grub configuration files, particularly if you use a nondefault partition scheme. if so, then run this below command, and reboot the system sudo grubmkconfig o bootgrubgrub.cfg reboot the system sudo reboot add libcuda.so symbolic link, if necessary the libcuda.so library is installed in the usrlib,nvidia directory. for preexisting projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the usrlib, directory. perform the postinstallation actions. . sles .. prepare sles perform the preinstallation actions. the kernel development packages for the currently running kernel can be installed with sudo zypper install y kernelvariantdevelversion to run the above command, you will need the variant and version of the currently running kernel. use the output of the uname command to determine the currently running kernels variant and version uname r .default in the above example, the variant is default and version is .. the kernel development packages for the default kernel variant can be installed with sudo zypper install y kerneldefaultdeveluname r sed sdefault the kernel headers and development packages for the currently running kernel can be installed with sudo zypper install y kernelvariantdevelversion on sles sp, install the mesalibgldevel linux packages before proceeding. see mesalibgldevel. add the user to the video group sudo usermod a g video username remove outdated signing key sudo rpm erase gpgpubkeyfaaf choose an installation method local repo or network repo. .. local repo installation for sles install local repository on file system sudo rpm install cudareposlesxylocalversion.x.rpm .. network repo installation for sles enable the network repo sudo zypper addrepo where distroarch should be replaced by one of the following slescrosslinuxsbsa slessbsa slesx install the new cuda public gpg key the new gpg public key for the cuda repository rpmbased distros is dd. on a fresh installation of sles, the zypper package manager will prompt the user to accept new keys when installing packages the first time. indicate you accept the change when prompted. for upgrades, you must also also fetch an updated .repo entry sudo zypper removerepo cudadistroarch sudo zypper addrepo refresh zypper repository cache sudo suseconnect product packagehubarchitecture sudo zypper refresh .. common installation instructions for sles these instructions apply to both local and network installation for sles. install cuda sdk sudo zypper install cudatoolkit install cuda samples gl dependencies refer to cuda crossplatform samples. reboot the system sudo reboot perform the postinstallation actions. . opensuse .. prepare opensuse perform the preinstallation actions. the kernel development packages for the currently running kernel can be installed with sudo zypper install y kernelvariantdevelversion to run the above command, you will need the variant and version of the currently running kernel. use the output of the uname command to determine the currently running kernels variant and version uname r .default in the above example, the variant is default and version is .. the kernel development packages for the default kernel variant can be installed with sudo zypper install y kerneldefaultdeveluname r sed sdefault add the user to the video group sudo usermod a g video username remove outdated signing key sudo rpm erase gpgpubkeyfaaf choose an installation method local repo or network repo. .. local repo installation for opensuse install local repository on file system sudo rpm install cudarepoopensuseversion.x.rpm .. network repo installation for opensuse enable the network repo sudo zypper addrepo install the new cuda public gpg key the new gpg public key for the cuda repository rpmbased distros is dd. on fresh installation of opensuse, the zypper package manager will prompt the user to accept new keys when installing packages the first time. indicate you accept the change when prompted. for upgrades, you must also also fetch an updated .repo entry sudo zypper removerepo cudaopensusex sudo zypper addrepo refresh zypper repository cache sudo zypper refresh .. common installation instructions for opensuse these instructions apply to both local and network installation for opensuse. install cuda sdk sudo zypper install cudatoolkit reboot the system sudo reboot perform the postinstallation actions. . wsl these instructions must be used if you are installing in a wsl environment. do not use the ubuntu instructions in this case it is important to not install the cudadrivers packages within the wsl environment. .. prepare wsl perform the preinstallation actions. remove outdated signing key sudo aptkey del faaf choose an installation method local repo or network repo. .. local repo installation for wsl install local repositiry on file system sudo dpkg i cudarepowslubuntuxylocalversionx.deb enroll ephemeral public gpg key sudo cp varcudarepowslubuntuxylocalcudakeyring.gpg usrsharekeyrings .. network repo installation for wsl the new gpg public key for the cuda repository debianbased distros is bfcc. this must be enrolled on the system, either using the cudakeyring package or manually the aptkey command is deprecated and not recommended. install the newcudakeyring package wget sudo dpkg i cudakeyring.all.deb or if you are unable to install the cudakeyring package, you can optionally enroll the new signing key manually wget sudo mv cudaarchivekeyring.gpg usrsharekeyringscudaarchivekeyring.gpg enable the network repository echo deb signedbyusrsharekeyringscudaarchivekeyring.gpg sudo tee etcaptsources.list.dcudawslubuntux.list add pin file to prioritize cuda repository wget sudo mv cudawslubuntu.pin etcaptpreferences.dcudarepositorypin .. common installation instructions for wsl these instructions apply to both local and network installation for wsl. update the apt repository cache sudo aptget update install cuda sdk sudo aptget install cudatoolkit perform the postinstallation actions. . ubuntu .. prepare ubuntu perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo aptget install linuxheadersuname r remove outdated signing key sudo aptkey del faaf choose an installation method local repo or network repo. .. local repo installation for ubuntu install local repository on file system sudo dpkg i cudarepodistroversionarchitecture.deb enroll ephemeral public gpg key sudo cp varcudarepodistroxylocalcudakeyring.gpg usrsharekeyrings add pin file to prioritize cuda repository wget sudo mv cudadistro.pin etcaptpreferences.dcudarepositorypin .. network repo installation for ubuntu the new gpg public key for the cuda repository is bfcc. this must be enrolled on the system, either using the cudakeyring package or manually the aptkey command is deprecated and not recommended. install the new cudakeyring package wget sudo dpkg i cudakeyring.all.deb where distroarch should be replaced by one of the following ubuntux ubuntucrosslinuxsbsa ubuntusbsa ubuntux ubuntucrosslinuxaarch ubuntuarm ubuntucrosslinuxsbsa ubuntusbsa ubuntux ubuntusbsa ubuntux note armjetson repos native distroarm cross distrocrosslinuxaarch sudo dpkg i cudakeyring.all.deb or if you are unable to install the cudakeyring package, you can optionally enroll the new signing key manually wget sudo mv cudaarchivekeyring.gpg usrsharekeyringscudaarchivekeyring.gpg enable the network repository echo deb signedbyusrsharekeyringscudaarchivekeyring.gpg sudo tee etcaptsources.list.dcudadistroarch.list add pin file to prioritize cuda repository wget sudo mv cudadistro.pin etcaptpreferences.dcudarepositorypin .. common installation instructions for ubuntu these instructions apply to both local and network installation for ubuntu. update the apt repository cache sudo aptget update install cuda sdk note these two commands must be executed separately. sudo aptget install cudatoolkit to include all gds packages sudo aptget install nvidiagds reboot the system sudo reboot perform the postinstallation actions . debian .. prepare debian perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo aptget install linuxheadersuname r enable the contrib repository sudo addaptrepository contrib remove outdated signing key sudo aptkey del faaf choose an installation method local repo or network repo. .. local repo installation for debian install local repository on file system sudo dpkg i cudarepodistroxylocalversionx.deb enroll ephemeral public gpg key sudo cp varcudarepodistroxylocalcudakeyring.gpg usrsharekeyrings .. network repo installation for debian the new gpg public key for the cuda repository debianbased distros is bfcc. this must be enrolled on the system, either using the cudakeyring package or manually the aptkey command is deprecated and not recommended. install the new cudakeyring package wget where distroarch should be replaced by one of the following debianx debianx sudo dpkg i cudakeyring.all.deb or if you are unable to install the cudakeyring package, you can optionally enroll the new signing key manually wget sudo mv cudaarchivekeyring.gpg usrsharekeyringscudaarchivekeyring.gpg enable the network repository echo deb signedbyusrsharekeyringscudaarchivekeyring.gpg sudo tee etcaptsources.list.dcudadistrox.list .. common installation instructions for debian these instructions apply to both local and network installation for debian. update the apt repository cache sudo aptget update note if you are using debian , you may instead need to run sudo aptget allowreleaseinfochange update install cuda sdk sudo aptget y install cuda reboot the system sudo reboot perform the postinstallation actions. . amazon linux .. prepare amazon linux perform the preinstallation actions. the kernel headers and development packages for the currently running kernel can be installed with sudo dnf install kerneldeveluname r kernelheadersuname r kernelmodulesextrauname r choose an installation method local repo or network repo. .. local repo installation for amazon linux install local repository on file system sudo rpm install cudarepoamznxylocalversion.x.rpm .. network repo installation for amazon linux enable the network repository sudo dnf configmanager addrepo clean dnf repository cache sudo dnf clean expirecache .. common installation instructions for amazon linux these instructions apply to both local and network installation for amazon linux. install cuda sdk sudo dnf module install nvidiadriverlatestdkms sudo dnf install cudatoolkit install gpudirect filesystem sudo dnf install nvidiagds add libcuda.so symbolic link, if necessary the libcuda.so library is installed in the usrlib,nvidia directory. for preexisting projects which use libcuda.so, it may be useful to add a symbolic link from libcuda.so in the usrlib, directory. reboot the system sudo reboot perform the postinstallation actions. . additional package manager capabilities below are some additional capabilities of the package manager that users can take advantage of. .. available packages the recommended installation package is the cuda package. this package will install the full set of other cuda packages required for native development and should cover most scenarios. the cuda package installs all the available packages for native developments. that includes the compiler, the debugger, the profiler, the math libraries, and so on. for x platforms, this also includes nsight eclipse edition and the visual profilers. it also includes the nvidia driver package. on supported platforms, the cudacrossaarch and cudacrosssbsa packages install all the packages required for crossplatform development to armjetson and armserver, respectively. the libraries and header files of the target architectures display driver package are also installed to enable the cross compilation of driver applications. the cudacrossarch packages do not install the native display driver. note bit compilation native and crosscompilation is removed from cuda and later toolkit. use the cuda toolkit from earlier releases for bit compilation. cuda driver will continue to support running existing bit applications on existing gpus except hopper. hopper does not support bit applications. ada will be the last architecture with driver support for bit applications. the packages installed by the packages above can also be installed individually by specifying their names explicitly. the list of available packages be can obtained with yum disablerepo enablerepocuda list available redhat dnf disablerepo enablerepocuda list available fedora zypper packages r cuda opensuse sles cat varlibaptlistscudapackages grep package ubuntu .. meta packages meta packages are rpmdebconda packages which contain no or few files but have multiple dependencies. they are used to install many cuda packages when you may not know the details of the packages you want. the following table lists the meta packages. table meta packages available for cuda meta package purpose cuda installs all cuda toolkit and driver packages. handles upgrading to the next version of the cuda package when its released. cuda installs all cuda toolkit and driver packages. remains at version until an additional version of cuda is installed. cudatoolkit installs all cuda toolkit packages required to develop cuda applications. does not include the driver. cudatoolkit installs all cuda toolkit packages required to develop applications. will not upgrade beyond the x series toolkits. does not include the driver. cudatoolkit installs all cuda toolkit packages required to develop applications. handles upgrading to the next x version of cuda when its released. does not include the driver. cudatools installs all cuda command line and visual tools. cudaruntime installs all cuda toolkit packages required to run cuda applications, as well as the driver packages. cudacompiler installs all cuda compiler packages. cudalibraries installs all runtime cuda library packages. cudalibrariesdev installs all development cuda library packages. cudadrivers installs all nvidia driver packages with proprietary kernel modules. handles upgrading to the next version of the driver packages when theyre released. cudadrivers installs all nvidia driver packages with proprietary kernel modules. will not upgrade beyond the branch drivers. .. optional bit packages for linux x .deb.rpm these packages provide bit driver libraries needed for things such as steam popular game app storelauncher, older video games, and some compute applications. for debian and debian sudo dpkg addarchitecture i sudo aptget update sudo aptget install libcudai nvidiadriverlibsi for debian sudo dpkg addarchitecture i sudo aptget update apt install nvidiadriverlibsi for ubuntu sudo dpkg addarchitecture i sudo aptget update sudo aptget install libnvidiacomputebranchi libnvidiadecodebranchi libnvidiaencodebranchi libnvidiaextrabranchi libnvidiafbcbranchi libnvidiaglbranchi where branch is the driver version, for example . for fedora and rhel sudo dnf install nvidiadrivercudalibs.i nvidiadriverdevel.i nvidiadriverlibs.i nvidiadrivernvfbcopengl.i nvidiadrivernvml.i note there is no modularity profile support. for opensusesles sudo zypper install nvidiacomputegbit nvidiaglgbit nvidiavideogbit .. package upgrades the cuda package points to the latest stable release of the cuda toolkit. when a new version is available, use the following commands to upgrade the toolkit and driver sudo dnf install cudatoolkit fedora, rhel, rhel, and kylinos sudo zypper install cudatoolkit opensuse and sles sudo aptget install cudatoolkit ubuntu and debian the cudacrossarch packages can also be upgraded in the same manner. the cudadrivers package points to the latest driver release available in the cuda repository. when a new version is available, use the following commands to upgrade the driver sudo dnf module install nvidiadriverlatestdkms fedora, rhel, rhel, and kylinos sudo zypper install cudadrivers nvidiagfxgkmpdefault opensuse and sles sudo aptget install cudadrivers ubuntu and debian some desktop environments, such as gnome or kde, will display a notification alert when new packages are available. to avoid any automatic upgrade, and lock down the toolkit installation to the x.y release, install the cudaxy or cudacrossarchxy package. sidebyside installations are supported. for instance, to install both the x.y cuda toolkit and the x.y cuda toolkit, install the cudax.y and cudax.y packages. . driver installation this section is for users who want to install a specific driver version. for debian and ubuntu sudo aptget install cudadriversdriverbranch for example sudo aptget install cudadrivers for opensuse and sles sudo zypper v install cudadriversdriverbranch for example sudo zypper v install cudadrivers this allows you to get the highest version in the specified branch. for fedora and rhel sudo dnf module install nvidiadriverstreamprofile where profile by default is default and does not need to be specified. example dkms streams dkms or latestdkms example precompiled streams or latest note precompiled streams are only supported on rhel x and rhel x. to uninstall or change streams on fedora and rhel sudo dnf module remove all nvidiadriver sudo dnf module reset nvidiadriver . nvidia open gpu kernel modules the nvidia linux gpu driver contains several kernel modules nvidia.ko nvidiamodeset.ko nvidiauvm.ko nvidiadrm.ko nvidiapeermem.ko starting in the driver release series, two flavors of these kernel modules are provided proprietary this is the flavor that nvidia has historically shipped. opensource published kernel modules that are dual licensed mitgplv. these are new starting in release . with every driver release, the source code to the open kernel modules will be published on and a tarball will be provided on verify that your nvidia gpu is at least turing or newer generation. lspci grep vga experimental support for geforce and quadro skus can be enabled with echo options nvidia nvregopenrmenableunsupportedgpus sudo tee etcmodprobe.dnvidiagsp.conf to install nvidia open gpu kernel modules, follow the instructions below. . cuda runfile pass the cli argument to the cuda runfile to opt in to nvidia open gpu kernel modules sh cudareleaseversionlinux.run mkernelopen . debian install the nvidia open gpu kernel modules package sudo aptget install nvidiakernelopendkms install the rest of the nvidia driver packages sudo aptget install cudadrivers or to install a specific driver version install the nvidia open gpu kernel modules package sudo aptget install v nvidiakernelopendkmsversion install the rest of the nvidia driver packages sudo aptget install v cudadriversdriverbranch for example sudo aptget install v nvidiakernelopendkms. sudo aptget install v cudadrivers . fedora install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriveropendkms or to install a specific driver version install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriverdriverbranchopen . kylinos install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriveropendkms or to install a specific driver version install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriverdriverbranchopen . rhel and rocky install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriveropendkms or to install a specific driver version install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriverdriverbranchopen . rhel and rocky install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriveropendkms or to install a specific driver version install the nvidia open gpu kernel modules package and the rest of the nvidia driver packages sudo dnf module install nvidiadriverdriverbranchopen . opensuse and sles install the nvidia open gpu kernel modules package sudo zypper install nvidiaopendrivergkmpdefault install the rest of the nvidia driver packages sudo zypper install cudadrivers or to install a specific driver version install the nvidia open gpu kernel modules package sudo zypper v install zypper search s nvidiaopendrivergkmpflavor sed s g awk f driverbranch print install the rest of the nvidia driver packages sudo zypper v install cudadriversdriverbranch for example sudo zypper v install zypper search s nvidiaopendrivergkmpdefault sed s g awk f print sudo zypper v install cudadrivers . ubuntu install the nvidia open gpu kernel modules package sudo aptget install nvidiadriverdriverbranchopen install the rest of the nvidia driver packages sudo aptget install cudadriversdriverbranch note endusers on ubuntu should upgrade their nvidia open gpu kernel modules using the following sudo aptget install verboseversions nvidiakernelsourceopen cudadrivers or to install a specific driver version install the nvidia open gpu kernel modules package sudo aptget install v nvidiadriverdriverbranchopen install the rest of the nvidia driver packages sudo aptget install v cudadriversdriverbranch for example sudo aptget install v nvidiadriveropen sudo aptget install v cudadrivers . precompiled streams precompiled streams offer an optional method of streamlining the installation process. the advantages of precompiled streams precompiled faster boot up after driver andor kernel updates pretested kernel and driver combination has been validated removes gcc dependency no compiler installation required removes dkms dependency enabling epel repository not required removes kerneldevel and kernelheaders dependencies no black screen if matching packages are missing when using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale .ko files. to prevent system breakages, the nvidia dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists. this can delay the application of security fixes but ensures that a tested kernel and driver combination is always used. a warning is displayed by dnf during that upgrade situation note skipping kernel installation since no nvidia driver kernel module package kmodnvidiadriverkernel ... could be found packaging templates and instructions are provided on github to allow you to maintain your own precompiled kernel module packages for custom kernels and derivative linux distros nvidiayumpackagingprecompiledkmod to use the new driver packages on rhel or rhel first, ensure that the red hat repositories are enabled rhel subscriptionmanager repos enablerhelforxappstreamrpms subscriptionmanager repos enablerhelforxbaseosrpms or rhel subscriptionmanager repos enablerhelforxappstreamrpms subscriptionmanager repos enablerhelforxbaseosrpms choose one of the four options below depending on the desired driver latest always updates to the highest versioned driver precompiled sudo dnf module install nvidiadriverlatest id locks the driver updates to the specified driver branch precompiled sudo dnf module install nvidiadriverid note replace id with the appropriate driver branch streams, for example , , , or . latestdkms always updates to the highest versioned driver nonprecompiled sudo dnf module install nvidiadriverlatestdkms note this is the default stream. iddkms locks the driver updates to the specified driver branch nonprecompiled sudo dnf module install nvidiadriveriddkms note valid streams include dkms, dkms, dkms, and dkms. . precompiled streams support matrix this table shows the supported precompiled and legacy dkms streams for each driver. nvidia driver precompiled stream legacy dkms stream open dkms stream highest version latest latestdkms opendkms locked at x dkms open locked at x dkms open prior to switching between module streams, first reset sudo dnf module reset nvidiadriver note this is also required for upgrading between branch locked streams. or alternatively sudo dnf module switchto nvidiadriverstream . modularity profiles modularity profiles work with any supported modularity stream and allow for additional use cases. these modularity profiles are available on rhel and fedora. table table . list of nvidiadriver module profiles stream profile use case default default installs all the driver packages in a stream. kickstart ks performs unattended linux os installation using a config file. nvswitch fabric fm installs all the driver packages plus components required for bootstrapping an nvswitch system including the fabric manager and nscq telemetry. source src source headers for compilation precompiled streams only. for example sudo dnf module nvidiadriverstreamdefault sudo dnf module nvidiadriverstreamks sudo dnf module nvidiadriverstreamfm sudo dnf module nvidiadriverstreamsrc you can install multiple modularity profiles using bash curly brace expansion, for example sudo dnf module install nvidiadriverlatestdefault,src see in the developer blog and for more information. . kickstart installation . rhel rocky linux enable the epel repository repo nameepel baseurl enable the cuda repository repo namecudarhel baseurl in the packages section of the ks.cfg file, make sure you are using the ks profile and latestdkms stream nvidiadriverlatestdkmsks perform the postinstallation actions. . rhel rocky linux enable the epel repository repo nameepel baseurl enable the cuda repository repo namecudarhel baseurl in the packages section of the ks.cfg file, make sure you are using the ks profile and latestdkms stream nvidiadriverlatestdkmsks perform the postinstallation actions. . runfile installation basic instructions can be found in the quick start guide. read on for more detailed instructions. this section describes the installation and configuration of cuda when using the standalone installer. the standalone installer is a .run file and is completely selfcontained. . runfile overview the runfile installation installs the nvidia driver and cuda toolkit via an interactive ncursesbased interface. the installation steps are listed below. distributionspecific instructions on disabling the nouveau drivers as well as steps for verifying device node creation are also provided. finally, advanced options for the installer and uninstallation steps are detailed below. the runfile installation does not include support for crossplatform development. for crossplatform development, see the cuda crossplatform environment section. . installation perform the preinstallation actions. disable the nouveau drivers. reboot into text mode runlevel . this can usually be accomplished by adding the number to the end of the systems kernel boot parameters. since the nvidia drivers are not yet installed, the text terminals may not display correctly. temporarily adding nomodeset to the systems kernel boot parameters may fix this issue. consult your systems bootloader documentation for information on how to make the above boot parameter changes. the reboot is required to completely unload the nouveau drivers and prevent the graphical interface from loading. the cuda driver cannot be installed while the nouveau drivers are loaded or while the graphical interface is active. verify that the nouveau drivers are not loaded. if the nouveau drivers are still loaded, consult your distributions documentation to see if further steps are needed to disable nouveau. run the installer and follow the onscreen prompts sudo sh cudaversionlinux.run the installer will prompt for the following eula acceptance cuda driver installation cuda toolkit installation, location, and usrlocalcuda symbolic link the default installation location for the toolkit is usrlocalcuda the usrlocalcuda symbolic link points to the location where the cuda toolkit was installed. this link allows projects to use the latest cuda toolkit without any configuration file update. the installer must be executed with sufficient privileges to perform some actions. when the current privileges are insufficient to perform an action, the installer will ask for the users password to attempt to install with root privileges. actions that cause the installer to attempt to install with root privileges are installing the cuda driver installing the cuda toolkit to a location the user does not have permission to write to creating the usrlocalcuda symbolic link running the installer with sudo, as shown above, will give permission to install to directories that require root permissions. directories and files created while running the installer with sudo will have root ownership. if installing the driver, the installer will also ask if the opengl libraries should be installed. if the gpu used for display is not an nvidia gpu, the nvidia opengl libraries should not be installed. otherwise, the opengl libraries used by the graphics driver of the nonnvidia gpu will be overwritten and the gui will not work. if performing a silent installation, the noopengllibs option should be used to prevent the opengl libraries from being installed. see the advanced options section for more details. if the gpu used for display is an nvidia gpu, the x server configuration file, etcxxorg.conf, may need to be modified. in some cases, nvidiaxconfig can be used to automatically generate an xorg.conf file that works for the system. for nonstandard systems, such as those with more than one gpu, it is recommended to manually edit the xorg.conf file. consult the xorg.conf documentation for more information. note installing mesa may overwrite the usrliblibgl.so that was previously installed by the nvidia driver, so a reinstallation of the nvidia driver might be required after installing these libraries. reboot the system to reload the graphical interface sudo reboot verify the device nodes are created properly. perform the postinstallation actions. . disabling nouveau to install the display driver, the nouveau drivers must first be disabled. each distribution of linux has a different method for disabling nouveau. the nouveau drivers are loaded if the following command prints anything lsmod grep nouveau .. fedora create a file at usrlibmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo dracut force run the following command sudo grubmkconfig o bootgrubgrub.cfg reboot the system. .. rhel rocky and kylinos create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo dracut force .. opensuse create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initrd sudo sbinmkinitrd .. sles no actions to disable nouveau are required as nouveau is not installed on sles. .. wsl no actions to disable nouveau are required as nouveau is not installed on wsl. .. ubuntu create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo updateinitramfs u .. debian create a file at etcmodprobe.dblacklistnouveau.conf with the following contents blacklist nouveau options nouveau modeset regenerate the kernel initramfs sudo updateinitramfs u . device node verification check that the device filesdevnvidia exist and have the correct file permissions. these files are used by the cuda driver to communicate with the kernelmode portion of the nvidia driver. applications that use the nvidia driver, such as a cuda application or the x server if any, will normally automatically create these files if they are missing using the setuidnvidiamodprobe tool that is bundled with the nvidia driver. however, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below binbash sbinmodprobe nvidia if ? eq then count the number of nvidia controllers found. nvdevslspci grep i nvidia ndecho nvdevs grep d controller wc l nvgaecho nvdevs grep vga compatible controller wc l nexpr nd nvga for i in seq n do mknod m devnvidiai c i done mknod m devnvidiactl c else exit fi sbinmodprobe nvidiauvm if ? eq then find out the major device number used by the nvidiauvm driver dgrep nvidiauvm procdevices awk print mknod m devnvidiauvm c d else exit fi . advanced options action options used explanation silent installation silent required for any silent installation. performs an installation with no further userinput and minimal commandline output based on the options provided below. silent installations are useful for scripting the installation of cuda. using this option implies acceptance of the eula. the following flags can be used to customize the actions taken during installation. at least one of driver, uninstall, and toolkit must be passed if running with nonroot permissions. driver install the cuda driver. toolkit install the cuda toolkit. toolkitpathpath install the cuda toolkit to the path directory. if not provided, the default path of usrlocalcuda is used. defaultrootpath install libraries to the path directory. if the path is not provided, then the default path of your distribution is used. this only applies to the libraries installed outside of the cuda toolkit path. extraction extractpath extracts to the path the following the driver runfile, the raw files of the toolkit to path. this is especially useful when one wants to install the driver using one or more of the commandline options provided by the driver installer which are not exposed in this installer. overriding installation checks override ignores compiler, thirdparty library, and toolkit detection checks which would prevent the cuda toolkit from installing. no opengl libraries noopengllibs prevents the driver installation from installing nvidias gl libraries. useful for systems where the display is driven by a nonnvidia gpu. in such systems, nvidias gl libraries could prevent x from loading properly. no man pages nomanpage do not install the man pages under usrshareman. overriding kernel source kernelsourcepathpath tells the driver installation to use path as the kernel source directory when building the nvidia kernel module. required for systems where the kernel source is installed to a nonstandard location. running nvidiaxconfig runnvidiaxconfig tells the driver installation to run nvidiaxconfig to update the system x configuration file so that the nvidia x driver is used. the preexisting x configuration file will be backed up. no nvidiadrm kernel module nodrm do not install the nvidiadrm kernel module. this option should only be used to work around failures to build or install the nvidiadrm kernel module on systems that do not need the provided features. custom temporary directory selection tmpdirpath performs any temporary actions within path instead of tmp. useful in cases where tmp cannot be used doesnt exist, is full, is mounted with noexec, etc.. show installer options help prints the list of commandline options to stdout. . uninstallation to uninstall the cuda toolkit, run the uninstallation script provided in the bin directory of the toolkit. by default, it is located in usrlocalcudabin sudo usrlocalcudabincudauninstaller to uninstall the nvidia driver, run nvidiauninstall sudo usrbinnvidiauninstall to enable the nouveau drivers, remove the blacklist file created in the disabling nouveau section, and regenerate the kernel initramfsinitrd again as described in that section. . conda installation this section describes the installation and configuration of cuda when using the conda installer. the conda packages are available at . conda overview the conda installation installs the cuda toolkit. the installation steps are listed below. . installing cuda using conda to perform a basic install of all cuda toolkit components using conda, run the following command conda install cuda c nvidia . uninstalling cuda using conda to uninstall the cuda toolkit using conda, run the following command conda remove cuda . installing previous cuda releases all conda packages released under a specific cuda version are labeled with that release version. to install a previous version, include that label in the install command such as conda install cuda c nvidialabelcuda. . upgrading from cudatoolkit package if you had previously installed cuda using the cudatoolkit package and want to maintain a similar install footprint, you can limit your installation to the following packages cudalibrariesdev cudanvcc cudanvtx cudacupti note some extra files, such as headers, will be included in this installation which were not included in the cudatoolkit package. if you need to reduce your installation further, replace cudalibrariesdev with the specific libraries you need. . pip wheels nvidia provides python wheels for installing cuda through pip, primarily for using cuda with python. these packages are intended for runtime use and do not currently include developer tools these can be installed separately. please note that with this installation method, cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment. prerequisites to install wheels, you must first install the nvidiapyindex package, which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo. if your pip and setuptools python modules are not uptodate, then use the following command to upgrade these python modules. if these python modules are outofdate then the commands which follow later in this section may fail. python m pip install upgrade setuptools pip wheel you should now be able to install the nvidiapyindex module. python m pip install nvidiapyindex if your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidiapyindex package extraindexurl procedure install the cuda runtime package python m pip install nvidiacudaruntimecu optionally, install additional packages as listed below using the following command python m pip install nvidialibrary metapackages the following metapackages will install the latest version of the named component on linux for the indicated cuda version. cu should be read as cuda. nvidiacudaruntimecu nvidiacudaccclcu nvidiacudacupticu nvidiacudanvcccu nvidiacudaopenclcu nvidiacudanvrtccu nvidiacublascu nvidiacudasanitizerapicu nvidiacufftcu nvidiacurandcu nvidiacusolvercu nvidiacusparsecu nvidianppcu nvidianvfatbincu nvidianvjitlinkcu nvidianvjpegcu nvidianvmldevcu nvidianvtxcu these metapackages install the following packages nvidiacudaruntimecu nvidiacudaccclcu nvidiacudacupticu nvidiacudanvcccu nvidiacudaopenclcu nvidiacublascu nvidiacudasanitizerapicu nvidiacudanvrtccu nvidiacufftcu nvidiacurandcu nvidiacusolvercu nvidiacusparsecu nvidianppcu nvidianvfatbincu nvidianvjitlinkcu nvidianvjpegcu nvidianvmldevcu nvidianvtxcu . tarball and zip archive deliverables in an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community cicd systems, tarball and zip archives are available for each component. these tarball and zip archives, known as binary archives, are provided at these component .tar.xz and .zip binary archives do not replace existing packages such as .deb, .rpm, runfile, conda, etc. and are not meant for general consumption, as they are not installers. however this standardized approach will replace existing .txz archives. for each release, a json manifest is provided such as redistrib..json, which corresponds to the cuda . release label cuda update which includes the release date, the name of each component, license name, relative url for each platform and checksums. package maintainers are advised to check the provided license for each component prior to redistribution. instructions for developers using cmake and bazel build systems are provided in the next sections. . parsing redistrib json the following example of a json manifest contains keys for each component name, license, version, and a platform array which includes relativepath, sha, md, and size bytes for each archive. releasedate , cudacudart name cuda runtime cudart, license cuda toolkit, version ., linuxx relativepath cudacudartlinuxxcudacudartlinuxx.archive.tar.xz, sha dabeaaaeadcbddeadeaeeccadb, md dabacbfed, size , linuxppcle relativepath cudacudartlinuxppclecudacudartlinuxppcle.archive.tar.xz, sha dffeaeebdadcdeeeafdeabbeacec, md caefebbbfcca, size , linuxsbsa relativepath cudacudartlinuxsbsacudacudartlinuxsbsa.archive.tar.xz, sha abbbaebdcfadddfaaefcbcceccefdcc, md aebcfbba, size , windowsx relativepath cudacudartwindowsxcudacudartwindowsx.archive.zip, sha bcdeaacddcddaddedd, md fabaabfc, size a json schema is provided at a sample script that parses these json manifests is available on github downloads each archive validates sha checksums extracts archives flattens into a collapsed directory structure table available tarball and zip archives product example cuda toolkit .parseredist.py product cuda label . cublasmp .parseredist.py product cublasmp label . cudnn .parseredist.py product cudnn label . cudss .parseredist.py product cudss label . cuquantum .parseredist.py product cuquantum label . cusparselt .parseredist.py product cusparselt label . cutensor .parseredist.py product cutensor label . nvidia driver .parseredist.py product nvidiadriver label . nvjpeg .parseredist.py product nvjpeg label . nvpl .parseredist.py product nvpl label nvtiff .parseredist.py product nvtiff label . . importing tarballs into cmake the recommended module for importing these tarballs into the cmake build system is via findcudatoolkit and newer. note the findcuda module is deprecated. the path to the extraction location can be specified with the cudatoolkitroot environmental variable. for example cmakelists.txt and commands, see cmakefindcudatoolkit. for older versions of cmake, the externalprojectadd module is an alternative method. for example cmakelists.txt file and commands, see cmakeexternalproject. . importing tarballs into bazel the recommended method of importing these tarballs into the bazel build system is using httparchive and pkgtar. for an example, see bazelpkgtar. . cuda crossplatform environment cross development for armsbsa is supported on ubuntu , ubuntu , rhel , rhel , and sles . cross development for armjetson is only supported on ubuntu we recommend selecting a host development environment that matches the supported crosstarget environment. this selection helps prevent possible hosttarget incompatibilities, such as gcc or glibc version mismatches. . cuda crossplatform installation some of the following steps may have already been performed as part of the native ubuntu installation. such steps can safely be skipped. these steps should be performed on the x host system, rather than the target system. to install the native cuda toolkit on the target system, refer to the native ubuntu installation section. perform the preinstallation actions. install repository metadata package with sudo dpkg i cudarepocrossidentifierall.deb where identifier indicates the operating system, architecture, andor the version of the package. update the apt repository cache sudo aptget update install the appropriate crossplatform cuda toolkit for aarch sudo aptget install cudacrossaarch for qnx sudo aptget install cudacrossqnx perform the postinstallation actions. . cuda crossplatform samples cuda samples are now located in which includes instructions for obtaining, building, and running the samples. . postinstallation actions the postinstallation actions must be manually performed. these actions are split into mandatory, recommended, and optional sections. . mandatory actions some actions must be taken after the installation before the cuda toolkit and driver can be used. .. environment setup the path variable needs to include export pathusrlocalcudabinpathpath. nsight compute has moved to optnvidiansightcompute only in rpmdeb installation method. when using .run installer it is still located under usrlocalcuda. to add this path to the path variable export pathusrlocalcudabinpathpath in addition, when using the runfile installation method, the ldlibrarypath variable needs to contain usrlocalcudalib on a bit system, or usrlocalcudalib on a bit system to change the environment variables for bit operating systems export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath to change the environment variables for bit operating systems export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath note that the above paths change when using a custom install path with the runfile installation method. . recommended actions other actions are recommended to verify the integrity of the installation. .. install persistence daemon nvidia is providing a userspace daemon on linux to support persistence of driver state across cuda job runs. the daemon approach provides a more elegant and robust solution to this problem than persistence mode. for more details on the nvidia persistence daemon, see the documentation here. the nvidia persistence daemon can be started as the root user by running usrbinnvidiapersistenced verbose this command should be run on boot. consult your linux distributions init documentation for details on how to automate this. .. install writable samples cuda samples are now located in which includes instructions for obtaining, building, and running the samples. .. verify the installation before continuing, it is important to verify that the cuda toolkit can find and communicate correctly with the cudacapable hardware. to do this, you need to compile and run some of the sample programs, located in note ensure the path and, if using the runfile installation method, ldlibrarypath variables are set correctly. .. verify the driver version if you installed the driver, verify that the correct version of it is loaded. if you did not install the driver, or are using an operating system where the driver is not loaded via a kernel module, such as lt, skip this step. when the driver is loaded, the driver version can be found by executing the command cat procdrivernvidiaversion note that this command will not work on an igpudgpu system. .. running the binaries after compilation, find and run devicequeryfrom if the cuda software is installed and configured correctly, the output for devicequery should look similar to that shown in figure . figure figure . valid results from devicequery cuda sample the exact appearance and the output lines might be different on your system. the important outcomes are that a device was found the first highlighted line, that the device matches the one on your system the second highlighted line, and that the test passed the final highlighted line. if a cudacapable device and the cuda driver are installed but devicequery reports that no cudacapable devices are present, this likely means that the devnvidia files are missing or have the wrong permissions. on systems where selinux is enabled, you might need to temporarily disable this security feature to run devicequery. to do this, type setenforce from the command line as the superuser. running the bandwidthtest program ensures that the system and the cudacapable device are able to communicate correctly. its output is shown in figure . figure figure . valid results from bandwidthtest cuda sample note that the measurements for your cudacapable device description will vary from system to system. the important point is that you obtain measurements, and that the secondtolast line in figure confirms that all necessary tests passed. should the tests not pass, make sure you have a cudacapable nvidia gpu on your system and make sure it is properly installed. if you run into difficulties with the link step such as libraries not being found, consult the linux release notes found in .. install nsight eclipse plugins to install nsight eclipse plugins, an installation script is provided usrlocalcudabinnsighteepluginsmanage.sh install eclipsedir refer to nsight eclipse plugins installation guide for more details. .. local repo removal removal of the local repo installer is recommended after installation of cuda sdk. ubuntu and debian sudo aptget remove purge cudarepodistroxylocal fedora sudo dnf remove cudarepodistroxylocal rhel rocky linux and rhel rocky linux sudo dnf remove cudarepodistroxylocal opensuse and sles sudo zypper remove cudarepodistroxylocal removal of the local repo installer is recommended after installation of nvida driver. ubuntu and debian sudo aptget remove purge nvidiadriverlocalrepodistro fedora sudo dnf remove nvidiadriverlocalrepodistro rhel rocky linux and rhel rocky linux sudo dnf remove nvidiadriverlocalrepodistro opensuse and sles sudo zypper remove nvidiadriverlocalrepodistro . optional actions other options are not necessary to use the cuda toolkit, but are available to provide additional features. .. install thirdparty libraries some cuda samples use thirdparty libraries which may not be installed by default on your system. these samples attempt to detect any required libraries when building. if a library is not detected, it waives itself and warns you which library is missing. to build and run these samples, you must install the missing libraries. in cases where these dependencies are not installed, follow the instructions below. rhel rocky linux sudo dnf install freeglutdevel libxdevel libxidevel libxmudevel make mesalibgludevel freeimagedevel libglfwdevel rhel rocky linux sudo dnf install freeglutdevel libxdevel libxidevel libxmudevel make mesalibgludevel freeimagedevel libglfwdevel kylinos sudo dnf install freeglutdevel libxdevel libxidevel libxmudevel make mesalibgludevel freeimagedevel libglfwdevel fedora sudo dnf install freeglutdevel libxdevel libxidevel libxmudevel make mesalibgludevel freeimagedevel libglfwdevel sles sudo zypper install libglut libxdevel libxi libxmu libglu make opensuse sudo zypper install freeglutdevel libxdevel libxidevel libxmudevel make mesalibgldevel freeimagedevel ubuntu sudo aptget install g freeglutdev buildessential libxdev libxmudev libxidev libglumesadev libfreeimagedev libglfwdev debian sudo aptget install g freeglutdev buildessential libxdev libxmudev libxidev libglumesadev libfreeimagedev libglfwdev .. install the source code for cudagdb the cudagdb source must be explicitly selected for installation with the runfile installation method. during the installation, in the component selection page, expand the component cuda tools and select cudagdbsrc for installation. it is unchecked by default. to obtain a copy of the source code for cudagdb using the rpm and debian installation methods, the cudagdbsrc package must be installed. the source code is installed as a tarball in the usrlocalcudaextras directory. .. select the active version of cuda for applications that rely on the symlinks usrlocalcuda and usrlocalcudamajor, you may wish to change to a different installed version of cuda using the provided alternatives. to show the active version of cuda and all available versions updatealternatives display cuda to show the active minor version of a given major cuda release updatealternatives display cuda to update the active version of cuda sudo updatealternatives config cuda . advanced setup below is information on some advanced setup scenarios which are not covered in the basic instructions above. table advanced setup scenarios when installing cuda scenario instructions install cuda using the package manager installation method without installing the nvidia gl libraries. fedora install cuda using the following command sudo dnf install cudatoolkit nvidiadrivercuda akmodnvidia follow the instructions here to ensure that nouveau is disabled. if performing an upgrade over a previous installation, the nvidia kernel module may need to be rebuilt by following the instructions here. opensusesles on some system configurations the nvidia gl libraries may need to be locked before installation using sudo zypper addlock nvidiaglg install cuda using the following command sudo zypper install norecommends cudatoolkit nvidiacomputeg nvidiagfxgkmpdefault follow the instructions here to ensure that nouveau is disabled. ubuntu this functionality isnt supported on ubuntu. instead, the driver packages integrate with the bumblebee framework to provide a solution for users who wish to control what applications the nvidia drivers are used for. see ubuntus bumblebee wiki for more information. upgrade from a rpmdeb driver installation which includes the diagnostic driver packages to a driver installation which does not include the diagnostic driver packages. rhelcentos remove diagnostic packages using the following command sudo yum remove cudadriversdiagnostic xorgxdrvnvidiadiagnostic follow the instructions here to continue installation as normal. fedora remove diagnostic packages using the following command sudo dnf remove cudadriversdiagnostic xorgxdrvnvidiadiagnostic follow the instructions here to continue installation as normal. opensusesles remove diagnostic packages using the following command sudo zypper remove cudadriversdiagnostic nvidiadiagnosticg follow the instructions here to continue installation as normal. ubuntu remove diagnostic packages using the following command sudo aptget purge cudadriversdiagnostic nvidiadiagnostic follow the instructions here to continue installation as normal. use a specific gpu for rendering the display. add or replace a device entry in your xorg.conf file, located at etcxxorg.conf. the device entry should resemble the following section device identifier device driver drivername vendorname vendorname busid busid endsection the details will you will need to add differ on a casebycase basis. for example, if you have two nvidia gpus and you want the first gpu to be used for display, you would replace drivername with nvidia, vendorname with nvidia corporation and busid with the bus id of the gpu. the bus id will resemble pci and can be found by running lspci. install cuda to a specific directory using the package manager installation method. rpm the rpm packages dont support custom install locations through the package managers yum and zypper, but it is possible to install the rpm packages to a custom location using rpms relocate parameter sudo rpm install relocate usrlocalcudanewtoolkit package.rpm you will need to install the packages in the correct dependency order this task is normally taken care of by the package managers. for example, if package foo has a dependency on package bar, you should install package bar first, and package foo second. you can check the dependencies of a rpm package as follows rpm qrp package.rpm note that the driver packages cannot be relocated. deb the deb packages do not support custom install locations. it is however possible to extract the contents of the deb packages and move the files to the desired install location. see the next scenario for more details one xtracting deb packages. extract the contents of the installers. runfile the runfile can be extracted into the standalone toolkit and driver runfiles by using the extract parameter. the toolkit standalone runfiles can be further extracted by running .runfile.run tar mxvf the driver runfile can be extracted by running .runfile.run x rpm the rpm packages can be extracted by running rpmcpio package.rpm cpio idmv deb the deb packages can be extracted by running dpkgdeb x package.deb outputdir modify ubuntus apt package manager to query specific architectures for specific repositories. this is useful when a foreign architecture has been added, causing not found errors to appear when the repository metadata is updated. each repository you wish to restrict to specific architectures must have its sources.list entry modified. this is done by modifying the etcaptsources.list file and any files containing repositories you wish to restrict under the etcaptsources.list.d directory. normally, it is sufficient to modify only the entries in etcaptsources.list an architecturerestricted repository entry looks like deb archarch,arch url for example, if you wanted to restrict a repository to only the amd and i architectures, it would look like deb archamd,i url it is not necessary to restrict the debsrc repositories, as these repositories dont provide architecturespecific packages. for more details, see the sources.list manpage. the nvidia.ko kernel module fails to load, saying some symbols are unknown. for example nvidia unknown symbol drmopen err check to see if there are any optionally installable modules that might provide these symbols which are not currently installed. for the example of the drmopen symbol, check to see if there are any packages which provide drmopen and are not already installed. for instance, on ubuntu , the linuximageextra package provides the drm kernel module which provides drmopen. this package is optional even though the kernel headers reflect the availability of drm regardless of whether this package is installed or not. the runfile installer fails to extract due to limited space in the tmp directory. this can occur on systems with limited storage in the tmp directory usually tmp, or on systems which use a tmpfs in memory to handle temporary storage. in this case, the tmpdir commandline option should be used to instruct the runfile to use a directory with sufficient space to extract into. more information on this option can be found here. reenable wayland after installing the rpm driver on fedora. wayland is disabled during installation of the fedora driver rpm due to compatability issues. to reenable wayland, comment out this line in etcgdmcustom.conf waylandenablefalse in case of the error e failed to fetch filevarcudarepo file not found debian and ubuntu this can occur when installing cuda after uninstalling a different version. use the following command before installation sudo rm v varlibaptlistscuda varlibaptlistsnvidia verbose installation on debian and ubuntu use the verboseversions flag, for example sudo aptget install verboseversions cuda . frequently asked questions . how do i install the toolkit in a different location? the runfile installation asks where you wish to install the toolkit during an interactive install. if installing using a noninteractive install, you can use the toolkitpath parameter to change the install location .runfile.run silent toolkit toolkitpathmynewtoolkit the rpm and deb packages cannot be installed to a custom install location directly using the package managers. see the install cuda to a specific directory using the package manager installation method scenario in the advanced setup section for more information. . why do i see nvcc no such file or directory when i try to build a cuda application? your path environment variable is not set up correctly. ensure that your path includes the bin directory where you installed the toolkit, usually usrlocalcudabin. export pathusrlocalcudabinpathpath . why do i see error while loading shared libraries lib name cannot open shared object file no such file or directory when i try to run a cuda application that uses a cuda library? your ldlibrarypath environment variable is not set up correctly. ensure that your ldlibrarypath includes the lib andor lib directory where you installed the toolkit, usually usrlocalcudalib, export ldlibrarypathusrlocalcudalib ldlibrarypathldlibrarypath . why do i see multiple not found errors when updating my repository metadata on ubuntu? these errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the systems sources.list file. repositories that do not host packages for the newly added architecture will present this error. while noisy, the error itself does no harm. please see the advanced setup section for details on how to modify your sources.list file to prevent these errors. . how can i tell x to ignore a gpu for computeonly use? to make sure x doesnt use a certain gpu for display, you need to specify which other gpu to use for display. for more information, please refer to the use a specific gpu for rendering the display scenario in the advanced setup section. . why doesnt the cudarepo package install the cuda toolkit and drivers? when using rpm or deb, the downloaded package is a repository package. such a package only informs the package manager where to find the actual installation packages, but will not install them. see the package manager installation section for more details. . how do i get cuda to work on a laptop with an igpu and a dgpu running ubuntu.? after installing cuda, set the driver value for the intel device in etcxxorg.conf to modesetting as shown below section device identifier intel driver modesetting ... endsection to prevent ubuntu from reverting the change in xorg.conf, edit etcdefaultgrub to add nogpumanager to grubcmdlinelinuxdefault. run the following command to update grub before rebooting sudo updategrub . what do i do if the display does not load, or cuda does not work, after performing a system update? system updates may include an updated linux kernel. in many cases, a new linux kernel will be installed without properly updating the required linux kernel headers and development packages. to ensure the cuda driver continues to work when performing a system update, rerun the commands in the kernel headers and development packages section. additionally, on fedora, the akmods framework will sometimes fail to correctly rebuild the nvidia kernel module packages when a new linux kernel is installed. when this happens, it is usually sufficient to invoke akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting sudo akmods force sudo depmod you can reach a virtual console by hitting ctrlaltf at the same time. . how do i install a cuda driver with a version less than using a network repo? to install a cuda driver at a version earlier than using a network repo, the required packages will need to be explicitly installed at the desired version. for example, to install , instead of installing the cudadrivers metapackage at version , you will need to install all required packages of cudadrivers at version . . how do i install an older cuda version using a network repo? depending on your system configuration, you may not be able to install old versions of cuda using the cuda metapackage. in order to install a specific version of cuda, you may need to specify all of the packages that would normally be installed by the cuda metapackage at the version you want to install. if you are using yum to install certain packages at an older version, the dependencies may not resolve as expected. in this case you may need to pass setoptobsoletes to yum to allow an install of packages which are obsoleted at a later version than you are trying to install. . why does the installation on suse install the mesadrinouveau dependency? this dependency comes from the suse repositories and shouldnt affect the use of the nvidia driver or the cuda toolkit. to disable this dependency, you can lock that package with the following command sudo zypper al mesadrinouveau . how do i handle errors were encountered while processing glxdiversions? this sometimes occurs when trying to uninstall cuda after a clean .deb installation. run the following commands sudo aptget install glxdiversions reinstall sudo aptget remove nvidiaalternative then rerun the commands from removing cuda toolkit and driver. . additional considerations now that you have cudacapable hardware and the nvidia cuda toolkit installed, you can examine and enjoy the numerous included programs. to begin using cuda to accelerate the performance of your own applications, consult the cuda c programming guide, located in usrlocalcudadoc. a number of helpful development tools are included in the cuda toolkit to assist you as you develop your cuda programs, such as nvidia nsight eclipse edition, nvidia visual profiler, cudagdb, and cudamemcheck. for technical support on programming questions, consult and participate in the developer forums at . switching between driver module flavors use the following steps to switch between the nvidia driver legacy and open module flavors on your system. note if switching to open module, experimental support for geforce and quadro skus can be enabled with echo options nvidia nvregopenrmenableunsupportedgpus sudo tee etcmodprobe.dnvidiagsp.conf note replace xxx with the nvidia driver branch number such as . fedora, rhel rocky linux , rhel rocky linux to switch between legacy and open uninstall, then reinstall. kylin os to switch between legacy and open uninstall, then reinstall. ubuntu to switch from legacy to open sudo aptget purge remove nvidiakernelsourcexxx sudo aptget install verboseversions nvidiakernelopenxxx sudo aptget install verboseversions cudadriversxxx to switch from open to legacy sudo aptget remove purge nvidiakernelopenxxx sudo aptget install verboseversions cudadriversxxx debian to switch from legacy to open sudo aptget purge remove nvidiakerneldkms sudo aptget install verboseversions nvidiakernelopendkms sudo aptget install verboseversions cudadriversxxx to switch from open to legacy sudo aptget remove purge nvidiakernelopendkms sudo aptget install verboseversions cudadriversxxx opensuse to switch from legacy to open sudo zypper remove nvidiadrivergkmpdefault sudo zypper install details nvidiaopendrivergkmpdefault sudo zypper install details cudadriversxxx to switch from open to legacy sudo zypper remove nvidiaopendrivergkmpdefault sudo zypper install details cudadriversxxx sles to switch from legacy to open sudo zypper remove nvidiadrivergkmpdefault nvidiadrivergkmpazure sudo zypper install details nvidiaopendrivergkmpdefault nvidiaopendrivergkmpazure sudo zypper install details cudadriversxxx to switch from open to legacy sudo zypper remove nvidiaopendrivergkmpdefault nvidiadrivergopenkmpazure sudo zypper install details cudadriversxxx note the azure package is only available for sles x. . removing cuda toolkit and driver follow the below steps to properly uninstall the cuda toolkit and nvidia drivers from your system. these steps will ensure that the uninstallation will be clean. kylinos to remove cuda toolkit sudo dnf remove cuda cublas cufft cufile curand cusolver cusparse gdstools npp nvjpeg nsight nvvm to remove nvidia drivers sudo dnf module remove all nvidiadriver to reset the module stream sudo dnf module reset nvidiadriver rhel rocky linux to remove cuda toolkit sudo dnf remove cuda cublas cufft cufile curand cusolver cusparse gdstools npp nvjpeg nsight nvvm to remove nvidia drivers sudo dnf module remove all nvidiadriver to reset the module stream sudo dnf module reset nvidiadriver rhel rocky linux to remove cuda toolkit sudo dnf remove cuda cublas cufft cufile curand cusolver cusparse gdstools npp nvjpeg nsight nvvm to remove nvidia drivers sudo dnf module remove all nvidiadriver to reset the module stream sudo dnf module reset nvidiadriver fedora to remove cuda toolkit sudo dnf remove cuda cublas cufft cufile curand cusolver cusparse gdstools npp nvjpeg nsight nvvm to remove nvidia drivers sudo dnf module remove all nvidiadriver to reset the module stream sudo dnf module reset nvidiadriver to remove rd party nvidia drivers sudo dnf remove nvidia opensuse sles to remove cuda toolkit sudo zypper remove cuda cublas cufft cufile curand cusolver cusparse gdstools npp nvjpeg nsight nvvm to remove nvidia drivers sudo zypper remove nvidia ubuntu and debian to remove cuda toolkit sudo aptget purge remove cuda cublas cufft cufile curand cusolver cusparse gdstools npp nvjpeg nsight nvvm to remove nvidia drivers sudo aptget purge remove nvidia libxnvctrl to clean up the uninstall sudo aptget autoremove . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated. . copyright nvidia corporation affiliates. all rights reserved. this product includes software developed by the syncro soft srl",
    "depth": 4
  },
  {
    "url": "https://docs.nvidia.com/cuda/pascal-compatibility-guide/index.html",
    "content": "pascal compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia pascal architecture. . pascal compatibility . about this document this application note, pascal compatibility guide for cuda applications, is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia pascal architecture. this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with pascal. . application compatibility on pascal the nvidia cuda c compiler, nvcc, can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel. each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number. for example, cubin files that target compute capability are supported on all computecapability x kepler devices but are not supported on computecapability x maxwell or x pascal devices. for this reason, to ensure forward compatibility with gpu architectures introduced after the application has been released, it is recommended that all applications include ptx versions of their kernels. note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default, keeping the ptx path strictly for forwardcompatibility purposes. applications that already include ptx versions of their kernels should work asis on pascalbased gpus. applications that only support specific gpu architectures via cubin files, however, will need to be updated to provide pascalcompatible ptx or cubins. . verifying pascal compatibility for existing applications the first step is to check that pascalcompatible device code at least ptx is compiled in to the application. the following sections show how to accomplish this for applications built with different cuda toolkit versions. .. applications using cuda toolkit or earlier cuda applications built using cuda toolkit versions through are compatible with pascal as long as they are built to include ptx versions of their kernels. to test that ptx jit is working for your application, you can do the following download and install the latest driver from set the environment variable cudaforceptxjit. launch your application. when starting a cuda application for the first time with the above environment flag, the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code. if you set the environment variable above and then launch your program and it works properly, then you have successfully verified pascal compatibility. note be sure to unset the cudaforceptxjit environment variable when you are done testing. .. applications using cuda toolkit cuda applications built using cuda toolkit are compatible with pascal as long as they are built to include kernels in either pascalnative cubin format see building applications with pascal support or ptx format see applications using cuda toolkit or earlier or both. . building applications with pascal support when a cuda application launches a kernel, the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available. if a cubin file supporting the architecture of the target gpu is available, it is used otherwise, the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it. if neither is available, then the kernel launch will fail. the method used to build your application with either native cubin or at least ptx support for pascal depend on the version of the cuda toolkit used. the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx. all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. especially when using large libraries, this jit compilation can take a significant amount of time. the cuda driver will cache the cubins generated as a result of the ptx jit, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus, meaning that nativecompiled code may be faster or of greater accuracy. .. applications using cuda toolkit or earlier the compilers included in cuda toolkit or earlier generate cubin files native to earlier nvidia architectures such as kepler and maxwell, but they cannot generate cubin files native to the pascal architecture. to allow support for pascal and future architectures when using version or earlier of the cuda toolkit, the compiler must generate a ptx version of each kernel. below are compiler settings that could be used to build mykernel.cu to run on kepler or maxwell devices natively and on pascal devices via ptx jit. note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide pascal compatibility. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, you may be familiar with the simplified nvcc commandline option archsmxx, which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmxx expands to the following gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. .. applications using cuda toolkit with version of the cuda toolkit, nvcc can generate cubin files native to the pascal architectures compute capability and . when using cuda toolkit , to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. . revision history version initial public release. version use cuda c instead of cuda cc . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.",
    "depth": 5
  },
  {
    "url": "https://docs.nvidia.com/cuda/volta-compatibility-guide/index.html",
    "content": "volta compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia volta architecture. . volta compatibility . about this document this application note, volta compatibility guide for cuda applications, is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia volta architecture. this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with volta. . application compatibility on volta the nvidia cuda c compiler, nvcc, can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel. each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number. for example, cubin files that target compute capability are supported on all computecapability x kepler devices but are not supported on computecapability x maxwell or x pascal devices. for this reason, to ensure forward compatibility with gpu architectures introduced after the application has been released, it is recommended that all applications include ptx versions of their kernels. note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default, keeping the ptx path strictly for forwardcompatibility purposes. applications that already include ptx versions of their kernels should work asis on voltabased gpus. applications that only support specific gpu architectures via cubin files, however, will need to be updated to provide voltacompatible ptx or cubins. . verifying volta compatibility for existing applications the first step is to check that voltacompatible device code at least ptx is compiled into the application. the following sections show how to accomplish this for applications built with different cuda toolkit versions. .. applications using cuda toolkit or earlier cuda applications built using cuda toolkit versions through are compatible with volta as long as they are built to include ptx versions of their kernels. to test that ptx jit is working for your application, you can do the following download and install the latest driver from set the environment variable cudaforceptxjit. launch your application. when starting a cuda application for the first time with the above environment flag, the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code. if you set the environment variable above and then launch your program and it works properly, then you have successfully verified volta compatibility. note be sure to unset the cudaforceptxjit environment variable when you are done testing. .. applications using cuda toolkit cuda applications built using cuda toolkit are compatible with volta as long as they are built to include kernels in either voltanative cubin format see building applications with volta support or ptx format see applications using cuda toolkit or earlier or both. . building applications with volta support when a cuda application launches a kernel, the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available. if a cubin file supporting the architecture of the target gpu is available, it is used otherwise, the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it. if neither is available, then the kernel launch will fail. the method used to build your application with either native cubin or at least ptx support for volta depend on the version of the cuda toolkit used. the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx. all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. especially when using large libraries, this jit compilation can take a significant amount of time. the cuda driver will cache the cubins generated as a result of the ptx jit, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus, meaning that nativecompiled code may be faster or of greater accuracy. .. applications using cuda toolkit or earlier the compilers included in cuda toolkit or earlier generate cubin files native to earlier nvidia architectures such as maxwell and pascal, but they cannot generate cubin files native to the volta architecture. to allow support for volta and future architectures when using version or earlier of the cuda toolkit, the compiler must generate a ptx version of each kernel. below are compiler settings that could be used to build mykernel.cu to run on maxwell or pascal devices natively and on volta devices via ptx jit. note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide volta compatibility. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, you may be familiar with the simplified nvcc commandline option archsmxx, which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmxx expands to the following gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. .. applications using cuda toolkit with version of the cuda toolkit, nvcc can generate cubin files native to the volta architecture compute capability . when using cuda toolkit , to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. also, note that cuda removes support for compute capability x fermi devices. any computex and smx flags need to be removed from your compiler commands. .. independent thread scheduling compatibility the volta architecture introduces independent thread scheduling among threads in a warp. if the developer made assumptions about warpsynchronicity, this feature can alter the set of threads participating in the executed code compared to previous architectures. please see compute capability in the cuda c programming guide for details and corrective actions. to aid migration volta developers can optin to the pascal scheduling model with the following combination of compiler options. nvcc archcompute codesm ... . revision history version initial public release. version use cuda c instead of cuda cc updated references to the cuda c programming guide and cuda c best practices guide. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated. warpsynchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.",
    "depth": 5
  },
  {
    "url": "https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html",
    "content": "turing compatibility guide for cuda applications the guide to building cuda applications for nvidia turing gpus. . turing compatibility . about this document this application note, turing compatibility guide for cuda applications, is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia turing architecture. this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with turing. . application compatibility on turing the nvidia cuda c compiler, nvcc, can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel. each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number. for example, cubin files that target compute capability are supported on all computecapability x kepler devices but are not supported on computecapability x maxwell or x pascal devices. for this reason, to ensure forward compatibility with gpu architectures introduced after the application has been released, it is recommended that all applications include ptx versions of their kernels. note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default, keeping the ptx path strictly for forwardcompatibility purposes. applications that already include ptx versions of their kernels should work asis on turingbased gpus. applications that only support specific gpu architectures via cubin files, however, will need to be updated to provide turingcompatible ptx or cubins. . compatibility between volta and turing the turing architecture is based on voltas instruction set architecture isa , extending it with new instructions. as a consequence, any binary that runs on volta will be able to run on turing forward compatibility, but a turing binary will not be able to run on volta. please note that volta kernels using more than kb of shared memory via the explicit optin, see cuda c programming guide will not be able to launch on turing, as they would exceed turings shared memory capacity. most applications compiled for volta should run efficiently on turing, except if the application uses heavily the tensor cores, or if recompiling would allow use of new turingspecific instructions. voltas tensor core instructions can only reach half of the peak performance on turing. recompiling explicitly for turing is thus recommended. . verifying turing compatibility for existing applications the first step is to check that turingcompatible device code at least ptx is compiled into the application. the following sections show how to accomplish this for applications built with different cuda toolkit versions. .. applications using cuda toolkit or earlier cuda applications built using cuda toolkit versions through are compatible with turing as long as they are built to include ptx versions of their kernels. to test that ptx jit is working for your application, you can do the following download and install the latest driver from set the environment variable cudaforceptxjit. launch your application. when starting a cuda application for the first time with the above environment flag, the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code. if you set the environment variable above and then launch your program and it works properly, then you have successfully verified turing compatibility. note be sure to unset the cudaforceptxjit environment variable when you are done testing. .. applications using cuda toolkit x cuda applications built using cuda toolkit x are compatible with turing as long as they are built to include kernels in either voltanative cubin format see compatibility between volta and turing or ptx format see applications using cuda toolkit or earlier or both. .. applications using cuda toolkit cuda applications built using cuda toolkit are compatible with turing as long as they are built to include kernels in voltanative or turingnative cubin format see compatibility between volta and turing, or ptx format see applications using cuda toolkit or earlier, or both. . building applications with turing support when a cuda application launches a kernel, the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available. if a cubin file supporting the architecture of the target gpu is available, it is used otherwise, the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it. if neither is available, then the kernel launch will fail. the method used to build your application with either native cubin or at least ptx support for turing depend on the version of the cuda toolkit used. the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx. all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. especially when using large libraries, this jit compilation can take a significant amount of time. the cuda driver will cache the cubins generated as a result of the ptx jit, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus, meaning that nativecompiled code may be faster or of greater accuracy. .. applications using cuda toolkit or earlier the compilers included in cuda toolkit or earlier generate cubin files native to earlier nvidia architectures such as maxwell and pascal, but they cannot generate cubin files native to volta or turing architecture. to allow support for volta, turing and future architectures when using version or earlier of the cuda toolkit, the compiler must generate a ptx version of each kernel. below are compiler settings that could be used to build mykernel.cu to run on maxwell or pascal devices natively and on turing devices via ptx jit. note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide turing compatibility. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, you may be familiar with the simplified nvcc commandline option archsmxx, which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmxx expands to the following gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. .. applications using cuda toolkit x with versions x of the cuda toolkit, nvcc can generate cubin files native to the volta architecture compute capability . when using cuda toolkit x, to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. also, note that cuda removes support for compute capability x fermi devices. any computex and smx flags need to be removed from your compiler commands. .. applications using cuda toolkit with version of the cuda toolkit, nvcc can generate cubin files native to the turing architecture compute capability . when using cuda toolkit , to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. .. independent thread scheduling compatibility the volta and turing architectures feature independent thread scheduling among threads in a warp. if the developer made assumptions about warpsynchronicity, this feature can alter the set of threads participating in the executed code compared to previous architectures. please see compute capability in the cuda c programming guide for details and corrective actions. to aid migration volta and turing developers can optin to the pascal scheduling model with the following combination of compiler options. nvcc archcompute codesm ... . revision history version initial public release. version use cuda c instead of cuda cc updated references to the cuda c programming guide and cuda c best practices guide. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated. warpsynchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.",
    "depth": 5
  },
  {
    "url": "https://docs.nvidia.com/nsight-compute/index.html",
    "content": "nsight compute documentation nsight compute release notesrelease notes, including new features and important bug fixes. supported platforms and gpus. list of known issues for the current release. kernel profiling guidekernel profiling guide with metric types and meaning, data collection modes and faq for common problems. nsight computenvidia nsight compute user interface ui manual. information on all views, controls and workflows within the tool ui. transitions guide for visual profiler. nsight compute clinvidia nsight compute command line interface cli manual. information on workflows and options for the command line, including multiprocess profiling and nvtx filtering. transitions guide for nvprof. developer interfaces customization guideuser manual on customizing nvidia nsight compute tools or integrating them with custom workflows. information on writing section files, rules for automatic result analysis and scripting access to report files. training trainingnvidia nsight compute training resources. release information archivesfind documentation for previous versions of nvidia nsight compute. copyright and licenses copyright and licensesinformation on the nvidia software license agreement as well as third party software and tools used by nsight compute.",
    "depth": 4
  },
  {
    "url": "https://docs.nvidia.com/nsight-visual-studio-edition/index.html",
    "content": "nvidia nsight visual studio edition introductionnvidia nsight visual studio edition is an application development environment which brings gpu computing into microsoft visual studio. allows you to build and debug integrated gpu kernels and native cpu code as well as inspect the state of the cpu, gpu, and memory. release notessee the latest features and updates for this version of nvidia nsight visual studio edition. installation and setupthis chapter walks you through the system requirements for nvidia nsight visual studio edition, and the steps youll need to install and get started using the software. cuda debugger getting started with the cuda debuggerthis section provides a walkthrough and tutorial for using the cuda debugger with nvidia nsight visual studio edition. build and runthis section details how to configure the properties of a cuda project, launching the cuda debugger, and how to attach debugging to a running cuda process. control gpu executionin this section, learn more about how to control gpu execution, set gpu breakpoints, and use global freeze. inspect statein this section, learn more about how to use various state inspection features of the cuda debugger, such as specifying the debugger context, viewing memory and variables, using the cuda info view, and using the cuda warp watch. advanced topicsin this section, learn more about advanced cuda topics, such as ptx and sass assembly debugging, as well as how to use the cuda memory checker. reference referenceadditional resources for learning more about working with nvidia nsight visual studio edition. release information archivesfind documentation for previous versions of nvidia nsight visual studio edition. copyright and license notices eulathis document is the end user license agreement eula for nvidia nsight visual studio edition. this document contains specific license terms and conditions for nvidia nsight visual studio edition. by accepting this agreement, you agree to comply with all the terms and conditions applicable to the specific products included herein.",
    "depth": 4
  },
  {
    "url": "https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html",
    "content": "nvidia cuda toolkit release notes the release notes for the cuda toolkit. . cuda update release notes the release notes for the nvidia cuda toolkit can be found online at note the release notes have been reorganized into two major sections the general cuda release notes, and the cuda libraries release notes including historical information for x releases. . cuda toolkit major component versions cuda componentsstarting with cuda , the various components in the toolkit are versioned independently. for cuda update , the table below indicates the versions table cuda update component versions component name version information supported architectures supported platforms cuda c core compute libraries thrust . x, armsbsa, aarchjetson linux, windows cub . libcu . cooperative groups . cuda compatibility . aarchjetson linux cuda runtime cudart . x, armsbsa, aarchjetson linux, windows, wsl cuobjdump . x, armsbsa, aarchjetson linux, windows cupti . x, armsbsa, aarchjetson linux, windows, wsl cuda cuxxfilt demangler . x, armsbsa, aarchjetson linux, windows cuda demo suite . x linux, windows cuda gdb . x, armsbsa, aarchjetson linux, wsl cuda nsight eclipse plugin . x linux cuda nvcc . x, armsbsa, aarchjetson linux, windows, wsl cuda nvdisasm . x, armsbsa, aarchjetson linux, windows cuda nvml headers . x, armsbsa, aarchjetson linux, windows, wsl cuda nvprof . x linux, windows cuda nvprune . x, armsbsa, aarchjetson linux, windows, wsl cuda nvrtc . x, armsbsa, aarchjetson linux, windows, wsl nvtx . x, armsbsa, aarchjetson linux, windows, wsl cuda nvvp . x, linux, windows cuda opencl . x linux, windows cuda profiler api . x, armsbsa, aarchjetson linux, windows, wsl cuda compute sanitizer api . x, armsbsa, aarchjetson linux, windows, wsl cuda cublas . x, armsbsa, aarchjetson linux, windows, wsl cudla . aarchjetson linux cuda cufft . x, armsbsa, aarchjetson linux, windows, wsl cuda cufile . x, armsbsa, aarchjetson linux cuda curand . x, armsbsa, aarchjetson linux, windows, wsl cuda cusolver . x, armsbsa, aarchjetson linux, windows, wsl cuda cusparse . x, armsbsa, aarchjetson linux, windows, wsl cuda npp . x, armsbsa, aarchjetson linux, windows, wsl cuda nvfatbin . x, armsbsa, aarchjetson linux, windows, wsl cuda nvjitlink . x, armsbsa, aarchjetson linux, windows, wsl cuda nvjpeg . x, armsbsa, aarchjetson linux, windows, wsl nsight compute . x, armsbsa, aarchjetson linux, windows, wsl windows nsight systems . x, armsbsa, linux, windows, wsl nsight visual studio edition vse . x windows windows nvidiafs . x, armsbsa, aarchjetson linux visual studio integration . x windows windows nvidia linux driver . x, armsbsa linux nvidia windows driver x windows windows, wsl cuda driverrunning a cuda application requires the system with at least one cuda capable gpu and a driver that is compatible with the cuda toolkit. see table . for more information various gpu products that are cuda capable, visit each release of the cuda toolkit requires a minimum version of the cuda driver. the cuda driver is backward compatible, meaning that applications compiled against a particular version of the cuda will continue to work on subsequent later driver releases. more information on compatibility can be found at cudacompatibilityandupgrades. note starting with cuda , the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below. the minimum required driver version for cuda minor version compatibility is shown below. cuda minor version compatibility is described in detail in table cuda toolkit and minimum required driver version for cuda minor version compatibility cuda toolkit minimum required driver version for cuda minor version compatibility linux x driver version windows x driver version cuda x . cuda .x cuda .x cuda .x cuda .x cuda .x cuda .x cuda .x cuda .x . cuda . . using a minimum required version that is different from toolkit driver version could be allowed in compatibility mode please read the cuda compatibility guide for details. cuda was released with an earlier driver version, but by upgrading to tesla recommended drivers . linux windows, minor version compatibility is possible across the cuda x family of toolkits. the version of the development nvidia gpu driver packaged in each cuda toolkit release is shown below. table cuda toolkit and corresponding driver versions cuda toolkit toolkit driver version linux x driver version windows x driver version cuda update . cuda ga . cuda update . cuda ga . cuda update . cuda ga . cuda update . cuda update . cuda ga . cuda update . cuda ga . cuda update . cuda ga . cuda ga . cuda update . cuda ga . cuda update . cuda update . cuda ga . cuda update . cuda update . cuda ga . cuda update . cuda update . cuda update . cuda update . cuda . ga . cuda . update . cuda . ga . cuda . update . cuda . update . cuda . ga . cuda . update cuda ga cuda . update . cuda . ga . cuda . rc . cuda . cuda . general release, and updates cuda . cuda . update cuda . cuda . cuda . cuda . ga cuda . cuda . cuda . for convenience, the nvidia driver is installed as part of the cuda toolkit installation. note that this driver is for development purposes and is not recommended for use in production with tesla gpus. for running cuda applications in production with tesla gpus, it is recommended to download the latest driver for tesla gpus from the nvidia driver downloads site at during the installation of the cuda toolkit, the installation of the nvidia driver may be skipped on windows when using the interactive or silent installation or on linux by using meta packages. for more information on customizing the install process on windows, see installcudasoftware. for meta packages on linux, see packagemanagermetas. . new features this section lists new general cuda and cuda compilers features. .. general cuda in an upcoming cuda release the nvidia open gpu kernel module flavor will be the default and recommended installation option. endusers with maxwell, pascal, or volta gpus may need to take action to install the nvidia proprietary kernel modules. mps multiprocess service is now supported on lt and embeddedlinux tegra platforms. more details can be found here. .. cuda compiler for changes to ptx, refer to ptxisaversion. .. cuda developer tools for changes to nvprof and visual profiler, see the changelog. for new features, improvements, and bug fixes in nsight systems, see the changelog. for new features, improvements, and bug fixes in nsight visual studio edition, see the changelog. for new features, improvements, and bug fixes in cupti, see the changelog. for new features, improvements, and bug fixes in nsight compute, see the changelog. for new features, improvements, and bug fixes in compute sanitizer, see the changelog. for new features, improvements, and bug fixes in cudagdb, see the changelog. . resolved issues .. cuda compiler resolved an issue found when trying sm ptx of fp gemm kernel compiled by when run on an sm device. resolved an issue in which nvcc failed to compile any cuda code when specifying c with cuda and visual studio .. also added a new environment variable nvccreportallerror to emit error messages if the error is coming from a system header, instead of aborting the compiler. resolved a compiler issue that caused different results when compiling with the g flag than without the flag. fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multiblock loops. resolved issues seen when compiling cublasdx device functions, in some conditions leading to misaligned shared or local address. fix to correct the calculation of writeafterread hazard latency. . known issues and limitations runfile will not be supported for amazon linux . confidential computing is not supported on cuda . please continue to use cuda and drivers r.xx to use these features. launching cooperative group kernels with mps is not supported on tegra platforms. . deprecated or dropped features features deprecated in the current release of the cuda software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. we recommend that developers employ alternative solutions to these features in their software. .. deprecated or dropped architectures nvidia cuda support for the powerpc architecture is removed in cuda . .. deprecated operating systems nvidia cuda support for red hat enterprise linux and centos is removed in cuda . cuda is the last release to support debian . support for microsoft windows h and microsoft windows h sv is deprecated. .. deprecated toolchains cuda toolkit deprecated support for the following host compilers microsoft visual cc msvc all gcc versions prior to gcc .. cuda tools support for the macos host client of cudagdb is deprecated. it will be dropped in an upcoming release. . cuda libraries this section covers cuda libraries release notes for x releases. cuda math libraries toolchain uses c features, and a ccompatible standard library libstdc is required on the host. . cublas library .. cublas release update new features performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on hopper gpus. known issues the bias epilogue without relu or gelu may be not supported on hopper gpus for strided batch cases. a workaround is to implement batching manually. this will be fixed in a future release. cublasgemmgroupedbatchedex and cublastgemmgroupedbatched have large cpu overheads. this will be addressed in an upcoming release. resolved issues under rare circumstances, executing symmhemm concurrently with gemm on hopper gpus might have caused race conditions in the host code, which could lead to an illegal memory access cuda error. cublasltmatmul could produce an illegal instruction cuda error on pascal gpus under the following conditions batch is greater than , and beta is not equal to , and the computations are outofplace c d. .. cublas release new features cublas adds an experimental api to support mixed precision grouped batched gemms. this enables grouped batched gemms with fp or bf inputsoutputs with the fp compute type. refer to cublasgemmgroupedbatchedex for more details. known issues cublasltmatmul ignores inputs to cublasltmatmuldescdscalepointer and cublasltmatmuldescepilogueauxscalepointer if the elements of the respective matrix are not of fp types. resolved issues cublasltmatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. for instance, an unsupported configuration of cublasltmatmul with the scale type being fp and all other types being fp would run with the implicit assumption that the scale type is fp and produce incorrect results. cublas symv failed for large n dimension and above for ssymv, and above for csymv and dsymv, and and above for zsymv. .. cublas release update known issues setting a cublas handle stream to cudastreamperthread and setting the workspace via cublassetworkspace will cause any subsequent cublassetworkspace calls to fail. this will be fixed in an upcoming release. cublasltmatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. for example, an unsupported configuration of cublasltmatmul with the scale type being fp and all other types being fp would run with the implicit assumption that the scale type is fp which can produce incorrect results. this will be fixed in an upcoming release. resolved issues cublasltmatmul ignored the cublasltmatmuldescamaxdpointer for unsupported configurations instead of returning an error. in particular, computing absolute maximum of d is currently supported only for fp matmul when the output data type is also fp cudarfem or cudarfem. reduced hostside overheads for some of the cublaslt apis cublasltmatmul, cublasltmatmulalgocheck, and cublasltmatmulalgogetheuristic. the issue was introduced in cuda toolkit . cublasltmatmul and cublasltmatmulalgogetheuristic could have resulted in floating point exceptions fpe on some hopperbased gpus, including multiinstance gpu mig. the issue was introduced in cublas . .. cublas release new features cublas adds experimental apis to support grouped batched gemm for single precision and double precision. single precision also supports the math mode, cublastftensoropmath. grouped batch mode allows you to concurrently solve gemms of different dimensions m, n, k, leading dimensions lda, ldb, ldc, transpositions transa, transb, and scaling factors alpha, beta. please see gemmgroupedbatched for more details. known issues when the current context has been created using cugreenctxcreate, cublas does not properly detect the number of sms available. the user may provide the corrected sm count to cublas using an api such as cublassetsmcounttarget. blas level and functions might not treat alpha in a blas compliant manner when alpha is zero and the pointer mode is set to cublaspointermodedevice. this is the same known issue documented in cublas update . cublasltmatmul with k equals and epilogue cublasltepiloguedrelu,gelubgrad could outofbound access the workspace. the issue exists since cublas update . cublasltmatmul with k equals and epilogue cublasltepiloguedrelu,gelu could produce illegal memory access if no workspace is provided. the issue exists since cublas . when captured in cuda graph stream capture, cublas routines can create memory nodes through the use of streamordered allocation apis, cudamallocasync and cudafreeasync. however, as there is currently no support for memory nodes in child graphs or graphs launched from the device, attempts to capture cublas routines in such scenarios may fail. to avoid this issue, use the cublassetworkspace function to provide userowned workspace memory. .. cublas release update new features improved performance of heuristics cache for workloads that have a high eviction rate. known issues blas level and functions might not treat alpha in a blas compliant manner when alpha is zero and the pointer mode is set to cublaspointermodedevice. the expected behavior is that the corresponding computations would be skipped. you may encounter the following issues her,,x,k,k may zero the imaginary part on the diagonal elements of the output matrix and her,,x,k,k, syr,,x,k,k and others may produce nan resulting from performing computation on matrices a and b which would otherwise be skipped. if strict compliance with blas is required, the user may manually check for alpha value before invoking the functions or switch to cublaspointermodehost. resolved issues cublaslt matmul operations might have computed the output incorrectly under the following conditions the data type of matrices a and b is fp, the data type of matrices c and d is fp, fp, or bf, the beta value is , the c and d matrices are the same, the epilogue contains gelu activation function. when an application compiled with cublaslt from cuda toolkit update or earlier runs with cublaslt from cuda toolkit update or cuda toolkit , matrix multiply descriptors initialized using cublasltmatmuldescinit sometimes did not respect attribute changes using cublasltmatmuldescsetattribute. fixed creation of cublas or cublaslt handles on hopper gpus under the multiprocess service mps. cublasltmatmul with k equals and epilogue cublasltepiloguebgrada,b might have returned incorrect results for the bias gradient. .. cublas release new features improved performance on nvidia ls ada gpus. known issues cublaslt matmul operations may compute the output incorrectly under the following conditions the data type of matrices a and b is fp, the data type of matrices c and d is fp, fp, or bf, the beta value is , the c and d matrices are the same, the epilogue contains gelu activation function. when an application compiled with cublaslt from cuda toolkit update or earlier runs with cublaslt from cuda toolkit update or later, matrix multiply descriptors initialized using cublasltmatmuldescinit may not respect attribute changes using cublasltmatmuldescsetattribute. to workaround this issue, create the matrix multiply descriptor using cublasltmatmuldesccreate instead of cublasltmatmuldescinit. this will be fixed in an upcoming release. .. cublas release update new features cublaslt will now attempt to decompose problems that cannot be run by a single gemm kernel. it does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times. this improves functional coverage for very large m, n, or batch size cases and makes the transition from the cublas api to the cublaslt api more reliable. known issues cublaslt matmul operations may compute the output incorrectly under the following conditions the data type of matrices a and b is fp, the data type of matrices c and d is fp, fp, or bf, the beta value is , the c and d matrices are the same, the epilogue contains gelu activation function. .. cublas release known issues cublas initialization fails on hopper architecture gpus when mps is in use with cudampsactivethreadpercentage set to a value less than . there is currently no workaround for this issue. some hopper kernels produce incorrect results for batched matmuls with cublasltepiloguerelubias or cublasltepiloguegelubias and a nonzero cublasltmatmuldescbiasbatchstride. the kernels apply the first batchs bias vector to all batches. this will be fixed in a future release. .. cublas release update new features support for fp on nvidia ada gpus. improved performance on nvidia l ada gpus. introduced an api that instructs the cublaslt library to not use some cpu instructions. this is useful in some rare cases where certain cpu instructions used by cublaslt heuristics negatively impact cpu performance. refer to disablingcpuinstructions. known issues when creating a matrix layout using the cublasltmatrixlayoutcreate function, the object pointed at by cublasltmatrixlayoutt is smaller than cublasltmatrixlayoutopaquet but enough to hold the internal structure. as a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. if one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeofcublasltmatrixlayoutopaquet bytes, and initialize it using cublasltmatrixlayoutinit function. the same applies to cublasltmatmuldesct and cublasltmatrixtransformdesct. the issue will be fixed in future releases by ensuring that cublasltmatrixlayoutcreate allocates at least sizeofcublasltmatrixlayoutopaquet bytes. .. cublas release update new features improved performance on nvidia h sxm and nvidia h pcie gpus. known issues for optimal performance on nvidia hopper architecture, cublas needs to allocate a bigger internal workspace mib than on the previous architectures mib. in the current and previous releases, cublas allocates mib. this will be addressed in a future release. a possible workaround is to set the cublasworkspaceconfig environment variable to when running cublas on nvidia hopper architecture. resolved issues reduced cublas hostside overheads caused by not using the cublaslt heuristics cache. this began in the cuda toolkit release. added forward compatible single precision complex gemm that does not require workspace. .. cublas release new features cublasltmatmul now supports fp with a nonzero beta. added int apis to enable larger problem sizes refer to bit integer interface. added more hopperspecific kernels for cublasltmatmul with epilogues cublasltepiloguebgrada,b cublasltepiloguerelu,geluaux cublasltepiloguedrelu,gelu improved hopper performance on armsbsa by adding hopper kernels that were previously supported only on the x architecture for windows and linux. known issues there are no forward compatible kernels for single precision complex gemms that do not require workspace. support will be added in a later release. resolved issues fixed an issue on nvidia ampere architecture and newer gpus where cublasltmatmul with epilogue cublasltepiloguebgrada,b and a nontrivial reduction scheme that is, not cublasltreductionschemenone could return incorrect results for the bias gradient. cublasltmatmul for gemvlike cases that is, m or n equals might ignore bias with the cublasltepiloguerelubias and cublasltepiloguebias epilogues. deprecations disallow including cublas.h and cublasv.h in the same translation unit. removed cublasmatmulstagesx and cublasmatmulstagesx from cublasltmatmulstagest. no kernels utilize these stages anymore. cublasltmmodet, cublasltmatmulprefmathmodemask, and cublasltmatmulprefgaussianmodemask from cublasltmatmulpreferenceattributest. instead, use the corresponding flags from cublasltnumericalimplflagst. cublasltmatmulprefpointermodemask, cublasltmatmulprefepiloguemask, and cublasltmatmulprefsmcounttarget from cublasltmatmulpreferenceattributest. the corresponding parameters are taken directly from cublasltmatmuldesct. cublasltpointermodemasknofiltering from cublasltpointermodemaskt. this mask was only applicable to cublasltmatmulprefmathmodemask which was removed. . cufft library .. cufft release new features added justintime linktime optimized jit lto kernels for improved performance in rc and cr ffts for many sizes. we recommend testing your rc cr use cases with and without jit lto kernels and comparing the resulting performance. you can enable jit lto kernels using the perplan properties cufft api. .. cufft release update resolved issues a routine from the cufft lto ea library was added by mistake to the cufft advanced api header cufftxt.h in cuda . this routine has now been removed from the header. .. cufft release new features added justintime linktime optimized jit lto kernels for improved performance in ffts with bit indexing. added perplan properties to the cufft api. these new routines can be leveraged to give users more control over the behavior of cufft. currently they can be used to enable jit lto kernels for bit ffts. improved accuracy for certain singleprecision fp fft cases, especially involving ffts for larger sizes. known issues a routine from the cufft lto ea library was added by mistake to the cufft advanced api header cufftxt.h. this routine is not supported by cufft, and will be removed from the header in a future release. resolved issues fixed an issue that could cause overwriting of user data when performing outofplace realtocomplex rc transforms with userspecified output strides i.e. using the ostride component of the advanced data layout api. fixed inconsistent behavior between libcufftw and fftw when both inembed and onembed are nullptr null. from now on, as in fftw, passing nullptr null as inembedonembed parameter is equivalent to passing n, that is, the logical size for that dimension. .. cufft release update known issues executing a realtocomplex rc or complextoreal cr plan in a context different to the one used to create the plan could cause undefined behavior. this issue will be fixed in an upcoming release of cufft. resolved issues complextocomplex cc execution functions cufftexec and similar now properly errorout in case of error during kernel launch, for example due to a missing cuda context. .. cufft release new features callback kernels are more relaxed in terms of resource usage, and will use fewer registers. improved accuracy for double precision prime and composite fft sizes with factors larger than . slightly improved planning times for some fft sizes. .. cufft release new features cufftsetstream can be used in multigpu plans with a stream from any gpu context, instead of from the primary context of the first gpu listed in cufftxtsetgpus. improved performance of of ffts of sizes ranging from to . the improved performance spans hundreds of single precision and double precision cases for ffts with contiguous data layout, across multiple gpu architectures from maxwell to hopper gpus via ptx jit. reduced the size of the static libraries when compared to cufft in the release. resolved issues cufft no longer exhibits a race condition when threads simultaneously create and access plans with more than plans alive. cufft no longer exhibits a race condition when multiple threads call cufftxtsetgpus concurrently. .. cufft release update known issues cufft exhibits a race condition when one thread calls cufftcreate or cufftdestroy and another thread calls any api except cufftcreate or cufftdestroy, and when the total number of plans alive exceeds . cufft exhibits a race condition when multiple threads call cufftxtsetgpus concurrently on different plans. .. cufft release new features improved performance on hopper gpus for hundreds of ffts of sizes ranging from to . the improved performance spans over cases across single and double precision for ffts with contiguous data layout. known issues starting from cuda , cuda graphs are no longer supported for callback routines that load data in outofplace mode transforms. an upcoming release will update the cufft callback implementation, removing this limitation. cufft deprecated callback functionality based on separate compiled device code in cufft . resolved issues cufft no longer produces errors with computesanitizer at program exit if the cuda context used at plan creation was destroyed prior to program exit. .. cufft release update resolved issues scratch space requirements for multigpu, singlebatch, d ffts were reduced. .. cufft release new features ptx jit kernel compilation allowed the addition of many new accelerated cases for maxwell, pascal, volta and turing architectures. known issues cufft plan generation time increases due to ptx jit compiling. refer to plan initialization time. resolved issues cufft plans had an unintentional small memory overhead of a few kb per plan. this is resolved. . cusolver library .. cusolver release update resolved issues the potential outofbound accesses on bufferondevice by calls of cusolverdnxlarft have been resolved. .. cusolver release new features performance improvements of cusolverdnxgesvd and cusolverdntgesvd if jobu n or jobvt n. performance improvements of cusolverdnxgesvdp if jobz cusolvereigmodenovector. lower workspace requirement of cusolverdnxgesvdp for tallandskinnymatrices. known issues with cuda toolkit update , values ldt k in calls of cusolverdnxlarft can result in outofbound memory accesses on bufferondevice. as a workaround it is possible to allocate a larger device workspace buffer of size workspaceinbytesondevicealignldtk nksizeofcudadatatypedatatypet, with auto alignintt val return val and auto sizeofcudadatatypecudadatatype dt if dt cudarf return sizeoffloat if dt cudarf return sizeofdouble if dt cudacf return sizeofcucomplex if dt cudacf return sizeofcudoublecomplex .. cusolver release update new features the performance of cusolverdnxlarft has been improved. for large matrices, the speedup might exceed x. the performance on h is now consistently better than on a. the change in cusolverdnxlarft also results in a modest speedup in cusolverdntormqr, cusolverdntormtr, and cusolverdnxsyevd. the performance of cusolverdnxgesvd when singular vectors are sought has been improved. the job configuration that computes both left and right singular vectors is up to x faster. resolved issues cusolverdnxtrtribuffersize now returns the correct workspace size in bytes. deprecations using longdeprecated cusolverdnpotrf, cusolverdnpotrs, cusolverdngeqrf, cusolverdngetrf, cusolverdngetrs, cusolverdnsyevd, cusolverdnsyevdx, cusolverdngesvd, and their accompanying buffersize functions will result in a deprecation warning. the warning can be turned off by using the ddisablecusolverdeprecated flag while compiling however, users should use cusolverdnxpotrf, cusolverdnxpotrs, cusolverdnxgeqrf, cusolverdnxgetrf, cusolverdnxgetrs, cusolverdnxsyevd, cusolverdnxsyevdx, cusolverdnxgesvd, and the corresponding buffersize functions instead. .. cusolver release new features cusolverdnxlarft and cusolverdnxlarftbuffersize apis were introduced. cusolverdnxlarft forms the triangular factor of a real block reflector, while cusolverdnxlarftbuffersize returns its required workspace sizes in bytes. known issues cusolverdnxtrtribuffersize returns an incorrect required device workspace size. as a workaround the returned size can be multiplied by the size of the data type for example, bytes if matrix a is of type double to obtain the correct workspace size. .. cusolver release update resolved issues fixed an issue with cusolverdntgesvd, cusolverdngesvd, and cusolverdnxgesvd, which could cause wrong results for matrices larger than if jobu or jobvt was unequal to n. .. cusolver release new features a new api to ensure deterministic results or allow nondeterministic results for improved performance. see cusolverdnsetdeterministicmode and cusolverdngetdeterministicmode. affected functions are cusolverdntgeqrf, cusolverdntsyevd, cusolverdntsyevdx, cusolverdntgesvdj, cusolverdnxgeqrf, cusolverdnxsyevd, cusolverdnxsyevdx, cusolverdnxgesvdr, and cusolverdnxgesvdp. known issues concurrent executions of cusolverdntgetrf or cusolverdnxgetrf in different nonblocking cuda streams on the same device might result in a deadlock. . cusparse library .. cusparse release update new features added support for bsr format in cusparsespmm. resolved issues cusparsespmm would sometimes get incorrect results when alpha, numbatches, batchstride indicates that there is padding between batches. cusparsespmmbuffersize would return the wrong size when the sparse matrix is blocked ellpack and the dense matrices have only a single column n. cusparsespmm returned the wrong result when k for example when a has zero columns. the correct behavior is doing c beta. the bug behavior was not modifying c at all. cusparsecreateslicedell would return an error when the slice size is greater than the matrix number of rows. slicedellpack cusparsespsv produced wrong results for diagonal matrices. slicedellpack cusparsespsvanalysis failed due to insufficient resources for some matrices and some slice sizes. .. cusparse release new features added support for mixed input types in spmv single precision input matrix, double precision input vector, double precision output vector. resolved issues cusparsespmv introduces invalid memory accesses when the output vector is not aligned to bytes. .. cusparse release new features added the preprocessing step for sparse matrixvector multiplication cusparsespmvpreprocess. added support for mixed real and complex types for cusparsespmm. added a new api cusparsespsmupdatematrix to update the sparse matrix between the analysis and solving phase of cusparsespsm. known issues cusparsespmv introduces invalid memory accesses when the output vector is not aligned to bytes. resolved issues cusparsespvv provided incorrect results when the sparse vector has many nonzeros. .. cusparse release update new features added support for block sizes of and in cusparsesddmm. added a preprocessing step cusparsesddmmpreprocess for bsr cusparsesddmm that helps improve performance of the main computing stage. .. cusparse release new features the cusparsespsvbuffersize and cusparsespsvanalysis routines now accept null pointers for the dense vector. the cusparsespsmbuffersize and cusparsespsmanalysis routines now accept dense matrix descriptors with null pointer for values. known issues the cusparsespsvanalysis and cusparsespsmanalysis routines are blocking callsnot asynchronous. wrong results can occur for cusparsespsv using sliced ellpack format and transposetranspose conjugate operation on matrix a. resolved issues cusparsespsv provided indeterministic results in some cases. fixed an issue that caused cusparsespsvanalysis to hang sometimes in a multithread environment. fixed an issue with cusparsespsv and cusparsespsv that sometimes yielded wrong output when the output vectormatrix or input matrix contained nan. .. cusparse release update new features the library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. see logging api cusparseloggingapi. resolved issues removed cusparsespmmcsralg fallback to avoid confusion in the algorithm selection process. clarified the supported operations for cusparsesddmm. cusparsecreateconstslicedell now uses const pointers. fixed wrong results in rare edge cases of cusparsecsrcscex with base indexing. cusparsespsmbuffersize could ask slightly less memory than needed. cusparsespmv now checks the validity of the buffer pointer only when it is strictly needed. deprecations several legacy apis have been officially deprecated. a compiletime warning has been added to all of them. .. cusparse release update new features introduced block sparse row bsr sparse matrix storage for the generic apis with support for sddmm routine cusparsesddmm. introduced sliced ellpack sell sparse matrix storage format for the generic apis with support for sparse matrixvector multiplication cusparsespmv and triangular solver with a single righthand side cusparsespsv. added a new api call cusparsespsvupdatematrix to update matrix values andor the matrix diagonal in the sparse triangular solver with a single righthand side after the analysis step. .. cusparse release update new features cusparsesddmm now supports mixed precision computation. improved cusparsespmm alg mixedprecision performance on some matrices on nvidia ampere architecture gpus. improved cusparsespmv performance with a new load balancing algorithm. cusparsespsv and cusparsespsm now support inplace computation, namely the output and input vectorsmatrices have the same memory address. resolved issues cusparsespsm could produce wrong results if the leading dimension ld of the rhs matrix is greater than the number of columnsrows. .. cusparse release new features jit lto functionalities cusparsespmmop switched from driver to nvjitlto library. starting from cuda the user needs to link to libnvjitlto.so, see cusparse documentation. jit lto performance has also been improved for cusparsespmmopplan. introduced const descriptors for the generic apis, for example, cusparseconstspvecget. now the generic apis interface clearly declares when a descriptor and its data are modified by the cusparse functions. added two new algorithms to cusparsespgemm with lower memory utilization. the first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks. added intt support to cusparsegather, cusparsescatter, and cusparsecsrcscex. improved cusparsespsv performance for both the analysis and the solving phases. improved cusparsespsm performance for both the analysis and the solving phases. improved cusparsesddmm performance and added support for batch computation. improved cusparsecsrcscex performance. resolved issues cusparsespsv and cusparsespsm could produce wrong results. cusparsednmatgetstridedbatch did not accept batchstride . deprecations removed deprecated cuda x apis, enumerators, and descriptors. . math library .. cuda math release known issues as a result of ongoing testing we updated the interval bounds in which double precision lgamma function may experience greater than the documented ulp accuracy loss. new interval shall read . this finding is applicable to cuda and all previous versions. .. cuda math release resolved issues hostspecific code in cudafpbf headers is now free from typepunning and shall work correctly in the presence of optimizations based on strictaliasing rules. .. cuda math release new features performance of simd integer cuda math apis was improved. resolved issues the hisinf math apis from cudafp.h and cudabf.h headers were silently producing wrong results if compiled with the stdc compiler option because of an underlying nvcc compiler issue, resolved in version . known issues users of cudafp.h and cudabf.h headers are advised to disable host compilers strict aliasing rules based optimizations e.g. pass fnostrictaliasing to host gcc compiler as these may interfere with the typepunning idioms used in the half, half, nvbfloat, nvbfloat types implementations and expose the user program to undefined behavior. note, the headers suppress gcc diagnostics through pragma gcc diagnostic ignored wstrictaliasing. this behavior may improve in future versions of the headers. .. cuda math release new features cuda math apis for half and nvbfloat types received usability improvements, including host side emulated support for many of the arithmetic operations and conversions. half and nvbfloat types have implicit conversions tofrom integral types, which are now available with host compilers by default. these may cause build issues due to ambiguous overloads resolution. users are advised to update their code to select proper overloads. to optout user may want to define the following macros these macros will be removed in the future cuda release cudafpdisableimplicitintegerconvertsforhostcompilers cudabfdisableimplicitintegerconvertsforhostcompilers resolved issues during ongoing testing, nvidia identified that due to an algorithm error the results of bit floatingpoint division in default roundtonearesteven mode could produce spurious overflow to infinity. nvidia recommends that all developers requiring strict ieee compliance update to cuda toolkit or newer. the affected algorithm was present in both offline compilation as well as justintime jit compilation. as jit compilation is handled by the driver, nvidia recommends updating to driver version greater than or equal to r r on windows when ieee compliance is required and when using jit. this is a software algorithm fix and is not tied to specific hardware. updated the observed worst case error bounds for single precision intrinsic functions expf, expf and double precision functions asinh, acosh. .. cuda math release new features performance and accuracy improvements in atanf, acosf, asinf, sinpif, cospif, powf, erff, and tgammaf. .. cuda math release new features introduced new integerfpbf cuda math apis to help expose performance benefits of new dpx instructions. refer to known issues double precision inputs that cause the double precision division algorithm in the default round to nearest even mode produce spurious overflow an infinite result is delivered where dblmax xfefffffffffffff is expected. affected cuda math apis ddivrn. affected cuda language operation double precision operation in the device code. deprecations all previously deprecated undocumented apis are removed from cuda . . nvidia performance primitives npp .. npp release new features enhanced large file support with sizet. .. npp release deprecations deprecating nonctx api support from next release. resolved issues a performance issue with the npp resizesqrpixel api is now fixed and shows improved performance. . nvjpeg library .. nvjpeg release new features idct performance optimizations for single image cuda decode. zero copy behavior has been changed setting nvjpegflagsreducedmemorydecodezerocopy flag will no longer enable nvjpegflagsreducedmemorydecode. .. nvjpeg release update new features new apis nvjpegbufferpinnedresize and nvjpegbufferdeviceresize which can be used to resize pinned and device buffers before using them. .. nvjpeg release new features added support for jpeg lossless decode process , fo prediction. nvjpeg is now supported on lt. .. nvjpeg release new features immproved the gpu memory optimisation for the nvjpeg codec. resolved issues an issue that causes runtime failures when nvjpegdecmultipleinstances was tested with a large number of threads is resolved. an issue with cmyk four component color conversion is now resolved. known issues backend nvjpegbackendgpuhybrid unable to handle bistreams with extra scans lengths. deprecations the reuse of huffman table in encoder nvjpegencoderparamscopyhuffmantables. only available on select linux distros . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated.",
    "depth": 4
  },
  {
    "url": "https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html",
    "content": "nvidia ampere gpu architecture compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia ampere gpu architecture. . nvidia ampere gpu architecture compatibility . about this document this application note, nvidia ampere gpu architecture compatibility guide for cuda applications, is intended to help developers ensure that their nvidia cuda applications will run on the nvidia ampere architecture based gpus. this document provides guidance to developers who are familiar with programming in cuda c and want to make sure that their software applications are compatible with the nvidia ampere gpu architecture. . application compatibility on the nvidia ampere gpu architecture a cuda application binary with one or more gpu kernels can contain the compiled gpu code in two forms, binary cubin objects and forwardcompatible ptx assembly for each kernel. both cubin and ptx are generated for a certain target compute capability. a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability. for example, a cubin generated for compute capability is supported to run on a gpu with compute capability , however a cubin generated for compute capability is not supported to run on a gpu with compute capability , and a cubin generated with compute capability x is not supported to run on a gpu with compute capability x. kernel can also be compiled to a ptx form. at the application load time, ptx is compiled to cubin and the cubin is used for kernel execution. unlike cubin, ptx is forwardcompatible. meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx. for example, ptx code generated for compute capability x is supported to run on compute capability x or any higher revision major or minor, including compute capability x. therefore although it is optional, it is recommended that all applications should include ptx of the kernels to ensure forwardcompatibility. to read more about cubin and ptx compatibilities see compilation with nvcc from the programming guide. when a cuda application launches a kernel on a gpu, the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel. if a cubin compatible with that gpu is present in the binary, the cubin is used asis for execution. otherwise, the cuda runtime first generates compatible cubin by jitcompiling the ptx and then the cubin is used for the execution. if neither compatible cubin nor ptx is available, kernel launch results in a failure. application binaries that include ptx version of kernels, should work asis on the nvidia ampere architecture based gpus. in such cases, rebuilding the application is not required. however application binaries which do not include ptx only include cubins, need to be rebuilt to run on the nvidia ampere architecture based gpus. to know more about building compatible applications read building applications with the nvidia ampere gpu architecture support. . verifying ampere compatibility for existing applications the first step towards making a cuda application compatible with the nvidia ampere gpu architecture is to check if the application binary already contains compatible gpu code at least the ptx. the following sections explain how to accomplish this for an already built cuda application. .. applications built using cuda toolkit or earlier cuda applications built using cuda toolkit versions through are compatible with nvidia ampere architecture based gpus as long as they are built to include ptx versions of their kernels. this can be tested by forcing the ptx to jitcompile at application load time with following the steps download and install the latest driver from set the environment variable cudaforceptxjit. launch the application. with cudaforceptxjit, gpu binary code embedded in an application binary is ignored. instead ptx code for each kernel is jitcompiled to produce gpu binary code. an application fails to execute if it does not include ptx. this means the application is not compatible with the nvidia ampere gpu architecture and needs to be rebuilt for compatibility. on the other hand, if the application works properly with this environment variable set, then the application is compatible with the nvidia ampere gpu architecture. note be sure to unset the cudaforceptxjit environment variable after testing is done. .. applications built using cuda toolkit cuda applications built using cuda toolkit are compatible with the nvidia ampere gpu architecture as long as they are built to include kernels in native cubin compute capability or ptx form or both. . building applications with the nvidia ampere gpu architecture support depending on the version of the cuda toolkit used for building the application, it can be built to include ptx andor native cubin for the nvidia ampere gpu architecture. although it is enough to just include ptx, including native cubin also has the following advantages it saves the end user the time it takes to jitcompile kernels that are available only as ptx. all kernels which do not have native cubins are jitcompiled from ptx, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. especially when using large libraries, this jit compilation can take a significant amount of time. the cuda driver caches the cubins generated as a result of the ptx jit, so this is mostly a onetime cost for a user, but it is time best avoided whenever possible. ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus, meaning that nativecompiled cubins may be faster or of greater accuracy. .. building applications using cuda toolkit x or earlier the nvcc compiler included with versions x , and of the cuda toolkit can generate cubins native to the volta and turing architectures compute capability x. when using cuda toolkit x, to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, the simplified nvcc commandline option archsmxx can be used. it is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmxx expands to the following gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. for cuda toolkits prior to , one or more of the gencode options will need to be removed according to the architectures supported by the specific toolkit version for example, cuda toolkit x supports architectures up to and . the final gencode to generate ptx would also need to be update for further information and examples see the documentation for the specific cuda toolkit version. note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. .. building applications using cuda toolkit with versions of the cuda toolkit, nvcc can generate cubin native to the nvidia ampere gpu architecture compute capability . when using cuda toolkit , to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. .. independent thread scheduling compatibility nvidia gpus since volta architecture have independent thread scheduling among threads in a warp. if the developer made assumptions about warpsynchronicity, this feature can alter the set of threads participating in the executed code compared to previous architectures. please see compute capability in the programming guide for details and corrective actions. to aid migration to the nvidia ampere gpu architecture, developers can optin to the pascal scheduling model with the following combination of compiler options. nvcc gencodearchcompute,codesm ... . revision history version initial public release. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated. justintime compilation warpsynchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.",
    "depth": 5
  },
  {
    "url": "https://docs.nvidia.com/cuda/hopper-compatibility-guide/index.html",
    "content": "hopper compatibility guide for cuda applications the guide to building cuda applications for hopper gpus . hopper architecture compatibility . about this document this application note, hopper architecture compatibility guide for cuda applications, is intended to help developers ensure that their nvidia cuda applications will run on the nvidia hopper architecture based gpus. this document provides guidance to developers who are familiar with programming in cuda c and want to make sure that their software applications are compatible with hopper architecture. . application compatibility on hopper architecture a cuda application binary with one or more gpu kernels can contain the compiled gpu code in two forms, binary cubin objects and forwardcompatible ptx assembly for each kernel. both cubin and ptx are generated for a certain target compute capability. a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability. for example, a cubin generated for compute capability is supported to run on a gpu with compute capability , however a cubin generated for compute capability is not supported to run on a gpu with compute capability , and a cubin generated with compute capability x is not supported to run on a gpu with compute capability . kernel can also be compiled to a ptx form. at the application load time, ptx is compiled to cubin and the cubin is used for kernel execution. unlike cubin, ptx is forwardcompatible. meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx. for example, ptx code generated for compute capability x is supported to run on compute capability x or any higher revision major or minor, including compute capability . therefore although it is optional, it is recommended that all applications should include ptx of the kernels to ensure forwardcompatibility. to read more about cubin and ptx compatibilities see compilation with nvcc from the cuda c programming guide. when a cuda application launches a kernel on a gpu, the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel. if a cubin compatible with that gpu is present in the binary, the cubin is used asis for execution. otherwise, the cuda runtime first generates compatible cubin by jitcompiling the ptx and then the cubin is used for the execution. if neither compatible cubin nor ptx is available, kernel launch results in a failure. application binaries that include ptx version of kernels, should work asis on the hopper gpus. in such cases, rebuilding the application is not required. however application binaries which do not include ptx only include cubins, need to be rebuilt to run on the hopper gpus. to know more about building compatible applications read building applications with hopper architecture support application binaries that include ptx version of kernels with architecture conditional features using sma or computea in order to take full advantage of hopper gpu architecture, are not forward or backward compatible. . verifying hopper compatibility for existing applications the first step towards making a cuda application compatible with hopper architecture is to check if the application binary already contains compatible gpu code at least the ptx. the following sections explain how to accomplish this for an already built cuda application. .. applications built using cuda toolkit or earlier cuda applications built using cuda toolkit versions through are compatible with hopper gpus as long as they are built to include ptx versions of their kernels. this can be tested by forcing the ptx to jitcompile at application load time with following the steps download and install the latest driver from set the environment variable cudaforceptxjit. launch the application. with cudaforceptxjit, gpu binary code embedded in an application binary is ignored. instead ptx code for each kernel is jitcompiled to produce gpu binary code. an application fails to execute if it does not include ptx. this means the application is not hopper architecture compatible and needs to be rebuilt for compatibility. on the other hand, if the application works properly with this environment variable set, then the application is hopper compatible. note be sure to unset the cudaforceptxjit environment variable after testing is done. .. applications built using cuda toolkit cuda applications built using cuda toolkit are compatible with hopper architecture as long as they are built to include kernels in native cubin compute capability or ptx form or both. . building applications with hopper architecture support depending on the version of the cuda toolkit used for building the application, it can be built to include ptx andor native cubin for the hopper architecture. although it is enough to just include ptx, including native cubin also has the following advantages it saves the end user the time it takes to jitcompile kernels that are available only as ptx. all kernels which do not have native cubins are jitcompiled from ptx, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. especially when using large libraries, this jit compilation can take a significant amount of time. the cuda driver caches the cubins generated as a result of the ptx jit, so this is mostly a onetime cost for a user, but it is time best avoided whenever possible. ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus, meaning that nativecompiled cubins may be faster or of greater accuracy. ptx code compiled to target architecture conditional features using sma or computea only runs on devices with compute capability and is not backward or forward compatible. .. building applications using cuda toolkit or earlier the nvcc compiler included with version or earlier of the cuda toolkit can generate cubins native to the nvidia ampere gpu architectures compute capability x. when using cuda toolkit or earlier, to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu alternatively, the simplified nvcc commandline option archsmxx can be used. it is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmxx expands to the following gencodearchcomputexx,codesmxx gencodearchcomputexx,codecomputexx however, while the archsmxx commandline option does result in inclusion of a ptx backend target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple arch options on the same nvcc command line, which is why the examples above use gencode explicitly. for cuda toolkits prior to , one or more of the gencode options need to be removed according to the architectures supported by the specific toolkit version for example, cuda toolkit x supports architectures up to sm and sm. the final gencode to generate ptx also needs to be updated. for further information and examples see the documentation for the specific cuda toolkit version. note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. .. building applications using cuda toolkit with versions of the cuda toolkit, nvcc can generate cubin native to the hopper architecture compute capability . when using cuda toolkit , to ensure that nvcc will generate cubin files for all recent gpu architectures as well as a ptx version for forward compatibility with future gpu architectures, specify the appropriate gencode parameters on the nvcc command line as shown in the examples below. windows nvcc.exe ccbin cvsvcbin xcompiler ehsc w nologo o zi mt gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute compile o releasemykernel.cu.obj mykernel.cu maclinux usrlocalcudabinnvcc gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codesm gencodearchcompute,codecompute o o mykernel.o c mykernel.cu note computexx refers to a ptx version and smxx refers to a cubin version. the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version. the code clause specifies the backend compilation target and can either be cubin or ptx or both. only the backend target versions specified by the code clause will be retained in the resulting binary at least one should be ptx to provide compatibility with future architectures. .. independent thread scheduling compatibility nvidia gpus since volta architecture have independent thread scheduling among threads in a warp. if the developer made assumptions about warpsynchronicity, this feature can alter the set of threads participating in the executed code compared to previous architectures. please see compute capability x in the cuda c programming guide for details and corrective actions. to aid migration to the hopper architecture, developers can optin to the pascal scheduling model with the following combination of compiler options. nvcc gencodearchcompute,codesm ... . revision history version initial public release. . notices . notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. nvidia corporation nvidia makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. this document is not a commitment to develop, release, or deliver any material defined below, code, or functionality. nvidia reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale. nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document. no contractual obligations are formed either directly or indirectly by this document. nvidia products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury, death, or property or environmental damage. nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk. nvidia makes no representation or warranty that products based on this document will be suitable for any specified use. testing of all parameters of each product is not necessarily performed by nvidia. it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document. nvidia accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs. no license, either expressed or implied, is granted under any nvidia patent right, copyright, or other nvidia intellectual property right under this document. information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof. use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from nvidia under the patents or other intellectual property rights of nvidia. reproduction of information in this document is permissible only if approved in advance by nvidia in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. this document and all nvidia design specifications, reference boards, files, drawings, diagnostics, lists, and other documents together and separately, materials are being provided as is. nvidia makes no warranties, expressed, implied, statutory, or otherwise with respect to the materials, and expressly disclaims all implied warranties of noninfringement, merchantability, and fitness for a particular purpose. to the extent not prohibited by law, in no event will nvidia be liable for any damages, including without limitation any direct, indirect, special, incidental, punitive, or consequential damages, however caused and regardless of the theory of liability, arising out of any use of this document, even if nvidia has been advised of the possibility of such damages. notwithstanding any damages that customer might incur for any reason whatsoever, nvidias aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the terms of sale for the product. . opencl opencl is a trademark of apple inc. used under license to the khronos group inc. . trademarks nvidia and the nvidia logo are trademarks or registered trademarks of nvidia corporation in the u.s. and other countries. other company and product names may be trademarks of the respective companies with which they are associated. justintime compilation. starting with cuda toolkit , this default behavior can be changed with environment variable cudamoduleloading. see environment variables in the cuda c programming guide for details. warpsynchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization.",
    "depth": 5
  }
]